{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GATE Neural Network with Linear Two Neurons - Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Layers with Forward and Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AffineWithTwoInputs:\n",
    "    def __init__(self):\n",
    "        self.w = np.array([random.random(), random.random()])   # weight of one input\n",
    "        self.b = np.array([random.random()])  # bias\n",
    "        self.x = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(self.w, self.x) + self.b\n",
    "        return out\n",
    "\n",
    "    def backward(self, din):\n",
    "        if isinstance(din, np.ndarray) and din.size == 1:\n",
    "            din = np.asscalar(din)\n",
    "        dx = np.dot(din, self.w.T)\n",
    "        self.dw = np.dot(self.x.T, din)\n",
    "        self.db = din\n",
    "        return dx\n",
    "\n",
    "class AffineWithOneInput:\n",
    "    def __init__(self):\n",
    "        self.w = np.array([random.random()])   # weight of one input\n",
    "        self.b = np.array([random.random()])   # bias\n",
    "        self.x = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(self.w, self.x) + self.b\n",
    "        return out\n",
    "\n",
    "    def backward(self, din):\n",
    "        dx = np.dot(din, self.w.T)\n",
    "        self.dw = np.dot(self.x.T, din)\n",
    "        self.db = din\n",
    "        return dx\n",
    "    \n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        mask = (self.x <= 0)\n",
    "        out = self.x.copy()\n",
    "        out[mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, din):\n",
    "        if isinstance(din, np.ndarray):\n",
    "            mask = (self.x <= 0)\n",
    "            din[mask] = 0\n",
    "            dx = din\n",
    "        else:\n",
    "            if self.x <= 0:\n",
    "                dx = 0\n",
    "            else:\n",
    "                dx = din\n",
    "        return dx\n",
    "    \n",
    "class SquaredError:\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "        self.z_target = None\n",
    "    \n",
    "    def forward(self, z, z_target):\n",
    "        self.z = z\n",
    "        self.z_target = z_target\n",
    "        loss = 1.0 / 2.0 * math.pow(self.z - self.z_target, 2)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, din):\n",
    "        dx = (self.z - self.z_target) * din\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network Model of Linear Two Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearTwoNeurons:\n",
    "    def __init__(self):\n",
    "        self.n1 = AffineWithTwoInputs()\n",
    "        self.relu1 = Relu()\n",
    "        self.n2 = AffineWithOneInput()\n",
    "        self.relu2 = Relu()\n",
    "        self.loss = SquaredError()\n",
    "        print(\"Neuron n1 - Initial w: {0}, b: {1}\".format(self.n1.w, self.n1.b))\n",
    "        print(\"Neuron n2 - Initial w: {0}, b: {1}\".format(self.n2.w, self.n2.b))\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        u1 = self.n1.forward(x)\n",
    "        z1 = self.relu1.forward(u1)\n",
    "        u2 = self.n2.forward(z1)\n",
    "        z2 = self.relu2.forward(u2)\n",
    "        return z2\n",
    "    \n",
    "    def backpropagation_gradient(self, x, z_target):\n",
    "        # forward\n",
    "        z2 = self.predict(x)\n",
    "        self.loss.forward(z2, z_target)\n",
    "\n",
    "        # backward\n",
    "        din = 1\n",
    "        din = self.loss.backward(din)\n",
    "        din = self.relu2.backward(din)\n",
    "        din = self.n2.backward(din)\n",
    "        din = self.relu1.backward(din)\n",
    "        self.n1.backward(din)\n",
    "\n",
    "    def learning(self, alpha, x, z_target):\n",
    "        self.backpropagation_gradient(x, z_target)\n",
    "\n",
    "        self.n1.w = self.n1.w - alpha * self.n1.dw\n",
    "        self.n1.b = self.n1.b - alpha * self.n1.db\n",
    "        self.n2.w = self.n2.w - alpha * self.n2.dw\n",
    "        self.n2.b = self.n2.b - alpha * self.n2.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. OR gate with Two Linear Neurons - Learing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron n1 - Initial w: [ 0.00705989  0.2138305 ], b: [ 0.23089211]\n",
      "Neuron n2 - Initial w: [ 0.61048821], b: [ 0.14184584]\n",
      "x: [ 0.  0.], z2: [ 0.28280275], z_target: 0.0, error: 0.03999\n",
      "x: [ 1.  0.], z2: [ 0.28711274], z_target: 1.0, error: 0.25410\n",
      "x: [ 0.  1.], z2: [ 0.41334375], z_target: 1.0, error: 0.17208\n",
      "x: [ 1.  1.], z2: [ 0.41765373], z_target: 1.0, error: 0.16956\n",
      "Epoch   0-Error:0.14686, Neuron n1[w11: 0.01488, w12: 0.22083, b1: 0.24054], Neuron n2[w2: 0.61674, b2: 0.15761]\n",
      "Epoch 100-Error:0.04362, Neuron n1[w11: 0.30293, w12: 0.41913, b1: 0.29781], Neuron n2[w2: 0.79176, b2: 0.25576]\n",
      "Epoch 200-Error:0.03448, Neuron n1[w11: 0.42743, w12: 0.48276, b1: 0.23114], Neuron n2[w2: 0.85885, b2: 0.17540]\n",
      "Epoch 300-Error:0.03196, Neuron n1[w11: 0.48886, w12: 0.51099, b1: 0.19398], Neuron n2[w2: 0.89649, b2: 0.13357]\n",
      "Epoch 400-Error:0.03142, Neuron n1[w11: 0.51620, w12: 0.52253, b1: 0.17684], Neuron n2[w2: 0.91330, b2: 0.11519]\n",
      "Epoch 500-Error:0.03131, Neuron n1[w11: 0.52789, w12: 0.52721, b1: 0.16957], Neuron n2[w2: 0.91978, b2: 0.10784]\n",
      "Epoch 600-Error:0.03128, Neuron n1[w11: 0.53296, w12: 0.52927, b1: 0.16653], Neuron n2[w2: 0.92182, b2: 0.10513]\n",
      "Epoch 700-Error:0.03127, Neuron n1[w11: 0.53533, w12: 0.53035, b1: 0.16520], Neuron n2[w2: 0.92207, b2: 0.10428]\n",
      "Epoch 800-Error:0.03127, Neuron n1[w11: 0.53660, w12: 0.53108, b1: 0.16454], Neuron n2[w2: 0.92161, b2: 0.10416]\n",
      "Epoch 900-Error:0.03127, Neuron n1[w11: 0.53743, w12: 0.53168, b1: 0.16414], Neuron n2[w2: 0.92087, b2: 0.10433]\n",
      "Epoch1000-Error:0.03127, Neuron n1[w11: 0.53808, w12: 0.53224, b1: 0.16385], Neuron n2[w2: 0.92003, b2: 0.10461]\n",
      "x: [ 0.  0.], z2: [ 0.25536016], z_target: 0.0, error: 0.03260\n",
      "x: [ 1.  0.], z2: [ 0.75041335], z_target: 1.0, error: 0.03115\n",
      "x: [ 0.  1.], z2: [ 0.74503203], z_target: 1.0, error: 0.03250\n",
      "x: [ 1.  1.], z2: [ 1.24008522], z_target: 1.0, error: 0.02882\n"
     ]
    }
   ],
   "source": [
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.training_input_value = np.array([(0.0, 0.0), (1.0, 0.0), (0.0, 1.0), (1.0, 1.0)])\n",
    "        self.training_z_target = np.array([0.0, 1.0, 1.0, 1.0])\n",
    "        self.numTrainData = len(self.training_input_value)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ltn = LinearTwoNeurons()\n",
    "    d = Data()\n",
    "    for idx in range(d.numTrainData):\n",
    "        x = d.training_input_value[idx]\n",
    "        z2 = ltn.predict(x)\n",
    "        z_target = d.training_z_target[idx]\n",
    "        error = ltn.loss.forward(z2, z_target)\n",
    "        print(\"x: {0:s}, z2: {1:s}, z_target: {2:s}, error: {3:7.5f}\".format(str(x), str(z2), str(z_target), error))\n",
    "\n",
    "    max_epoch = 1000\n",
    "    print_epoch_period = 100\n",
    "    for i in range(max_epoch + 1):\n",
    "        for idx in range(d.numTrainData):\n",
    "            x = d.training_input_value[idx]\n",
    "            z_target = d.training_z_target[idx]\n",
    "            ltn.learning(0.01, x, z_target)\n",
    "\n",
    "        if i % print_epoch_period == 0:\n",
    "            sum = 0.0\n",
    "            for idx in range(d.numTrainData):\n",
    "                x = d.training_input_value[idx]\n",
    "                z2 = ltn.predict(x)\n",
    "                z_target = d.training_z_target[idx]\n",
    "                sum = sum + ltn.loss.forward(z2, z_target)\n",
    "\n",
    "            print(\"Epoch{0:4d}-Error:{1:7.5f}, Neuron n1[w11: {2:7.5f}, w12: {3:7.5f}, b1: {4:7.5f}], Neuron n2[w2: {5:7.5f}, b2: {6:7.5f}]\".format(\n",
    "                i, \n",
    "                sum / d.numTrainData,\n",
    "                ltn.n1.w[0],\n",
    "                ltn.n1.w[1],\n",
    "                ltn.n1.b[0],\n",
    "                ltn.n2.w[0],\n",
    "                ltn.n2.b[0])\n",
    "            )\n",
    "            \n",
    "    for idx in range(d.numTrainData):\n",
    "        x = d.training_input_value[idx]\n",
    "        z2 = ltn.predict(x)\n",
    "        z_target = d.training_z_target[idx]\n",
    "        error = ltn.loss.forward(z2, z_target)\n",
    "        print(\"x: {0:s}, z2: {1:s}, z_target: {2:s}, error: {3:7.5f}\".format(str(x), str(z2), str(z_target), error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. AND gate with Two Linear Neurons - Learing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron n1 - Initial w: [ 0.94079528  0.33696781], b: [ 0.6618524]\n",
      "Neuron n2 - Initial w: [ 0.52277765], b: [ 0.73916905]\n",
      "x: [ 0.  0.], z2: [ 1.08517069], z_target: 0.0, error: 0.58880\n",
      "x: [ 1.  0.], z2: [ 1.57699744], z_target: 0.0, error: 1.24346\n",
      "x: [ 0.  1.], z2: [ 1.26132993], z_target: 0.0, error: 0.79548\n",
      "x: [ 1.  1.], z2: [ 1.75315668], z_target: 1.0, error: 0.28362\n",
      "Epoch   0-Error:0.58495, Neuron n1[w11: 0.92985, w12: 0.32816, b1: 0.63937], Neuron n2[w2: 0.46733, b2: 0.69470]\n",
      "Epoch 100-Error:0.06764, Neuron n1[w11: 0.91231, w12: 0.33320, b1: 0.50092], Neuron n2[w2: 0.21678, b2: 0.07118]\n",
      "Epoch 200-Error:0.05013, Neuron n1[w11: 0.94734, w12: 0.42130, b1: 0.44332], Neuron n2[w2: 0.36755, b2: -0.11739]\n",
      "Epoch 300-Error:0.03895, Neuron n1[w11: 0.96247, w12: 0.52675, b1: 0.38216], Neuron n2[w2: 0.47038, b2: -0.25811]\n",
      "Epoch 400-Error:0.02826, Neuron n1[w11: 0.96691, w12: 0.63793, b1: 0.31187], Neuron n2[w2: 0.56200, b2: -0.39151]\n",
      "Epoch 500-Error:0.01863, Neuron n1[w11: 0.96629, w12: 0.74283, b1: 0.23670], Neuron n2[w2: 0.64914, b2: -0.51412]\n",
      "Epoch 600-Error:0.01110, Neuron n1[w11: 0.96584, w12: 0.83068, b1: 0.16312], Neuron n2[w2: 0.72863, b2: -0.62017]\n",
      "Epoch 700-Error:0.00603, Neuron n1[w11: 0.96915, w12: 0.89659, b1: 0.09754], Neuron n2[w2: 0.79670, b2: -0.70581]\n",
      "Epoch 800-Error:0.00304, Neuron n1[w11: 0.97640, w12: 0.94170, b1: 0.04391], Neuron n2[w2: 0.85114, b2: -0.77074]\n",
      "Epoch 900-Error:0.00145, Neuron n1[w11: 0.98550, w12: 0.97068, b1: 0.00305], Neuron n2[w2: 0.89212, b2: -0.81754]\n",
      "Epoch1000-Error:0.00066, Neuron n1[w11: 0.99428, w12: 0.98868, b1: -0.02647], Neuron n2[w2: 0.92153, b2: -0.85007]\n",
      "x: [ 0.  0.], z2: [ 0.], z_target: 0.0, error: 0.00000\n",
      "x: [ 1.  0.], z2: [ 0.04179845], z_target: 0.0, error: 0.00087\n",
      "x: [ 0.  1.], z2: [ 0.03664245], z_target: 0.0, error: 0.00067\n",
      "x: [ 1.  1.], z2: [ 0.95290215], z_target: 1.0, error: 0.00111\n"
     ]
    }
   ],
   "source": [
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.training_input_value = np.array([(0.0, 0.0), (1.0, 0.0), (0.0, 1.0), (1.0, 1.0)])\n",
    "        self.training_z_target = np.array([0.0, 0.0, 0.0, 1.0])\n",
    "        self.numTrainData = len(self.training_input_value)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ltn = LinearTwoNeurons()\n",
    "    d = Data()\n",
    "    for idx in range(d.numTrainData):\n",
    "        x = d.training_input_value[idx]\n",
    "        z2 = ltn.predict(x)\n",
    "        z_target = d.training_z_target[idx]\n",
    "        error = ltn.loss.forward(z2, z_target)\n",
    "        print(\"x: {0:s}, z2: {1:s}, z_target: {2:s}, error: {3:7.5f}\".format(str(x), str(z2), str(z_target), error))\n",
    "\n",
    "    max_epoch = 1000\n",
    "    print_epoch_period = 100\n",
    "    for i in range(max_epoch + 1):\n",
    "        for idx in range(d.numTrainData):\n",
    "            x = d.training_input_value[idx]\n",
    "            z_target = d.training_z_target[idx]\n",
    "            ltn.learning(0.01, x, z_target)\n",
    "\n",
    "        if i % print_epoch_period == 0:\n",
    "            sum = 0.0\n",
    "            for idx in range(d.numTrainData):\n",
    "                x = d.training_input_value[idx]\n",
    "                z2 = ltn.predict(x)\n",
    "                z_target = d.training_z_target[idx]\n",
    "                sum = sum + ltn.loss.forward(z2, z_target)\n",
    "\n",
    "            print(\"Epoch{0:4d}-Error:{1:7.5f}, Neuron n1[w11: {2:7.5f}, w12: {3:7.5f}, b1: {4:7.5f}], Neuron n2[w2: {5:7.5f}, b2: {6:7.5f}]\".format(\n",
    "                i, \n",
    "                sum / d.numTrainData,\n",
    "                ltn.n1.w[0],\n",
    "                ltn.n1.w[1],\n",
    "                ltn.n1.b[0],\n",
    "                ltn.n2.w[0],\n",
    "                ltn.n2.b[0])\n",
    "            )\n",
    "            \n",
    "    for idx in range(d.numTrainData):\n",
    "        x = d.training_input_value[idx]\n",
    "        z2 = ltn.predict(x)\n",
    "        z_target = d.training_z_target[idx]\n",
    "        error = ltn.loss.forward(z2, z_target)\n",
    "        print(\"x: {0:s}, z2: {1:s}, z_target: {2:s}, error: {3:7.5f}\".format(str(x), str(z2), str(z_target), error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. XOR gate with Two Linear Neurons - Learing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron n1 - Initial w: [ 0.31726888  0.51813443], b: [ 0.10051975]\n",
      "Neuron n2 - Initial w: [ 0.94270109], b: [ 0.28726142]\n",
      "x: [ 0.  0.], z2: [ 0.3820215], z_target: 0.0, error: 0.07297\n",
      "x: [ 1.  0.], z2: [ 0.68111121], z_target: 1.0, error: 0.05085\n",
      "x: [ 0.  1.], z2: [ 0.87046738], z_target: 1.0, error: 0.00839\n",
      "x: [ 1.  1.], z2: [ 1.1695571], z_target: 0.0, error: 0.68393\n",
      "Epoch   0-Error:0.19279, Neuron n1[w11: 0.30923, w12: 0.50825, b1: 0.09011], Neuron n2[w2: 0.93340, b2: 0.27624]\n",
      "Epoch 100-Error:0.13164, Neuron n1[w11: 0.15542, w12: 0.25162, b1: 0.07308], Neuron n2[w2: 0.76535, b2: 0.26647]\n",
      "Epoch 200-Error:0.12723, Neuron n1[w11: 0.10004, w12: 0.15079, b1: 0.10976], Neuron n2[w2: 0.72507, b2: 0.31732]\n",
      "Epoch 300-Error:0.12581, Neuron n1[w11: 0.06490, w12: 0.09111, b1: 0.12918], Neuron n2[w2: 0.70667, b2: 0.34586]\n",
      "Epoch 400-Error:0.12530, Neuron n1[w11: 0.04195, w12: 0.05395, b1: 0.14060], Neuron n2[w2: 0.69606, b2: 0.36343]\n",
      "Epoch 500-Error:0.12510, Neuron n1[w11: 0.02679, w12: 0.03030, b1: 0.14764], Neuron n2[w2: 0.68829, b2: 0.37483]\n",
      "Epoch 600-Error:0.12503, Neuron n1[w11: 0.01670, w12: 0.01506, b1: 0.15208], Neuron n2[w2: 0.68152, b2: 0.38252]\n",
      "Epoch 700-Error:0.12501, Neuron n1[w11: 0.00996, w12: 0.00514, b1: 0.15491], Neuron n2[w2: 0.67508, b2: 0.38788]\n",
      "Epoch 800-Error:0.12500, Neuron n1[w11: 0.00544, w12: -0.00135, b1: 0.15672], Neuron n2[w2: 0.66871, b2: 0.39176]\n",
      "Epoch 900-Error:0.12500, Neuron n1[w11: 0.00239, w12: -0.00564, b1: 0.15788], Neuron n2[w2: 0.66235, b2: 0.39469]\n",
      "Epoch1000-Error:0.12501, Neuron n1[w11: 0.00033, w12: -0.00850, b1: 0.15861], Neuron n2[w2: 0.65600, b2: 0.39699]\n",
      "x: [ 0.  0.], z2: [ 0.50103333], z_target: 0.0, error: 0.12552\n",
      "x: [ 1.  0.], z2: [ 0.50124686], z_target: 1.0, error: 0.12438\n",
      "x: [ 0.  1.], z2: [ 0.49545664], z_target: 1.0, error: 0.12728\n",
      "x: [ 1.  1.], z2: [ 0.49567017], z_target: 0.0, error: 0.12284\n"
     ]
    }
   ],
   "source": [
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.training_input_value = np.array([(0.0, 0.0), (1.0, 0.0), (0.0, 1.0), (1.0, 1.0)])\n",
    "        self.training_z_target = np.array([0.0, 1.0, 1.0, 0.0])\n",
    "        self.numTrainData = len(self.training_input_value)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ltn = LinearTwoNeurons()\n",
    "    d = Data()\n",
    "    for idx in range(d.numTrainData):\n",
    "        x = d.training_input_value[idx]\n",
    "        z2 = ltn.predict(x)\n",
    "        z_target = d.training_z_target[idx]\n",
    "        error = ltn.loss.forward(z2, z_target)\n",
    "        print(\"x: {0:s}, z2: {1:s}, z_target: {2:s}, error: {3:7.5f}\".format(str(x), str(z2), str(z_target), error))\n",
    "\n",
    "    max_epoch = 1000\n",
    "    print_epoch_period = 100\n",
    "    for i in range(max_epoch + 1):\n",
    "        for idx in range(d.numTrainData):\n",
    "            x = d.training_input_value[idx]\n",
    "            z_target = d.training_z_target[idx]\n",
    "            ltn.learning(0.01, x, z_target)\n",
    "\n",
    "        if i % print_epoch_period == 0:\n",
    "            sum = 0.0\n",
    "            for idx in range(d.numTrainData):\n",
    "                x = d.training_input_value[idx]\n",
    "                z2 = ltn.predict(x)\n",
    "                z_target = d.training_z_target[idx]\n",
    "                sum = sum + ltn.loss.forward(z2, z_target)\n",
    "\n",
    "            print(\"Epoch{0:4d}-Error:{1:7.5f}, Neuron n1[w11: {2:7.5f}, w12: {3:7.5f}, b1: {4:7.5f}], Neuron n2[w2: {5:7.5f}, b2: {6:7.5f}]\".format(\n",
    "                i, \n",
    "                sum / d.numTrainData,\n",
    "                ltn.n1.w[0],\n",
    "                ltn.n1.w[1],\n",
    "                ltn.n1.b[0],\n",
    "                ltn.n2.w[0],\n",
    "                ltn.n2.b[0])\n",
    "            )\n",
    "            \n",
    "    for idx in range(d.numTrainData):\n",
    "        x = d.training_input_value[idx]\n",
    "        z2 = ltn.predict(x)\n",
    "        z_target = d.training_z_target[idx]\n",
    "        error = ltn.loss.forward(z2, z_target)\n",
    "        print(\"x: {0:s}, z2: {1:s}, z_target: {2:s}, error: {3:7.5f}\".format(str(x), str(z2), str(z_target), error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
