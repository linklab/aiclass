{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 멀티 뉴런 (Multiple Neurons) - Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Layers with Forward and Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AffineWithTwoInputs:\n",
    "    def __init__(self):\n",
    "        self.w = np.array([random.random(), random.random()])   # weight of one input\n",
    "        self.b = np.array([random.random()])  # bias\n",
    "        self.x = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(self.w, self.x) + self.b\n",
    "        return out\n",
    "\n",
    "    def backward(self, din):\n",
    "        if isinstance(din, np.ndarray) and din.size == 1:\n",
    "            din = np.asscalar(din)\n",
    "        dx = np.dot(din, self.w.T)\n",
    "        self.dw = np.dot(self.x.T, din)\n",
    "        self.db = din\n",
    "        return dx\n",
    "\n",
    "class AffineWithOneInput:\n",
    "    def __init__(self):\n",
    "        self.w = np.array([random.random()])   # weight of one input\n",
    "        self.b = np.array([random.random()])   # bias\n",
    "        self.x = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(self.w, self.x) + self.b\n",
    "        return out\n",
    "\n",
    "    def backward(self, din):\n",
    "        dx = np.dot(din, self.w.T)\n",
    "        self.dw = np.dot(self.x.T, din)\n",
    "        self.db = din\n",
    "        return dx\n",
    "    \n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        mask = (self.x <= 0)\n",
    "        out = self.x.copy()\n",
    "        out[mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, din):\n",
    "        if isinstance(din, np.ndarray):\n",
    "            mask = (self.x <= 0)\n",
    "            din[mask] = 0\n",
    "            dx = din\n",
    "        else:\n",
    "            if self.x <= 0:\n",
    "                dx = 0\n",
    "            else:\n",
    "                dx = din\n",
    "        return dx\n",
    "    \n",
    "class SquaredError:\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "        self.z_target = None\n",
    "    \n",
    "    def forward(self, z, z_target):\n",
    "        self.z = z\n",
    "        self.z_target = z_target\n",
    "        loss = 1.0 / 2.0 * math.pow(self.z - self.z_target, 2)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, din):\n",
    "        dx = (self.z - self.z_target) * din\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network Model of Linear Two Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearTwoNeurons:\n",
    "    def __init__(self):\n",
    "        self.n1 = AffineWithTwoInputs()\n",
    "        self.relu1 = Relu()\n",
    "        self.n2 = AffineWithOneInput()\n",
    "        self.relu2 = Relu()\n",
    "        self.loss = SquaredError()\n",
    "        print(\"Neuron n1 - Initial w: {0}, b: {1}\".format(self.n1.w, self.n1.b))\n",
    "        print(\"Neuron n2 - Initial w: {0}, b: {1}\".format(self.n2.w, self.n2.b))\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        u1 = self.n1.forward(x)\n",
    "        z1 = self.relu1.forward(u1)\n",
    "        u2 = self.n2.forward(z1)\n",
    "        z2 = self.relu2.forward(u2)\n",
    "        return z2\n",
    "    \n",
    "    def backpropagation_gradient(self, x, z_target):\n",
    "        # forward\n",
    "        z2 = self.predict(x)\n",
    "        self.loss.forward(z2, z_target)\n",
    "\n",
    "        # backward\n",
    "        din = 1\n",
    "        din = self.loss.backward(din)\n",
    "        din = self.relu2.backward(din)\n",
    "        din = self.n2.backward(din)\n",
    "        din = self.relu1.backward(din)\n",
    "        self.n1.backward(din)\n",
    "\n",
    "    def learning(self, alpha, x, z_target):\n",
    "        self.backpropagation_gradient(x, z_target)\n",
    "\n",
    "        self.n1.w = self.n1.w - alpha * self.n1.dw\n",
    "        self.n1.b = self.n1.b - alpha * self.n1.db\n",
    "        self.n2.w = self.n2.w - alpha * self.n2.dw\n",
    "        self.n2.b = self.n2.b - alpha * self.n2.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. OR gate with Two Linear Neurons - Learing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron n1 - Initial w: [ 0.259415    0.76477372], b: [ 0.21560677]\n",
      "Neuron n2 - Initial w: [ 0.74739234], b: [ 0.78961367]\n",
      "x: [ 0.  0.], z2: [ 0.95075652], z_target: 0.0, error: 0.45197\n",
      "x: [ 1.  0.], z2: [ 1.14464131], z_target: 1.0, error: 0.01046\n",
      "x: [ 0.  1.], z2: [ 1.52234255], z_target: 1.0, error: 0.13642\n",
      "x: [ 1.  1.], z2: [ 1.71622733], z_target: 1.0, error: 0.25649\n",
      "Epoch   0-Error:0.18529, Neuron n1[w11: 0.25343, w12: 0.75601, b1: 0.19877], Neuron n2[w2: 0.73155, b2: 0.76700]\n",
      "Epoch  20-Error:0.05533, Neuron n1[w11: 0.22806, w12: 0.68833, b1: 0.05451], Neuron n2[w2: 0.62298, b2: 0.55254]\n",
      "Epoch  40-Error:0.04768, Neuron n1[w11: 0.25376, w12: 0.67942, b1: 0.01811], Neuron n2[w2: 0.62071, b2: 0.49380]\n",
      "Epoch  60-Error:0.04477, Neuron n1[w11: 0.28727, w12: 0.68003, b1: 0.00180], Neuron n2[w2: 0.63525, b2: 0.46780]\n",
      "Epoch  80-Error:0.04256, Neuron n1[w11: 0.32022, w12: 0.68104, b1: 0.00086], Neuron n2[w2: 0.65161, b2: 0.44803]\n",
      "Epoch 100-Error:0.04064, Neuron n1[w11: 0.35078, w12: 0.68083, b1: -0.00012], Neuron n2[w2: 0.66670, b2: 0.42900]\n",
      "Epoch 120-Error:0.03900, Neuron n1[w11: 0.37898, w12: 0.67964, b1: 0.00187], Neuron n2[w2: 0.68053, b2: 0.41097]\n",
      "Epoch 140-Error:0.03758, Neuron n1[w11: 0.40488, w12: 0.67769, b1: 0.00133], Neuron n2[w2: 0.69315, b2: 0.39413]\n",
      "Epoch 160-Error:0.03639, Neuron n1[w11: 0.42857, w12: 0.67521, b1: 0.00127], Neuron n2[w2: 0.70465, b2: 0.37864]\n",
      "Epoch 180-Error:0.03541, Neuron n1[w11: 0.45011, w12: 0.67232, b1: 0.00168], Neuron n2[w2: 0.71501, b2: 0.36443]\n",
      "Epoch 200-Error:0.03457, Neuron n1[w11: 0.46962, w12: 0.66918, b1: 0.00006], Neuron n2[w2: 0.72432, b2: 0.35149]\n",
      "Epoch 220-Error:0.03392, Neuron n1[w11: 0.48721, w12: 0.66587, b1: 0.00155], Neuron n2[w2: 0.73262, b2: 0.33977]\n",
      "Epoch 240-Error:0.03337, Neuron n1[w11: 0.50303, w12: 0.66255, b1: 0.00117], Neuron n2[w2: 0.74004, b2: 0.32928]\n",
      "Epoch 260-Error:0.03293, Neuron n1[w11: 0.51723, w12: 0.65926, b1: 0.00142], Neuron n2[w2: 0.74665, b2: 0.31997]\n",
      "Epoch 280-Error:0.03257, Neuron n1[w11: 0.52990, w12: 0.65606, b1: -0.00013], Neuron n2[w2: 0.75248, b2: 0.31165]\n",
      "Epoch 300-Error:0.03229, Neuron n1[w11: 0.54115, w12: 0.65294, b1: 0.00115], Neuron n2[w2: 0.75754, b2: 0.30418]\n",
      "Epoch 320-Error:0.03207, Neuron n1[w11: 0.55120, w12: 0.65002, b1: 0.00072], Neuron n2[w2: 0.76204, b2: 0.29767]\n",
      "Epoch 340-Error:0.03189, Neuron n1[w11: 0.56011, w12: 0.64728, b1: 0.00080], Neuron n2[w2: 0.76599, b2: 0.29196]\n",
      "Epoch 360-Error:0.03176, Neuron n1[w11: 0.56801, w12: 0.64472, b1: 0.00132], Neuron n2[w2: 0.76942, b2: 0.28694]\n",
      "Epoch 380-Error:0.03165, Neuron n1[w11: 0.57502, w12: 0.64238, b1: 0.00010], Neuron n2[w2: 0.77245, b2: 0.28260]\n",
      "Epoch 400-Error:0.03156, Neuron n1[w11: 0.58115, w12: 0.64017, b1: 0.00134], Neuron n2[w2: 0.77499, b2: 0.27868]\n",
      "Epoch 420-Error:0.03150, Neuron n1[w11: 0.58661, w12: 0.63818, b1: 0.00083], Neuron n2[w2: 0.77724, b2: 0.27534]\n",
      "Epoch 440-Error:0.03144, Neuron n1[w11: 0.59142, w12: 0.63638, b1: 0.00064], Neuron n2[w2: 0.77919, b2: 0.27245]\n",
      "Epoch 460-Error:0.03140, Neuron n1[w11: 0.59566, w12: 0.63473, b1: 0.00071], Neuron n2[w2: 0.78086, b2: 0.26991]\n",
      "Epoch 480-Error:0.03137, Neuron n1[w11: 0.59939, w12: 0.63323, b1: 0.00101], Neuron n2[w2: 0.78228, b2: 0.26770]\n",
      "Epoch 500-Error:0.03135, Neuron n1[w11: 0.60270, w12: 0.63192, b1: 0.00158], Neuron n2[w2: 0.78354, b2: 0.26587]\n",
      "Epoch 520-Error:0.03133, Neuron n1[w11: 0.60563, w12: 0.63075, b1: 0.00028], Neuron n2[w2: 0.78465, b2: 0.26431]\n",
      "Epoch 540-Error:0.03132, Neuron n1[w11: 0.60813, w12: 0.62962, b1: 0.00106], Neuron n2[w2: 0.78546, b2: 0.26276]\n",
      "Epoch 560-Error:0.03130, Neuron n1[w11: 0.61046, w12: 0.62874, b1: 0.00018], Neuron n2[w2: 0.78635, b2: 0.26174]\n",
      "Epoch 580-Error:0.03130, Neuron n1[w11: 0.61236, w12: 0.62780, b1: 0.00118], Neuron n2[w2: 0.78686, b2: 0.26049]\n",
      "Epoch 600-Error:0.03129, Neuron n1[w11: 0.61417, w12: 0.62710, b1: 0.00048], Neuron n2[w2: 0.78748, b2: 0.25970]\n",
      "Epoch 620-Error:0.03128, Neuron n1[w11: 0.61576, w12: 0.62648, b1: 0.00198], Neuron n2[w2: 0.78802, b2: 0.25911]\n",
      "Epoch 640-Error:0.03128, Neuron n1[w11: 0.61700, w12: 0.62577, b1: 0.00112], Neuron n2[w2: 0.78821, b2: 0.25811]\n",
      "Epoch 660-Error:0.03128, Neuron n1[w11: 0.61826, w12: 0.62530, b1: 0.00067], Neuron n2[w2: 0.78860, b2: 0.25764]\n",
      "Epoch 680-Error:0.03127, Neuron n1[w11: 0.61938, w12: 0.62489, b1: 0.00032], Neuron n2[w2: 0.78894, b2: 0.25730]\n",
      "Epoch 700-Error:0.03127, Neuron n1[w11: 0.62036, w12: 0.62453, b1: 0.00003], Neuron n2[w2: 0.78921, b2: 0.25703]\n",
      "Epoch 720-Error:0.03127, Neuron n1[w11: 0.62103, w12: 0.62402, b1: 0.00137], Neuron n2[w2: 0.78912, b2: 0.25627]\n",
      "Epoch 740-Error:0.03127, Neuron n1[w11: 0.62179, w12: 0.62373, b1: 0.00104], Neuron n2[w2: 0.78927, b2: 0.25594]\n",
      "Epoch 760-Error:0.03127, Neuron n1[w11: 0.62249, w12: 0.62351, b1: 0.00081], Neuron n2[w2: 0.78943, b2: 0.25575]\n",
      "Epoch 780-Error:0.03127, Neuron n1[w11: 0.62311, w12: 0.62332, b1: 0.00061], Neuron n2[w2: 0.78955, b2: 0.25560]\n",
      "Epoch 800-Error:0.03127, Neuron n1[w11: 0.62366, w12: 0.62315, b1: 0.00045], Neuron n2[w2: 0.78964, b2: 0.25549]\n",
      "Epoch 820-Error:0.03127, Neuron n1[w11: 0.62415, w12: 0.62301, b1: 0.00030], Neuron n2[w2: 0.78969, b2: 0.25541]\n",
      "Epoch 840-Error:0.03127, Neuron n1[w11: 0.62458, w12: 0.62289, b1: 0.00017], Neuron n2[w2: 0.78972, b2: 0.25534]\n",
      "Epoch 860-Error:0.03127, Neuron n1[w11: 0.62496, w12: 0.62278, b1: 0.00006], Neuron n2[w2: 0.78973, b2: 0.25530]\n",
      "Epoch 880-Error:0.03127, Neuron n1[w11: 0.62516, w12: 0.62255, b1: 0.00166], Neuron n2[w2: 0.78949, b2: 0.25488]\n",
      "Epoch 900-Error:0.03127, Neuron n1[w11: 0.62542, w12: 0.62242, b1: 0.00135], Neuron n2[w2: 0.78937, b2: 0.25458]\n",
      "Epoch 920-Error:0.03127, Neuron n1[w11: 0.62572, w12: 0.62239, b1: 0.00120], Neuron n2[w2: 0.78937, b2: 0.25450]\n",
      "Epoch 940-Error:0.03127, Neuron n1[w11: 0.62601, w12: 0.62238, b1: 0.00109], Neuron n2[w2: 0.78937, b2: 0.25445]\n",
      "Epoch 960-Error:0.03127, Neuron n1[w11: 0.62626, w12: 0.62237, b1: 0.00099], Neuron n2[w2: 0.78936, b2: 0.25442]\n",
      "Epoch 980-Error:0.03127, Neuron n1[w11: 0.62649, w12: 0.62238, b1: 0.00089], Neuron n2[w2: 0.78933, b2: 0.25441]\n",
      "Epoch1000-Error:0.03127, Neuron n1[w11: 0.62671, w12: 0.62239, b1: 0.00081], Neuron n2[w2: 0.78928, b2: 0.25440]\n",
      "x: [ 0.  0.], z2: [ 0.25503965], z_target: 0.0, error: 0.03252\n",
      "x: [ 1.  0.], z2: [ 0.74968931], z_target: 1.0, error: 0.03133\n",
      "x: [ 0.  1.], z2: [ 0.74627903], z_target: 1.0, error: 0.03219\n",
      "x: [ 1.  1.], z2: [ 1.2409287], z_target: 1.0, error: 0.02902\n"
     ]
    }
   ],
   "source": [
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.training_input_value = np.array([(0.0, 0.0), (1.0, 0.0), (0.0, 1.0), (1.0, 1.0)])\n",
    "        self.training_z_target = np.array([0.0, 1.0, 1.0, 1.0])\n",
    "        self.numTrainData = len(self.training_input_value)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ltn = LinearTwoNeurons()\n",
    "    d = Data()\n",
    "    for idx in range(d.numTrainData):\n",
    "        x = d.training_input_value[idx]\n",
    "        z2 = ltn.predict(x)\n",
    "        z_target = d.training_z_target[idx]\n",
    "        error = ltn.loss.forward(z2, z_target)\n",
    "        print(\"x: {0:s}, z2: {1:s}, z_target: {2:s}, error: {3:7.5f}\".format(str(x), str(z2), str(z_target), error))\n",
    "\n",
    "    max_epoch = 1000\n",
    "    print_epoch_period = 20\n",
    "    for i in range(max_epoch + 1):\n",
    "        for idx in range(d.numTrainData):\n",
    "            x = d.training_input_value[idx]\n",
    "            z_target = d.training_z_target[idx]\n",
    "            ltn.learning(0.01, x, z_target)\n",
    "\n",
    "        if i % print_epoch_period == 0:\n",
    "            sum = 0.0\n",
    "            for idx in range(d.numTrainData):\n",
    "                x = d.training_input_value[idx]\n",
    "                z2 = ltn.predict(x)\n",
    "                z_target = d.training_z_target[idx]\n",
    "                sum = sum + ltn.loss.forward(z2, z_target)\n",
    "\n",
    "            print(\"Epoch{0:4d}-Error:{1:7.5f}, Neuron n1[w11: {2:7.5f}, w12: {3:7.5f}, b1: {4:7.5f}], Neuron n2[w2: {5:7.5f}, b2: {6:7.5f}]\".format(\n",
    "                i, \n",
    "                sum / d.numTrainData,\n",
    "                ltn.n1.w[0],\n",
    "                ltn.n1.w[1],\n",
    "                ltn.n1.b[0],\n",
    "                ltn.n2.w[0],\n",
    "                ltn.n2.b[0])\n",
    "            )\n",
    "            \n",
    "    for idx in range(d.numTrainData):\n",
    "        x = d.training_input_value[idx]\n",
    "        z2 = ltn.predict(x)\n",
    "        z_target = d.training_z_target[idx]\n",
    "        error = ltn.loss.forward(z2, z_target)\n",
    "        print(\"x: {0:s}, z2: {1:s}, z_target: {2:s}, error: {3:7.5f}\".format(str(x), str(z2), str(z_target), error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Neural Network Model of Three Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ThreeNeurons:\n",
    "    def __init__(self):\n",
    "        self.n1 = AffineWithTwoInputs()\n",
    "        self.relu1 = Relu()\n",
    "        self.n2 = AffineWithTwoInputs()\n",
    "        self.relu2 = Relu()\n",
    "        self.n3 = AffineWithTwoInputs()\n",
    "        self.relu3 = Relu()\n",
    "        self.loss = SquaredError()\n",
    "\n",
    "    def predict(self, x):\n",
    "        u1 = self.n1.forward(x)\n",
    "        z1 = self.relu1.forward(u1)\n",
    "        u2 = self.n2.forward(x)\n",
    "        z2 = self.relu2.forward(u2)\n",
    "        z  = np.array([np.asscalar(z1), np.asscalar(z2)])\n",
    "        u3 = self.n3.forward(z)\n",
    "        z3 = self.relu3.forward(u3)\n",
    "        return z3\n",
    "    \n",
    "    def backpropagation_gradient(self, x, z_target):\n",
    "        # forward\n",
    "        z3 = self.predict(x)\n",
    "        self.loss.forward(z3, z_target)\n",
    "\n",
    "        # backward\n",
    "        din = 1\n",
    "        din = self.loss.backward(din)\n",
    "        \n",
    "        din = self.relu3.backward(din)\n",
    "        din = self.n3.backward(din)\n",
    "        \n",
    "        din_0 = self.relu1.backward(din[0])\n",
    "        self.n1.backward(din_0)\n",
    "        \n",
    "        din_1 = self.relu2.backward(din[1])\n",
    "        self.n2.backward(din_1)\n",
    "\n",
    "    def learning(self, alpha, x, z_target):\n",
    "        self.backpropagation_gradient(x, z_target)\n",
    "\n",
    "        self.n1.w = self.n1.w - alpha * self.n1.dw\n",
    "        self.n1.b = self.n1.b - alpha * self.n1.db\n",
    "        self.n2.w = self.n2.w - alpha * self.n2.dw\n",
    "        self.n2.b = self.n2.b - alpha * self.n2.db\n",
    "        self.n3.w = self.n3.w - alpha * self.n3.dw\n",
    "        self.n3.b = self.n3.b - alpha * self.n3.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. OR gate with Three Neurons - Learing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [ 0.  0.], z3: [ 1.15209077], z_target: 0.0, error: 0.66366\n",
      "x: [ 1.  0.], z3: [ 1.68362009], z_target: 1.0, error: 0.23367\n",
      "x: [ 0.  1.], z3: [ 1.56569511], z_target: 1.0, error: 0.16001\n",
      "x: [ 1.  1.], z3: [ 2.09722443], z_target: 1.0, error: 0.60195\n",
      "   0-Err: 0.3046, n1[w:[ 0.47  0.37],b:[ 0.48]], n2[w:[ 0.43  0.29],b:[ 0.69]], n3[w:[ 0.87  0.2 ],b:[ 0.5]]\n",
      "  20-Err: 0.0464, n1[w:[ 0.41  0.33],b:[ 0.32]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.73  0.01],b:[ 0.3]]\n",
      "  40-Err: 0.0428, n1[w:[ 0.43  0.35],b:[ 0.3]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.74  0.01],b:[ 0.26]]\n",
      "  60-Err: 0.0403, n1[w:[ 0.44  0.38],b:[ 0.28]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.76  0.01],b:[ 0.24]]\n",
      "  80-Err: 0.0384, n1[w:[ 0.46  0.4 ],b:[ 0.27]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.77  0.02],b:[ 0.22]]\n",
      " 100-Err: 0.0368, n1[w:[ 0.47  0.42],b:[ 0.25]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.79  0.02],b:[ 0.2]]\n",
      " 120-Err: 0.0355, n1[w:[ 0.48  0.44],b:[ 0.24]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.8   0.02],b:[ 0.19]]\n",
      " 140-Err: 0.0345, n1[w:[ 0.49  0.45],b:[ 0.23]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.81  0.02],b:[ 0.17]]\n",
      " 160-Err: 0.0337, n1[w:[ 0.5   0.46],b:[ 0.22]], n2[w:[ 0.43  0.28],b:[ 0.67]], n3[w:[ 0.82  0.02],b:[ 0.16]]\n",
      " 180-Err: 0.0331, n1[w:[ 0.51  0.48],b:[ 0.21]], n2[w:[ 0.43  0.28],b:[ 0.67]], n3[w:[ 0.83  0.02],b:[ 0.15]]\n",
      " 200-Err: 0.0327, n1[w:[ 0.52  0.49],b:[ 0.21]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.84  0.02],b:[ 0.14]]\n",
      " 220-Err: 0.0323, n1[w:[ 0.52  0.49],b:[ 0.2]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.84  0.03],b:[ 0.14]]\n",
      " 240-Err: 0.0321, n1[w:[ 0.53  0.5 ],b:[ 0.19]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.85  0.03],b:[ 0.13]]\n",
      " 260-Err: 0.0319, n1[w:[ 0.53  0.51],b:[ 0.19]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.85  0.03],b:[ 0.12]]\n",
      " 280-Err: 0.0317, n1[w:[ 0.53  0.51],b:[ 0.18]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.86  0.03],b:[ 0.12]]\n",
      " 300-Err: 0.0316, n1[w:[ 0.54  0.52],b:[ 0.18]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.86  0.03],b:[ 0.12]]\n",
      " 320-Err: 0.0315, n1[w:[ 0.54  0.52],b:[ 0.18]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.86  0.03],b:[ 0.11]]\n",
      " 340-Err: 0.0315, n1[w:[ 0.54  0.53],b:[ 0.18]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.87  0.03],b:[ 0.11]]\n",
      " 360-Err: 0.0314, n1[w:[ 0.54  0.53],b:[ 0.17]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.87  0.03],b:[ 0.11]]\n",
      " 380-Err: 0.0314, n1[w:[ 0.54  0.53],b:[ 0.17]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      " 400-Err: 0.0314, n1[w:[ 0.54  0.53],b:[ 0.17]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      " 420-Err: 0.0314, n1[w:[ 0.54  0.54],b:[ 0.17]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      " 440-Err: 0.0313, n1[w:[ 0.55  0.54],b:[ 0.17]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      " 460-Err: 0.0313, n1[w:[ 0.55  0.54],b:[ 0.17]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      " 480-Err: 0.0313, n1[w:[ 0.55  0.54],b:[ 0.17]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      " 500-Err: 0.0313, n1[w:[ 0.55  0.54],b:[ 0.17]], n2[w:[ 0.42  0.28],b:[ 0.67]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      " 520-Err: 0.0313, n1[w:[ 0.55  0.54],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      " 540-Err: 0.0313, n1[w:[ 0.55  0.54],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.88  0.03],b:[ 0.1]]\n",
      " 560-Err: 0.0313, n1[w:[ 0.55  0.54],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.88  0.03],b:[ 0.1]]\n",
      " 580-Err: 0.0313, n1[w:[ 0.55  0.54],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.88  0.03],b:[ 0.1]]\n",
      " 600-Err: 0.0313, n1[w:[ 0.55  0.54],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.88  0.03],b:[ 0.1]]\n",
      " 620-Err: 0.0313, n1[w:[ 0.55  0.54],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.88  0.03],b:[ 0.1]]\n",
      " 640-Err: 0.0313, n1[w:[ 0.55  0.54],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.88  0.03],b:[ 0.1]]\n",
      " 660-Err: 0.0313, n1[w:[ 0.55  0.54],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.88  0.03],b:[ 0.1]]\n",
      " 680-Err: 0.0313, n1[w:[ 0.55  0.54],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.88  0.03],b:[ 0.1]]\n",
      " 700-Err: 0.0313, n1[w:[ 0.55  0.54],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.88  0.03],b:[ 0.1]]\n",
      " 720-Err: 0.0313, n1[w:[ 0.55  0.54],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.88  0.03],b:[ 0.1]]\n",
      " 740-Err: 0.0313, n1[w:[ 0.55  0.54],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.88  0.03],b:[ 0.1]]\n",
      " 760-Err: 0.0313, n1[w:[ 0.55  0.54],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.88  0.03],b:[ 0.1]]\n",
      " 780-Err: 0.0313, n1[w:[ 0.55  0.54],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      " 800-Err: 0.0313, n1[w:[ 0.55  0.55],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      " 820-Err: 0.0313, n1[w:[ 0.55  0.55],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      " 840-Err: 0.0313, n1[w:[ 0.55  0.55],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      " 860-Err: 0.0313, n1[w:[ 0.55  0.55],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      " 880-Err: 0.0313, n1[w:[ 0.55  0.55],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      " 900-Err: 0.0313, n1[w:[ 0.55  0.55],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      " 920-Err: 0.0313, n1[w:[ 0.55  0.55],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      " 940-Err: 0.0313, n1[w:[ 0.55  0.55],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      " 960-Err: 0.0313, n1[w:[ 0.55  0.55],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      " 980-Err: 0.0313, n1[w:[ 0.55  0.55],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      "1000-Err: 0.0313, n1[w:[ 0.55  0.55],b:[ 0.16]], n2[w:[ 0.42  0.28],b:[ 0.66]], n3[w:[ 0.87  0.03],b:[ 0.1]]\n",
      "x: [ 0.  0.], z3: [ 0.25662812], z_target: 0.0, error: 0.03293\n",
      "x: [ 1.  0.], z3: [ 0.75100328], z_target: 1.0, error: 0.03100\n",
      "x: [ 0.  1.], z3: [ 0.74275624], z_target: 1.0, error: 0.03309\n",
      "x: [ 1.  1.], z3: [ 1.23713139], z_target: 1.0, error: 0.02812\n"
     ]
    }
   ],
   "source": [
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.training_input_value = np.array([(0.0, 0.0), (1.0, 0.0), (0.0, 1.0), (1.0, 1.0)])\n",
    "        self.training_z_target = np.array([0.0, 1.0, 1.0, 1.0])\n",
    "        self.numTrainData = len(self.training_input_value)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tn = ThreeNeurons()\n",
    "    d = Data()\n",
    "    for idx in range(d.numTrainData):\n",
    "        x = d.training_input_value[idx]\n",
    "        z3 = tn.predict(x)\n",
    "        z_target = d.training_z_target[idx]\n",
    "        error = tn.loss.forward(z3, z_target)\n",
    "        print(\"x: {0:s}, z3: {1:s}, z_target: {2:s}, error: {3:7.5f}\".format(str(x), str(z3), str(z_target), error))\n",
    "\n",
    "    max_epoch = 1000\n",
    "    print_epoch_period = 20\n",
    "    for i in range(max_epoch + 1):\n",
    "        for idx in range(d.numTrainData):\n",
    "            x = d.training_input_value[idx]\n",
    "            z_target = d.training_z_target[idx]\n",
    "            tn.learning(0.01, x, z_target)\n",
    "\n",
    "        if i % print_epoch_period == 0:\n",
    "            sum = 0.0\n",
    "            for idx in range(d.numTrainData):\n",
    "                x = d.training_input_value[idx]\n",
    "                z3 = tn.predict(x)\n",
    "                z_target = d.training_z_target[idx]\n",
    "                sum = sum + tn.loss.forward(z3, z_target)\n",
    "\n",
    "            print(\"{0:4d}-Err:{1:7.4f}, n1[w:{2:s},b:{3:s}], n2[w:{4:s},b:{5:s}], n3[w:{6:},b:{7:s}]\".format(\n",
    "                i, \n",
    "                sum / d.numTrainData,\n",
    "                np.array_str(tn.n1.w, precision=2),\n",
    "                np.array_str(tn.n1.b, precision=2),\n",
    "                np.array_str(tn.n2.w, precision=2),\n",
    "                np.array_str(tn.n2.b, precision=2),\n",
    "                np.array_str(tn.n3.w, precision=2),\n",
    "                np.array_str(tn.n3.b, precision=2))\n",
    "            )\n",
    "            \n",
    "    for idx in range(d.numTrainData):\n",
    "        x = d.training_input_value[idx]\n",
    "        z3 = tn.predict(x)\n",
    "        z_target = d.training_z_target[idx]\n",
    "        error = tn.loss.forward(z3, z_target)\n",
    "        print(\"x: {0:s}, z3: {1:s}, z_target: {2:s}, error: {3:7.5f}\".format(str(x), str(z3), str(z_target), error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6. XOR gate with Three Neurons - Learing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [ 0.  0.], z3: [ 1.04199236], z_target: 0.0, error: 0.54287\n",
      "x: [ 1.  0.], z3: [ 2.1597136], z_target: 1.0, error: 0.67247\n",
      "x: [ 0.  1.], z3: [ 1.92813386], z_target: 1.0, error: 0.43072\n",
      "x: [ 1.  1.], z3: [ 3.04585511], z_target: 0.0, error: 4.63862\n",
      "   0-Err: 1.1391, n1[w:[ 0.59  0.84],b:[-0.03]], n2[w:[ 0.61  0.06],b:[ 0.18]], n3[w:[ 0.86  0.8 ],b:[ 0.77]]\n",
      "  40-Err: 0.1263, n1[w:[ 0.34  0.62],b:[-0.22]], n2[w:[ 0.36 -0.25],b:[-0.12]], n3[w:[ 0.47  0.65],b:[ 0.33]]\n",
      "  80-Err: 0.1116, n1[w:[ 0.3   0.59],b:[-0.17]], n2[w:[ 0.4  -0.32],b:[-0.08]], n3[w:[ 0.39  0.7 ],b:[ 0.32]]\n",
      " 120-Err: 0.1010, n1[w:[ 0.27  0.58],b:[-0.13]], n2[w:[ 0.44 -0.4 ],b:[-0.05]], n3[w:[ 0.33  0.76],b:[ 0.32]]\n",
      " 160-Err: 0.0931, n1[w:[ 0.24  0.58],b:[-0.09]], n2[w:[ 0.47 -0.46],b:[-0.01]], n3[w:[ 0.29  0.81],b:[ 0.31]]\n",
      " 200-Err: 0.0877, n1[w:[ 0.21  0.59],b:[-0.06]], n2[w:[ 0.51 -0.51],b:[-0.]], n3[w:[ 0.26  0.86],b:[ 0.3]]\n",
      " 240-Err: 0.0837, n1[w:[ 0.18  0.59],b:[-0.03]], n2[w:[ 0.54 -0.55],b:[ -1.32e-06]], n3[w:[ 0.25  0.9 ],b:[ 0.29]]\n",
      " 280-Err: 0.0808, n1[w:[ 0.15  0.6 ],b:[-0.01]], n2[w:[ 0.57 -0.58],b:[-0.]], n3[w:[ 0.25  0.94],b:[ 0.28]]\n",
      " 320-Err: 0.0781, n1[w:[ 0.12  0.61],b:[ 0.]], n2[w:[ 0.6 -0.6],b:[-0.]], n3[w:[ 0.25  0.97],b:[ 0.27]]\n",
      " 360-Err: 0.0760, n1[w:[ 0.08  0.62],b:[ 0.]], n2[w:[ 0.62 -0.63],b:[-0.]], n3[w:[ 0.27  1.  ],b:[ 0.26]]\n",
      " 400-Err: 0.0736, n1[w:[ 0.04  0.64],b:[ 0.]], n2[w:[ 0.64 -0.65],b:[-0.]], n3[w:[ 0.29  1.02],b:[ 0.25]]\n",
      " 440-Err: 0.0710, n1[w:[-0.    0.65],b:[ -2.74e-05]], n2[w:[ 0.66 -0.67],b:[-0.]], n3[w:[ 0.32  1.04],b:[ 0.23]]\n",
      " 480-Err: 0.0671, n1[w:[-0.06  0.66],b:[-0.]], n2[w:[ 0.68 -0.68],b:[ 0.]], n3[w:[ 0.35  1.06],b:[ 0.22]]\n",
      " 520-Err: 0.0626, n1[w:[-0.13  0.68],b:[ 0.]], n2[w:[ 0.69 -0.69],b:[-0.]], n3[w:[ 0.4   1.08],b:[ 0.2]]\n",
      " 560-Err: 0.0569, n1[w:[-0.2   0.69],b:[ 0.]], n2[w:[ 0.7 -0.7],b:[-0.]], n3[w:[ 0.45  1.1 ],b:[ 0.18]]\n",
      " 600-Err: 0.0502, n1[w:[-0.28  0.71],b:[-0.]], n2[w:[ 0.71 -0.72],b:[-0.]], n3[w:[ 0.51  1.11],b:[ 0.17]]\n",
      " 640-Err: 0.0425, n1[w:[-0.36  0.73],b:[-0.]], n2[w:[ 0.72 -0.72],b:[ 0.]], n3[w:[ 0.58  1.12],b:[ 0.15]]\n",
      " 680-Err: 0.0341, n1[w:[-0.45  0.74],b:[  8.36e-05]], n2[w:[ 0.73 -0.74],b:[-0.]], n3[w:[ 0.65  1.13],b:[ 0.13]]\n",
      " 720-Err: 0.0256, n1[w:[-0.53  0.75],b:[ 0.]], n2[w:[ 0.74 -0.74],b:[-0.]], n3[w:[ 0.73  1.15],b:[ 0.12]]\n",
      " 760-Err: 0.0179, n1[w:[-0.61  0.77],b:[-0.]], n2[w:[ 0.75 -0.75],b:[-0.]], n3[w:[ 0.8   1.16],b:[ 0.11]]\n",
      " 800-Err: 0.0117, n1[w:[-0.68  0.78],b:[-0.]], n2[w:[ 0.76 -0.76],b:[ -9.74e-05]], n3[w:[ 0.87  1.16],b:[ 0.1]]\n",
      " 840-Err: 0.0072, n1[w:[-0.74  0.8 ],b:[ 0.]], n2[w:[ 0.76 -0.76],b:[-0.]], n3[w:[ 0.93  1.17],b:[ 0.09]]\n",
      " 880-Err: 0.0043, n1[w:[-0.79  0.81],b:[  3.26e-05]], n2[w:[ 0.77 -0.77],b:[-0.]], n3[w:[ 0.97  1.18],b:[ 0.08]]\n",
      " 920-Err: 0.0024, n1[w:[-0.82  0.82],b:[-0.]], n2[w:[ 0.77 -0.77],b:[ -6.36e-05]], n3[w:[ 1.01  1.18],b:[ 0.07]]\n",
      " 960-Err: 0.0015, n1[w:[-0.84  0.84],b:[  2.04e-05]], n2[w:[ 0.78 -0.78],b:[-0.]], n3[w:[ 1.04  1.19],b:[ 0.06]]\n",
      "1000-Err: 0.0009, n1[w:[-0.85  0.85],b:[  2.02e-06]], n2[w:[ 0.78 -0.78],b:[-0.]], n3[w:[ 1.06  1.2 ],b:[ 0.05]]\n",
      "1040-Err: 0.0006, n1[w:[-0.86  0.86],b:[-0.]], n2[w:[ 0.79 -0.79],b:[-0.]], n3[w:[ 1.08  1.2 ],b:[ 0.04]]\n",
      "1080-Err: 0.0004, n1[w:[-0.87  0.87],b:[ 0.]], n2[w:[ 0.79 -0.79],b:[-0.]], n3[w:[ 1.09  1.21],b:[ 0.03]]\n",
      "1120-Err: 0.0002, n1[w:[-0.87  0.87],b:[  6.79e-05]], n2[w:[ 0.79 -0.79],b:[ -3.43e-05]], n3[w:[ 1.1   1.21],b:[ 0.02]]\n",
      "1160-Err: 0.0001, n1[w:[-0.88  0.88],b:[  5.32e-05]], n2[w:[ 0.8 -0.8],b:[ -6.98e-05]], n3[w:[ 1.1   1.22],b:[ 0.02]]\n",
      "1200-Err: 0.0001, n1[w:[-0.88  0.88],b:[-0.]], n2[w:[ 0.8 -0.8],b:[-0.]], n3[w:[ 1.11  1.22],b:[ 0.02]]\n",
      "1240-Err: 0.0001, n1[w:[-0.88  0.88],b:[ -6.32e-05]], n2[w:[ 0.8 -0.8],b:[ -7.99e-05]], n3[w:[ 1.11  1.22],b:[ 0.01]]\n",
      "1280-Err: 0.0000, n1[w:[-0.88  0.88],b:[ -5.57e-05]], n2[w:[ 0.8 -0.8],b:[-0.]], n3[w:[ 1.11  1.23],b:[ 0.01]]\n",
      "1320-Err: 0.0000, n1[w:[-0.89  0.89],b:[ -6.80e-06]], n2[w:[ 0.8 -0.8],b:[ -1.09e-05]], n3[w:[ 1.11  1.23],b:[ 0.01]]\n",
      "1360-Err: 0.0000, n1[w:[-0.89  0.89],b:[ -6.99e-05]], n2[w:[ 0.81 -0.81],b:[  2.50e-07]], n3[w:[ 1.12  1.23],b:[ 0.01]]\n",
      "1400-Err: 0.0000, n1[w:[-0.89  0.89],b:[ -6.67e-05]], n2[w:[ 0.81 -0.81],b:[ -3.58e-05]], n3[w:[ 1.12  1.23],b:[ 0.]]\n",
      "1440-Err: 0.0000, n1[w:[-0.89  0.89],b:[  1.61e-05]], n2[w:[ 0.81 -0.81],b:[ -2.64e-06]], n3[w:[ 1.12  1.23],b:[ 0.]]\n",
      "1480-Err: 0.0000, n1[w:[-0.89  0.89],b:[ -7.06e-06]], n2[w:[ 0.81 -0.81],b:[ -1.58e-05]], n3[w:[ 1.12  1.23],b:[ 0.]]\n",
      "1520-Err: 0.0000, n1[w:[-0.89  0.89],b:[  4.15e-06]], n2[w:[ 0.81 -0.81],b:[  9.39e-06]], n3[w:[ 1.12  1.23],b:[ 0.]]\n",
      "1560-Err: 0.0000, n1[w:[-0.89  0.89],b:[ -1.12e-05]], n2[w:[ 0.81 -0.81],b:[  3.68e-06]], n3[w:[ 1.12  1.23],b:[ 0.]]\n",
      "1600-Err: 0.0000, n1[w:[-0.89  0.89],b:[ -5.30e-06]], n2[w:[ 0.81 -0.81],b:[  1.52e-06]], n3[w:[ 1.12  1.23],b:[ 0.]]\n",
      "1640-Err: 0.0000, n1[w:[-0.89  0.89],b:[ -1.55e-05]], n2[w:[ 0.81 -0.81],b:[ -1.20e-06]], n3[w:[ 1.12  1.23],b:[ 0.]]\n",
      "1680-Err: 0.0000, n1[w:[-0.89  0.89],b:[ -1.45e-06]], n2[w:[ 0.81 -0.81],b:[ -1.33e-05]], n3[w:[ 1.12  1.23],b:[ 0.]]\n",
      "1720-Err: 0.0000, n1[w:[-0.89  0.89],b:[ -6.83e-07]], n2[w:[ 0.81 -0.81],b:[ -2.86e-06]], n3[w:[ 1.12  1.23],b:[ 0.]]\n",
      "1760-Err: 0.0000, n1[w:[-0.89  0.89],b:[  6.45e-08]], n2[w:[ 0.81 -0.81],b:[  3.66e-06]], n3[w:[ 1.12  1.23],b:[ 0.]]\n",
      "1800-Err: 0.0000, n1[w:[-0.89  0.89],b:[  4.61e-07]], n2[w:[ 0.81 -0.81],b:[ -2.55e-06]], n3[w:[ 1.12  1.23],b:[ 0.]]\n",
      "1840-Err: 0.0000, n1[w:[-0.89  0.89],b:[  6.22e-07]], n2[w:[ 0.81 -0.81],b:[ -2.25e-06]], n3[w:[ 1.12  1.23],b:[ 0.]]\n",
      "1880-Err: 0.0000, n1[w:[-0.89  0.89],b:[  8.95e-07]], n2[w:[ 0.81 -0.81],b:[ -1.96e-06]], n3[w:[ 1.12  1.23],b:[ 0.]]\n",
      "1920-Err: 0.0000, n1[w:[-0.89  0.89],b:[  6.92e-07]], n2[w:[ 0.81 -0.81],b:[  7.34e-07]], n3[w:[ 1.12  1.23],b:[ 0.]]\n",
      "1960-Err: 0.0000, n1[w:[-0.89  0.89],b:[ -1.29e-06]], n2[w:[ 0.81 -0.81],b:[  3.38e-07]], n3[w:[ 1.12  1.23],b:[ 0.]]\n",
      "2000-Err: 0.0000, n1[w:[-0.89  0.89],b:[ -7.88e-07]], n2[w:[ 0.81 -0.81],b:[  2.55e-07]], n3[w:[ 1.12  1.23],b:[ 0.]]\n",
      "x: [ 0.  0.], z3: [ 0.00012949], z_target: 0.0, error: 0.00000\n",
      "x: [ 1.  0.], z3: [ 0.99991152], z_target: 1.0, error: 0.00000\n",
      "x: [ 0.  1.], z3: [ 0.99990818], z_target: 1.0, error: 0.00000\n",
      "x: [ 1.  1.], z3: [ 0.00012917], z_target: 0.0, error: 0.00000\n"
     ]
    }
   ],
   "source": [
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.training_input_value = np.array([(0.0, 0.0), (1.0, 0.0), (0.0, 1.0), (1.0, 1.0)])\n",
    "        self.training_z_target = np.array([0.0, 1.0, 1.0, 0.0])\n",
    "        self.numTrainData = len(self.training_input_value)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tn = ThreeNeurons()\n",
    "    d = Data()\n",
    "    for idx in range(d.numTrainData):\n",
    "        x = d.training_input_value[idx]\n",
    "        z3 = tn.predict(x)\n",
    "        z_target = d.training_z_target[idx]\n",
    "        error = tn.loss.forward(z3, z_target)\n",
    "        print(\"x: {0:s}, z3: {1:s}, z_target: {2:s}, error: {3:7.5f}\".format(str(x), str(z3), str(z_target), error))\n",
    "\n",
    "    max_epoch = 2000\n",
    "    print_epoch_period = 40\n",
    "    for i in range(max_epoch + 1):\n",
    "        for idx in range(d.numTrainData):\n",
    "            x = d.training_input_value[idx]\n",
    "            z_target = d.training_z_target[idx]\n",
    "            tn.learning(0.01, x, z_target)\n",
    "\n",
    "        if i % print_epoch_period == 0:\n",
    "            sum = 0.0\n",
    "            for idx in range(d.numTrainData):\n",
    "                x = d.training_input_value[idx]\n",
    "                z3 = tn.predict(x)\n",
    "                z_target = d.training_z_target[idx]\n",
    "                sum = sum + tn.loss.forward(z3, z_target)\n",
    "\n",
    "            print(\"{0:4d}-Err:{1:7.4f}, n1[w:{2:s},b:{3:s}], n2[w:{4:s},b:{5:s}], n3[w:{6:},b:{7:s}]\".format(\n",
    "                i, \n",
    "                sum / d.numTrainData,\n",
    "                np.array_str(tn.n1.w, precision=2),\n",
    "                np.array_str(tn.n1.b, precision=2),\n",
    "                np.array_str(tn.n2.w, precision=2),\n",
    "                np.array_str(tn.n2.b, precision=2),\n",
    "                np.array_str(tn.n3.w, precision=2),\n",
    "                np.array_str(tn.n3.b, precision=2))\n",
    "            )\n",
    "            \n",
    "    for idx in range(d.numTrainData):\n",
    "        x = d.training_input_value[idx]\n",
    "        z3 = tn.predict(x)\n",
    "        z_target = d.training_z_target[idx]\n",
    "        error = tn.loss.forward(z3, z_target)\n",
    "        print(\"x: {0:s}, z3: {1:s}, z_target: {2:s}, error: {3:7.5f}\".format(str(x), str(z3), str(z_target), error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
