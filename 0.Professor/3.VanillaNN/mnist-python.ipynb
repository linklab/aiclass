{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST - Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.MNIST 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yhhan/git/aiclass/0.Professor/3.VanillaNN/MNIST_data/train-images-idx3-ubyte.gz\n",
      "/Users/yhhan/git/aiclass/0.Professor/3.VanillaNN/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "/Users/yhhan/git/aiclass/0.Professor/3.VanillaNN/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "/Users/yhhan/git/aiclass/0.Professor/3.VanillaNN/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import urllib.request\n",
    "import os.path\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
    "key_file = {\n",
    "    'train_img':'train-images-idx3-ubyte.gz',\n",
    "    'train_label':'train-labels-idx1-ubyte.gz',\n",
    "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
    "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
    "}\n",
    "\n",
    "dataset_dir = os.path.dirname(\"/Users/yhhan/git/aiclass/0.Professor/3.VanillaNN/MNIST_data/.\")\n",
    "save_file = dataset_dir + \"/mnist.pkl\"\n",
    "\n",
    "train_num = 60000\n",
    "test_num = 10000\n",
    "img_dim = (1, 28, 28)\n",
    "img_size = 784\n",
    "\n",
    "def _download(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    print(file_path)\n",
    "    if os.path.exists(file_path):\n",
    "        return\n",
    "\n",
    "    print(\"Downloading \" + file_name + \" ... \")\n",
    "    urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "    print(\"Done\")\n",
    "    \n",
    "def download_mnist():\n",
    "    for v in key_file.values():\n",
    "       _download(v)\n",
    "        \n",
    "def _load_label(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def _load_img(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    print(\"Converting \" + file_name + \" to NumPy Array ...\")    \n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    data = data.reshape(-1, img_size)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def _convert_numpy():\n",
    "    dataset = {}\n",
    "    dataset['train_img'] =  _load_img(key_file['train_img'])\n",
    "    dataset['train_label'] = _load_label(key_file['train_label'])\n",
    "    dataset['test_img'] = _load_img(key_file['test_img'])\n",
    "    dataset['test_label'] = _load_label(key_file['test_label'])\n",
    "\n",
    "    dataset['validation_img'] = dataset['train_img'][55000:]\n",
    "    dataset['validation_label'] = dataset['train_label'][55000:]\n",
    "    dataset['train_img'] =  dataset['train_img'][:55000]\n",
    "    dataset['train_label'] = dataset['train_label'][:55000]\n",
    "    return dataset\n",
    "\n",
    "def init_mnist():\n",
    "    download_mnist()\n",
    "    dataset = _convert_numpy()\n",
    "    print(\"Creating pickle file ...\")\n",
    "    with open(save_file, 'wb') as f:\n",
    "        pickle.dump(dataset, f, -1)\n",
    "    print(\"Done!\")\n",
    "\n",
    "def _change_one_hot_label(X):\n",
    "    T = np.zeros((X.size, 10))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "    \n",
    "\n",
    "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
    "    if not os.path.exists(save_file):\n",
    "        init_mnist()\n",
    "        \n",
    "    with open(save_file, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    \n",
    "    if normalize:\n",
    "        for key in ('train_img', 'validation_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].astype(np.float32)\n",
    "            dataset[key] /= 255.0\n",
    "            \n",
    "    if one_hot_label:\n",
    "        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
    "        dataset['validation_label'] = _change_one_hot_label(dataset['validation_label'])\n",
    "        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n",
    "    \n",
    "    if not flatten:\n",
    "         for key in ('train_img', 'validation_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
    "\n",
    "    return (dataset['train_img'], dataset['train_label']), (dataset['validation_img'], dataset['validation_label']), (dataset['test_img'], dataset['test_label']) \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init_mnist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 2층 신경망을 이용한 MNIST 학습 및 모델 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    mask = (x <= 0)\n",
    "    out = x.copy()\n",
    "    out[mask] = 0\n",
    "    return out\n",
    "\n",
    "def softmax(x):\n",
    "    c = np.max(x)\n",
    "    exp_x = np.exp(x-c)\n",
    "    sum_exp_x = np.sum(exp_x)\n",
    "    y = exp_x / sum_exp_x\n",
    "    return y\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_layer1_size, hidden_layer2_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_layer1_size)\n",
    "        self.params['b1'] = np.zeros(hidden_layer1_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_layer1_size, hidden_layer2_size)\n",
    "        self.params['b2'] = np.zeros(hidden_layer2_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_layer2_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        print(\"W1-shape: {0}, b1-shape: {1}, W2-shape: {2}, b2-shape: {3}, W3-shape: {4}, b3-shape: {5}\".format(\n",
    "            self.params['W1'].shape,\n",
    "            self.params['b1'].shape,\n",
    "            self.params['W2'].shape,\n",
    "            self.params['b2'].shape,\n",
    "            self.params['W3'].shape,\n",
    "            self.params['b3'].shape\n",
    "        ))\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2, W3 = self.params['W1'], self.params['W2'], self.params['W3']\n",
    "        b1, b2, b3 = self.params['b1'], self.params['b2'], self.params['b3']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = relu(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        z2 = relu(a2)\n",
    "        a3 = np.dot(z2, W3) + b3        \n",
    "        return a3\n",
    "\n",
    "    def cross_entropy_error(self, x, t):\n",
    "        y = softmax(self.predict(x))\n",
    "        \n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(1, y.size)\n",
    "            t = t.reshape(1, t.size)\n",
    "\n",
    "        if t.size == y.size:\n",
    "            t = t.argmax(axis=1)\n",
    "        \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_derivative(self, params, x, z_target):\n",
    "        delta = 1e-4 # 0.0001\n",
    "        grad = np.zeros_like(params)\n",
    "       \n",
    "        it = np.nditer(params, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            temp_val = params[idx]\n",
    "\n",
    "            #f(x + delta) 계산\n",
    "            params[idx] = params[idx] + delta\n",
    "            fxh1 = self.cross_entropy_error(x, z_target)\n",
    "            \n",
    "            #f(x - delta) 계산\n",
    "            params[idx] = params[idx] - delta\n",
    "            fxh2 = self.cross_entropy_error(x, z_target)\n",
    "            \n",
    "            #f(x + delta) - f(x - delta) / 2 * delta 계산\n",
    "            grad[idx] = (fxh1 - fxh2) / (2 * delta)\n",
    "            params[idx] = temp_val\n",
    "            it.iternext()\n",
    "        return grad\n",
    "   \n",
    "    def learning(self, learning_rate, x_batch, t_batch):\n",
    "        for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "            print(key, end=', ', flush=True)\n",
    "            grad = self.numerical_derivative(self.params[key], x_batch, t_batch)\n",
    "            self.params[key] = self.params[key] - learning_rate * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1-shape: (784, 16), b1-shape: (16,), W2-shape: (16, 16), b2-shape: (16,), W3-shape: (16, 10), b3-shape: (10,)\n",
      "0, W1, b1, W2, b2, W3, b3, 1000, W1, b1, W2, b2, W3, b3, 2000, W1, b1, W2, b2, W3, b3, 3000, W1, b1, W2, b2, W3, b3, 4000, W1, b1, W2, b2, W3, b3, 5000, W1, b1, W2, b2, W3, b3, 6000, W1, b1, W2, b2, W3, b3, 7000, W1, b1, W2, b2, W3, b3, 8000, W1, b1, W2, b2, W3, b3, 9000, W1, b1, W2, b2, W3, b3, 10000, W1, b1, W2, b2, W3, b3, 11000, W1, b1, W2, b2, W3, b3, 12000, W1, b1, W2, b2, W3, b3, 13000, W1, b1, W2, b2, W3, b3, 14000, W1, b1, W2, b2, W3, b3, 15000, W1, b1, W2, b2, W3, b3, 16000, W1, b1, W2, b2, W3, b3, 17000, W1, b1, W2, b2, W3, b3, 18000, W1, b1, W2, b2, W3, b3, 19000, W1, b1, W2, b2, W3, b3, 20000, W1, b1, W2, b2, W3, b3, 21000, W1, b1, W2, b2, W3, b3, 22000, W1, b1, W2, b2, W3, b3, 23000, W1, b1, W2, b2, W3, b3, 24000, W1, b1, W2, b2, W3, b3, 25000, W1, b1, W2, b2, W3, b3, 26000, W1, b1, W2, b2, W3, b3, 27000, W1, b1, W2, b2, W3, b3, 28000, W1, b1, W2, b2, W3, b3, 29000, W1, b1, W2, b2, W3, b3, 30000, W1, b1, W2, b2, W3, b3, 31000, W1, b1, W2, b2, W3, b3, 32000, W1, b1, W2, b2, W3, b3, 33000, W1, b1, W2, b2, W3, b3, 34000, W1, b1, W2, b2, W3, b3, 35000, W1, b1, W2, b2, W3, b3, 36000, W1, b1, W2, b2, W3, b3, 37000, W1, b1, W2, b2, W3, b3, 38000, W1, b1, W2, b2, W3, b3, 39000, W1, b1, W2, b2, W3, b3, 40000, W1, b1, W2, b2, W3, b3, 41000, W1, b1, W2, b2, W3, b3, 42000, W1, b1, W2, b2, W3, b3, 43000, W1, b1, W2, b2, W3, b3, 44000, W1, b1, W2, b2, W3, b3, 45000, W1, b1, W2, b2, W3, b3, 46000, W1, b1, W2, b2, W3, b3, 47000, W1, b1, W2, b2, W3, b3, 48000, W1, b1, W2, b2, W3, b3, 49000, W1, b1, W2, b2, W3, b3, 50000, W1, b1, W2, b2, W3, b3, 51000, W1, b1, W2, b2, W3, b3, 52000, W1, b1, W2, b2, W3, b3, 53000, W1, b1, W2, b2, W3, b3, 54000, W1, b1, W2, b2, W3, b3, Epoch:   0, Train accuracy: 0.11295, validation accuracy: 0.10600\n",
      "0, W1, b1, W2, b2, W3, b3, 1000, W1, b1, W2, b2, W3, b3, 2000, W1, b1, W2, b2, W3, b3, 3000, W1, b1, W2, b2, W3, b3, 4000, W1, b1, W2, b2, W3, b3, 5000, W1, b1, W2, b2, W3, b3, 6000, W1, b1, W2, b2, W3, b3, 7000, W1, b1, W2, b2, W3, b3, 8000, W1, b1, W2, b2, W3, b3, 9000, W1, b1, W2, b2, W3, b3, 10000, W1, b1, W2, b2, W3, b3, 11000, W1, b1, W2, b2, W3, b3, 12000, W1, b1, W2, b2, W3, b3, 13000, W1, b1, W2, b2, W3, b3, 14000, W1, b1, W2, b2, W3, b3, 15000, W1, b1, W2, b2, W3, b3, 16000, W1, b1, W2, b2, W3, b3, 17000, W1, "
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "(img_train, label_train), (img_validation, label_validation), (img_test, label_test) = load_mnist(flatten=True, normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_layer1_size=16, hidden_layer2_size=16, output_size=10)\n",
    "\n",
    "num_epochs = 3000\n",
    "train_size = img_train.shape[0]\n",
    "batch_size = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "validation_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "num_batch = math.ceil(train_size / batch_size)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    for j in range(num_batch):\n",
    "        print(j * batch_size, end=', ', flush=True)\n",
    "        x_batch = img_train[j * batch_size : j * batch_size + batch_size]\n",
    "        t_batch = label_train[j * batch_size : j * batch_size + batch_size]\n",
    "        network.learning(learning_rate, x_batch, t_batch)\n",
    "    \n",
    "    error = network.cross_entropy_error(img_train, label_train)\n",
    "    train_loss_list.append(error)\n",
    "    \n",
    "    train_acc = network.accuracy(img_train, label_train)\n",
    "    train_acc_list.append(train_acc)\n",
    "\n",
    "    validation_acc = network.accuracy(img_validation, label_validation)\n",
    "    validation_acc_list.append(validation_acc)\n",
    "\n",
    "    print(\"Epoch: {0:3d}, Train accuracy: {1:7.5}, validation accuracy: {2:7.5f}\".format(\n",
    "        i,\n",
    "        train_acc,\n",
    "        validation_acc\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
