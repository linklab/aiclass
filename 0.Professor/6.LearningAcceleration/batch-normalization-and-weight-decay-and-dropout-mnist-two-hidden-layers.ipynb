{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST-Neural Network-Batch Normalization, Weight Decay and Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "from common.mnist import *\n",
    "from common.networks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Neural Network Model (Two Hidden Layers) and Learing/Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiLayerNetExtended(MultiLayerNet):\n",
    "    def __init__(self, input_size, hidden_size_list, output_size, activation='ReLU', initializer='N2', \n",
    "                 optimizer='AdaGrad', learning_rate=0.01, \n",
    "                 use_batch_normalization=False, \n",
    "                 use_weight_decay=False, weight_decay_lambda=0.0,\n",
    "                 use_dropout=False, dropout_ratio_list=None):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size_list = hidden_size_list\n",
    "        self.hidden_layer_num = len(hidden_size_list)\n",
    "        \n",
    "        self.use_batch_normalization = use_batch_normalization\n",
    "\n",
    "        self.use_weight_decay = use_weight_decay\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "        \n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout_ratio_list = dropout_ratio_list\n",
    "            \n",
    "        # Weight Initialization\n",
    "        self.params = {}\n",
    "        self.weight_initialization(initializer)\n",
    "        \n",
    "        # Layering\n",
    "        self.layers = OrderedDict()\n",
    "        self.last_layer = None\n",
    "        self.layering(activation)\n",
    "\n",
    "        # Optimization Method\n",
    "        self.optimizer = optimizers[optimizer](lr=learning_rate)\n",
    "    \n",
    "    def weight_initialization(self, initializer):\n",
    "        params_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
    "        initializer_obj = initializers[initializer](self.params, \n",
    "                                                    params_size_list, \n",
    "                                                    self.use_batch_normalization)\n",
    "        initializer_obj.initialize_params();\n",
    "        \n",
    "    def layering(self, activation):\n",
    "        for idx in range(1, self.hidden_layer_num + 1):\n",
    "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
    "            if self.use_batch_normalization:\n",
    "                self.layers['Batch_Normalization' + str(idx)] = BatchNormalization(self.params['gamma' + str(idx)], \n",
    "                                                                                   self.params['beta' + str(idx)])\n",
    "            self.layers['Activation' + str(idx)] = activation_layers[activation]()\n",
    "\n",
    "            if self.use_dropout:\n",
    "                self.layers['Dropout' + str(idx)] = Dropout(self.dropout_ratio_list[idx - 1])\n",
    "            \n",
    "        idx = self.hidden_layer_num + 1\n",
    "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
    "\n",
    "        self.last_layer = SoftmaxWithCrossEntropyLoss()  \n",
    "\n",
    "    def predict(self, x, is_train=False):\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"BatchNorm\" in key or \"Dropout\" in key:\n",
    "                x = layer.forward(x, is_train)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t, is_train=False):\n",
    "        y = self.predict(x, is_train)\n",
    "\n",
    "        if self.use_weight_decay:\n",
    "            weight_decay = 0.0\n",
    "            for idx in range(1, self.hidden_layer_num + 2):\n",
    "                W = self.params['W' + str(idx)]\n",
    "                weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
    "            return self.last_layer.forward(y, t) + weight_decay\n",
    "        else:\n",
    "            return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x, is_train=False)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy        \n",
    "\n",
    "    def backpropagation_gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t, is_train=True)\n",
    "\n",
    "        # backward\n",
    "        din = 1\n",
    "        din = self.last_layer.backward(din)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            din = layer.backward(din)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num + 2):\n",
    "            if self.use_weight_decay:\n",
    "                grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.params['W' + str(idx)]\n",
    "            else:\n",
    "                grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW\n",
    "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
    "\n",
    "            if self.use_batch_normalization and idx <= self.hidden_layer_num:\n",
    "                grads['gamma' + str(idx)] = self.layers['Batch_Normalization' + str(idx)].dgamma\n",
    "                grads['beta' + str(idx)] = self.layers['Batch_Normalization' + str(idx)].dbeta\n",
    "                \n",
    "        return grads\n",
    "\n",
    "    def learning(self, x_batch, t_batch):\n",
    "        grads = self.backpropagation_gradient(x_batch, t_batch)\n",
    "        self.optimizer.update(self.params, grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = mnist_data(\"/Users/yhhan/git/aiclass/0.Professor/data/MNIST_data/.\")\n",
    "(img_train, label_train), (img_validation, label_validation), (img_test, label_test) = data.load_mnist(flatten=True, normalize=True, one_hot_label=True)\n",
    "\n",
    "input_size=784\n",
    "hidden_layer1_size=128\n",
    "hidden_layer2_size=128\n",
    "output_size=10\n",
    "\n",
    "num_epochs = 50\n",
    "train_size = img_train.shape[0]\n",
    "batch_size = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "markers = {\"N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout\": \"+\", \n",
    "           \"N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout\": \"-\", \n",
    "           \"N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout\": \"*\", \n",
    "           \"N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout\": \"o\",\n",
    "           \"He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout\": \"x\", \n",
    "           \"He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout\": \">\",            \n",
    "           \"He, AdaGrad, Batch_Norm, lambda=0.0, Dropout\": \"h\", \n",
    "           \"He, AdaGrad, Batch_Norm, lambda=0.1, Dropout\": \"H\"}\n",
    "\n",
    "networks = {}\n",
    "train_errors = {}\n",
    "validation_errors = {}\n",
    "test_accuracy_values = {}\n",
    "max_test_accuracy_epoch = {}\n",
    "max_test_accuracy_value = {}\n",
    "\n",
    "for key in markers.keys():\n",
    "    if key == \"N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='N2',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=False, \n",
    "                                use_weight_decay=False, weight_decay_lambda=0.0,\n",
    "                                use_dropout=False, dropout_ratio_list=None)\n",
    "    elif key == \"N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='N2',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=False, \n",
    "                                use_weight_decay=True, weight_decay_lambda=0.1,\n",
    "                                use_dropout=True, dropout_ratio_list=[0.5, 0.5])        \n",
    "    elif key == \"N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='N2',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=True, \n",
    "                                use_weight_decay=False, weight_decay_lambda=0.0,\n",
    "                                use_dropout=True, dropout_ratio_list=[0.5, 0.5])\n",
    "    elif key == \"N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='N2',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=True, \n",
    "                                use_weight_decay=True, weight_decay_lambda=0.1,\n",
    "                                use_dropout=True, dropout_ratio_list=[0.5, 0.5])\n",
    "    elif key == \"He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='He',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=False, \n",
    "                                use_weight_decay=False, weight_decay_lambda=0.0,\n",
    "                                use_dropout=False, dropout_ratio_list=None)\n",
    "    elif key == \"He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='He',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=False, \n",
    "                                use_weight_decay=True, weight_decay_lambda=0.1,\n",
    "                                use_dropout=True, dropout_ratio_list=[0.5, 0.5])        \n",
    "    elif key == \"He, AdaGrad, Batch_Norm, lambda=0.0, Dropout\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='He',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=True, \n",
    "                                use_weight_decay=False, weight_decay_lambda=0.0,\n",
    "                                use_dropout=True, dropout_ratio_list=[0.5, 0.5])\n",
    "    elif key == \"He, AdaGrad, Batch_Norm, lambda=0.1, Dropout\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='He',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=True, \n",
    "                                use_weight_decay=True, weight_decay_lambda=0.1,\n",
    "                                use_dropout=True, dropout_ratio_list=[0.5, 0.5])\n",
    "        \n",
    "    train_errors[key] = [] \n",
    "    validation_errors[key] = []\n",
    "    test_accuracy_values[key] = []\n",
    "    max_test_accuracy_epoch[key] = 0\n",
    "    max_test_accuracy_value[key] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  0, Train Err.:0.10087, Validation Err.:0.13804, Test Accuracy:0.94500, Max Test Accuracy:0.94500\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  0, Train Err.:2.30158, Validation Err.:2.30163, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  0, Train Err.:0.17399, Validation Err.:0.15254, Test Accuracy:0.96260, Max Test Accuracy:0.96260\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  0, Train Err.:1.29782, Validation Err.:0.87225, Test Accuracy:0.81240, Max Test Accuracy:0.81240\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  0, Train Err.:0.08311, Validation Err.:0.12066, Test Accuracy:0.95060, Max Test Accuracy:0.95060\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  0, Train Err.:2.30152, Validation Err.:2.30169, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  0, Train Err.:0.18500, Validation Err.:0.13912, Test Accuracy:0.96200, Max Test Accuracy:0.96200\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  0, Train Err.:1.35374, Validation Err.:0.90883, Test Accuracy:0.82690, Max Test Accuracy:0.82690\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  1, Train Err.:0.08479, Validation Err.:0.12671, Test Accuracy:0.94850, Max Test Accuracy:0.94850\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  1, Train Err.:2.30163, Validation Err.:2.30164, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  1, Train Err.:0.15431, Validation Err.:0.15055, Test Accuracy:0.96330, Max Test Accuracy:0.96330\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  1, Train Err.:1.19450, Validation Err.:0.75973, Test Accuracy:0.86790, Max Test Accuracy:0.86790\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  1, Train Err.:0.06896, Validation Err.:0.11671, Test Accuracy:0.95390, Max Test Accuracy:0.95390\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  1, Train Err.:2.30158, Validation Err.:2.30167, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  1, Train Err.:0.16393, Validation Err.:0.14128, Test Accuracy:0.96550, Max Test Accuracy:0.96550\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  1, Train Err.:1.22006, Validation Err.:0.82920, Test Accuracy:0.84670, Max Test Accuracy:0.84670\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  2, Train Err.:0.07588, Validation Err.:0.12284, Test Accuracy:0.95100, Max Test Accuracy:0.95100\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  2, Train Err.:2.30168, Validation Err.:2.30165, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  2, Train Err.:0.14553, Validation Err.:0.14636, Test Accuracy:0.96530, Max Test Accuracy:0.96530\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  2, Train Err.:1.20230, Validation Err.:0.74333, Test Accuracy:0.87110, Max Test Accuracy:0.87110\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  2, Train Err.:0.05933, Validation Err.:0.11302, Test Accuracy:0.95670, Max Test Accuracy:0.95670\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  2, Train Err.:2.30163, Validation Err.:2.30166, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  2, Train Err.:0.14583, Validation Err.:0.14514, Test Accuracy:0.96840, Max Test Accuracy:0.96840\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  2, Train Err.:1.14716, Validation Err.:0.71654, Test Accuracy:0.87600, Max Test Accuracy:0.87600\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  3, Train Err.:0.06862, Validation Err.:0.11934, Test Accuracy:0.95260, Max Test Accuracy:0.95260\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  3, Train Err.:2.30172, Validation Err.:2.30166, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  3, Train Err.:0.15904, Validation Err.:0.14542, Test Accuracy:0.96610, Max Test Accuracy:0.96610\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  3, Train Err.:1.13129, Validation Err.:0.65316, Test Accuracy:0.89300, Max Test Accuracy:0.89300\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  3, Train Err.:0.05369, Validation Err.:0.11118, Test Accuracy:0.95760, Max Test Accuracy:0.95760\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  3, Train Err.:2.30169, Validation Err.:2.30167, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  3, Train Err.:0.17986, Validation Err.:0.13384, Test Accuracy:0.96990, Max Test Accuracy:0.96990\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  3, Train Err.:1.11712, Validation Err.:0.67483, Test Accuracy:0.88570, Max Test Accuracy:0.88570\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  4, Train Err.:0.06195, Validation Err.:0.11751, Test Accuracy:0.95470, Max Test Accuracy:0.95470\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  4, Train Err.:2.30176, Validation Err.:2.30167, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  4, Train Err.:0.13783, Validation Err.:0.14361, Test Accuracy:0.96610, Max Test Accuracy:0.96610\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  4, Train Err.:1.04284, Validation Err.:0.63800, Test Accuracy:0.88820, Max Test Accuracy:0.89300\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  4, Train Err.:0.04786, Validation Err.:0.10855, Test Accuracy:0.95970, Max Test Accuracy:0.95970\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  4, Train Err.:2.30171, Validation Err.:2.30167, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  4, Train Err.:0.13519, Validation Err.:0.14127, Test Accuracy:0.97040, Max Test Accuracy:0.97040\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  4, Train Err.:1.20186, Validation Err.:0.79497, Test Accuracy:0.85650, Max Test Accuracy:0.88570\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  5, Train Err.:0.05652, Validation Err.:0.11417, Test Accuracy:0.95650, Max Test Accuracy:0.95650\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  5, Train Err.:2.30179, Validation Err.:2.30167, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  5, Train Err.:0.14464, Validation Err.:0.14631, Test Accuracy:0.96700, Max Test Accuracy:0.96700\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  5, Train Err.:1.12141, Validation Err.:0.73155, Test Accuracy:0.86190, Max Test Accuracy:0.89300\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  5, Train Err.:0.04328, Validation Err.:0.10604, Test Accuracy:0.96090, Max Test Accuracy:0.96090\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  5, Train Err.:2.30176, Validation Err.:2.30168, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  5, Train Err.:0.17504, Validation Err.:0.14175, Test Accuracy:0.97130, Max Test Accuracy:0.97130\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  5, Train Err.:1.07622, Validation Err.:0.70191, Test Accuracy:0.88800, Max Test Accuracy:0.88800\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  6, Train Err.:0.05359, Validation Err.:0.11306, Test Accuracy:0.95850, Max Test Accuracy:0.95850\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  6, Train Err.:2.30181, Validation Err.:2.30168, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  6, Train Err.:0.14549, Validation Err.:0.14540, Test Accuracy:0.96790, Max Test Accuracy:0.96790\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  6, Train Err.:1.02385, Validation Err.:0.61003, Test Accuracy:0.90030, Max Test Accuracy:0.90030\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  6, Train Err.:0.03943, Validation Err.:0.10520, Test Accuracy:0.96180, Max Test Accuracy:0.96180\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  6, Train Err.:2.30176, Validation Err.:2.30169, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  6, Train Err.:0.14581, Validation Err.:0.13540, Test Accuracy:0.97120, Max Test Accuracy:0.97130\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  6, Train Err.:1.07951, Validation Err.:0.64272, Test Accuracy:0.88980, Max Test Accuracy:0.88980\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  7, Train Err.:0.05090, Validation Err.:0.11320, Test Accuracy:0.96000, Max Test Accuracy:0.96000\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  7, Train Err.:2.30184, Validation Err.:2.30168, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  7, Train Err.:0.16386, Validation Err.:0.14308, Test Accuracy:0.96590, Max Test Accuracy:0.96790\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  7, Train Err.:1.04947, Validation Err.:0.69028, Test Accuracy:0.86450, Max Test Accuracy:0.90030\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  7, Train Err.:0.03658, Validation Err.:0.10538, Test Accuracy:0.96260, Max Test Accuracy:0.96260\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  7, Train Err.:2.30182, Validation Err.:2.30169, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  7, Train Err.:0.12674, Validation Err.:0.13012, Test Accuracy:0.97130, Max Test Accuracy:0.97130\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  7, Train Err.:1.08419, Validation Err.:0.69087, Test Accuracy:0.87390, Max Test Accuracy:0.88980\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  8, Train Err.:0.04796, Validation Err.:0.11219, Test Accuracy:0.96070, Max Test Accuracy:0.96070\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  8, Train Err.:2.30186, Validation Err.:2.30169, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  8, Train Err.:0.13096, Validation Err.:0.14568, Test Accuracy:0.96680, Max Test Accuracy:0.96790\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  8, Train Err.:0.93982, Validation Err.:0.58173, Test Accuracy:0.90520, Max Test Accuracy:0.90520\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  8, Train Err.:0.03216, Validation Err.:0.10435, Test Accuracy:0.96400, Max Test Accuracy:0.96400\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  8, Train Err.:2.30183, Validation Err.:2.30171, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  8, Train Err.:0.13549, Validation Err.:0.13337, Test Accuracy:0.97230, Max Test Accuracy:0.97230\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  8, Train Err.:0.95530, Validation Err.:0.58415, Test Accuracy:0.90180, Max Test Accuracy:0.90180\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  9, Train Err.:0.04660, Validation Err.:0.11233, Test Accuracy:0.96170, Max Test Accuracy:0.96170\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  9, Train Err.:2.30187, Validation Err.:2.30169, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  9, Train Err.:0.15138, Validation Err.:0.14830, Test Accuracy:0.96740, Max Test Accuracy:0.96790\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  9, Train Err.:0.96790, Validation Err.:0.59774, Test Accuracy:0.89670, Max Test Accuracy:0.90520\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch:  9, Train Err.:0.02872, Validation Err.:0.10602, Test Accuracy:0.96470, Max Test Accuracy:0.96470\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch:  9, Train Err.:2.30185, Validation Err.:2.30170, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch:  9, Train Err.:0.11165, Validation Err.:0.13521, Test Accuracy:0.97260, Max Test Accuracy:0.97260\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch:  9, Train Err.:0.95204, Validation Err.:0.56481, Test Accuracy:0.90480, Max Test Accuracy:0.90480\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 10, Train Err.:0.04552, Validation Err.:0.11180, Test Accuracy:0.96200, Max Test Accuracy:0.96200\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 10, Train Err.:2.30189, Validation Err.:2.30170, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 10, Train Err.:0.12011, Validation Err.:0.14526, Test Accuracy:0.96820, Max Test Accuracy:0.96820\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 10, Train Err.:0.96054, Validation Err.:0.60919, Test Accuracy:0.89890, Max Test Accuracy:0.90520\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 10, Train Err.:0.02625, Validation Err.:0.10702, Test Accuracy:0.96620, Max Test Accuracy:0.96620\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 10, Train Err.:2.30187, Validation Err.:2.30170, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 10, Train Err.:0.12001, Validation Err.:0.13640, Test Accuracy:0.97230, Max Test Accuracy:0.97260\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 10, Train Err.:1.01273, Validation Err.:0.65139, Test Accuracy:0.86400, Max Test Accuracy:0.90480\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 11, Train Err.:0.04349, Validation Err.:0.11254, Test Accuracy:0.96240, Max Test Accuracy:0.96240\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 11, Train Err.:2.30190, Validation Err.:2.30170, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 11, Train Err.:0.12157, Validation Err.:0.15119, Test Accuracy:0.96860, Max Test Accuracy:0.96860\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 11, Train Err.:0.91618, Validation Err.:0.58359, Test Accuracy:0.89580, Max Test Accuracy:0.90520\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 11, Train Err.:0.02417, Validation Err.:0.10938, Test Accuracy:0.96660, Max Test Accuracy:0.96660\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 11, Train Err.:2.30190, Validation Err.:2.30171, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 11, Train Err.:0.12643, Validation Err.:0.13914, Test Accuracy:0.97260, Max Test Accuracy:0.97260\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 11, Train Err.:0.95138, Validation Err.:0.59202, Test Accuracy:0.90070, Max Test Accuracy:0.90480\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 12, Train Err.:0.04183, Validation Err.:0.11283, Test Accuracy:0.96270, Max Test Accuracy:0.96270\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 12, Train Err.:2.30192, Validation Err.:2.30171, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 12, Train Err.:0.10766, Validation Err.:0.14763, Test Accuracy:0.96880, Max Test Accuracy:0.96880\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 12, Train Err.:0.96626, Validation Err.:0.57226, Test Accuracy:0.89710, Max Test Accuracy:0.90520\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 12, Train Err.:0.02309, Validation Err.:0.10918, Test Accuracy:0.96680, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 12, Train Err.:2.30190, Validation Err.:2.30171, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 12, Train Err.:0.12146, Validation Err.:0.13172, Test Accuracy:0.97240, Max Test Accuracy:0.97260\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 12, Train Err.:0.91445, Validation Err.:0.57198, Test Accuracy:0.90640, Max Test Accuracy:0.90640\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 13, Train Err.:0.04025, Validation Err.:0.11232, Test Accuracy:0.96240, Max Test Accuracy:0.96270\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 13, Train Err.:2.30193, Validation Err.:2.30171, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 13, Train Err.:0.10959, Validation Err.:0.14491, Test Accuracy:0.96960, Max Test Accuracy:0.96960\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 13, Train Err.:0.96251, Validation Err.:0.53956, Test Accuracy:0.90810, Max Test Accuracy:0.90810\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 13, Train Err.:0.02125, Validation Err.:0.11002, Test Accuracy:0.96680, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 13, Train Err.:2.30192, Validation Err.:2.30171, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 13, Train Err.:0.13164, Validation Err.:0.13153, Test Accuracy:0.97350, Max Test Accuracy:0.97350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 13, Train Err.:0.96423, Validation Err.:0.59453, Test Accuracy:0.89840, Max Test Accuracy:0.90640\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 14, Train Err.:0.03902, Validation Err.:0.11269, Test Accuracy:0.96220, Max Test Accuracy:0.96270\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 14, Train Err.:2.30194, Validation Err.:2.30171, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 14, Train Err.:0.12340, Validation Err.:0.14260, Test Accuracy:0.96930, Max Test Accuracy:0.96960\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 14, Train Err.:0.91719, Validation Err.:0.52474, Test Accuracy:0.91180, Max Test Accuracy:0.91180\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 14, Train Err.:0.01956, Validation Err.:0.11058, Test Accuracy:0.96680, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 14, Train Err.:2.30193, Validation Err.:2.30171, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 14, Train Err.:0.12385, Validation Err.:0.13903, Test Accuracy:0.97220, Max Test Accuracy:0.97350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 14, Train Err.:0.85225, Validation Err.:0.54794, Test Accuracy:0.90620, Max Test Accuracy:0.90640\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 15, Train Err.:0.03803, Validation Err.:0.11266, Test Accuracy:0.96270, Max Test Accuracy:0.96270\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 15, Train Err.:2.30195, Validation Err.:2.30172, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 15, Train Err.:0.11258, Validation Err.:0.14687, Test Accuracy:0.97030, Max Test Accuracy:0.97030\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 15, Train Err.:0.86139, Validation Err.:0.52216, Test Accuracy:0.91140, Max Test Accuracy:0.91180\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 15, Train Err.:0.01830, Validation Err.:0.11149, Test Accuracy:0.96690, Max Test Accuracy:0.96690\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 15, Train Err.:2.30194, Validation Err.:2.30172, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 15, Train Err.:0.10848, Validation Err.:0.13200, Test Accuracy:0.97330, Max Test Accuracy:0.97350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 15, Train Err.:0.87960, Validation Err.:0.54860, Test Accuracy:0.91170, Max Test Accuracy:0.91170\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 16, Train Err.:0.03710, Validation Err.:0.11292, Test Accuracy:0.96260, Max Test Accuracy:0.96270\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 16, Train Err.:2.30196, Validation Err.:2.30172, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 16, Train Err.:0.11653, Validation Err.:0.15223, Test Accuracy:0.96940, Max Test Accuracy:0.97030\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 16, Train Err.:0.88468, Validation Err.:0.53153, Test Accuracy:0.91280, Max Test Accuracy:0.91280\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 16, Train Err.:0.01745, Validation Err.:0.11263, Test Accuracy:0.96680, Max Test Accuracy:0.96690\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 16, Train Err.:2.30195, Validation Err.:2.30172, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 16, Train Err.:0.11191, Validation Err.:0.13232, Test Accuracy:0.97270, Max Test Accuracy:0.97350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 16, Train Err.:0.88411, Validation Err.:0.55296, Test Accuracy:0.90670, Max Test Accuracy:0.91170\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 17, Train Err.:0.03611, Validation Err.:0.11250, Test Accuracy:0.96330, Max Test Accuracy:0.96330\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 17, Train Err.:2.30197, Validation Err.:2.30172, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 17, Train Err.:0.11109, Validation Err.:0.14915, Test Accuracy:0.97070, Max Test Accuracy:0.97070\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 17, Train Err.:0.89364, Validation Err.:0.54422, Test Accuracy:0.90760, Max Test Accuracy:0.91280\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 17, Train Err.:0.01613, Validation Err.:0.11312, Test Accuracy:0.96690, Max Test Accuracy:0.96690\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 17, Train Err.:2.30196, Validation Err.:2.30172, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 17, Train Err.:0.10623, Validation Err.:0.13207, Test Accuracy:0.97280, Max Test Accuracy:0.97350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 17, Train Err.:0.88925, Validation Err.:0.52985, Test Accuracy:0.91100, Max Test Accuracy:0.91170\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 18, Train Err.:0.03450, Validation Err.:0.11264, Test Accuracy:0.96310, Max Test Accuracy:0.96330\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 18, Train Err.:2.30198, Validation Err.:2.30172, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 18, Train Err.:0.11510, Validation Err.:0.14858, Test Accuracy:0.97040, Max Test Accuracy:0.97070\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 18, Train Err.:0.87431, Validation Err.:0.50837, Test Accuracy:0.91490, Max Test Accuracy:0.91490\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 18, Train Err.:0.01512, Validation Err.:0.11509, Test Accuracy:0.96680, Max Test Accuracy:0.96690\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 18, Train Err.:2.30197, Validation Err.:2.30172, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 18, Train Err.:0.11547, Validation Err.:0.13223, Test Accuracy:0.97320, Max Test Accuracy:0.97350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 18, Train Err.:0.86976, Validation Err.:0.57283, Test Accuracy:0.89510, Max Test Accuracy:0.91170\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 19, Train Err.:0.03363, Validation Err.:0.11320, Test Accuracy:0.96340, Max Test Accuracy:0.96340\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 19, Train Err.:2.30199, Validation Err.:2.30173, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 19, Train Err.:0.10865, Validation Err.:0.14963, Test Accuracy:0.97020, Max Test Accuracy:0.97070\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 19, Train Err.:0.87574, Validation Err.:0.51035, Test Accuracy:0.91230, Max Test Accuracy:0.91490\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 19, Train Err.:0.01425, Validation Err.:0.11606, Test Accuracy:0.96730, Max Test Accuracy:0.96730\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 19, Train Err.:2.30198, Validation Err.:2.30173, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 19, Train Err.:0.10553, Validation Err.:0.13174, Test Accuracy:0.97430, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 19, Train Err.:0.88101, Validation Err.:0.52440, Test Accuracy:0.90880, Max Test Accuracy:0.91170\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 20, Train Err.:0.03287, Validation Err.:0.11330, Test Accuracy:0.96320, Max Test Accuracy:0.96340\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 20, Train Err.:2.30200, Validation Err.:2.30173, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 20, Train Err.:0.10748, Validation Err.:0.15270, Test Accuracy:0.96960, Max Test Accuracy:0.97070\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 20, Train Err.:0.83528, Validation Err.:0.49715, Test Accuracy:0.91420, Max Test Accuracy:0.91490\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 20, Train Err.:0.01322, Validation Err.:0.11725, Test Accuracy:0.96650, Max Test Accuracy:0.96730\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 20, Train Err.:2.30199, Validation Err.:2.30173, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 20, Train Err.:0.12687, Validation Err.:0.12924, Test Accuracy:0.97270, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 20, Train Err.:0.82991, Validation Err.:0.50244, Test Accuracy:0.91230, Max Test Accuracy:0.91230\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 21, Train Err.:0.03174, Validation Err.:0.11488, Test Accuracy:0.96270, Max Test Accuracy:0.96340\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 21, Train Err.:2.30201, Validation Err.:2.30173, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 21, Train Err.:0.12369, Validation Err.:0.15213, Test Accuracy:0.96990, Max Test Accuracy:0.97070\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 21, Train Err.:0.85788, Validation Err.:0.49557, Test Accuracy:0.91570, Max Test Accuracy:0.91570\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 21, Train Err.:0.01217, Validation Err.:0.11852, Test Accuracy:0.96750, Max Test Accuracy:0.96750\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 21, Train Err.:2.30200, Validation Err.:2.30173, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 21, Train Err.:0.11275, Validation Err.:0.13901, Test Accuracy:0.97350, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 21, Train Err.:0.83223, Validation Err.:0.52180, Test Accuracy:0.91190, Max Test Accuracy:0.91230\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 22, Train Err.:0.03078, Validation Err.:0.11439, Test Accuracy:0.96350, Max Test Accuracy:0.96350\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 22, Train Err.:2.30201, Validation Err.:2.30173, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 22, Train Err.:0.14186, Validation Err.:0.15097, Test Accuracy:0.97110, Max Test Accuracy:0.97110\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 22, Train Err.:0.86399, Validation Err.:0.52205, Test Accuracy:0.92010, Max Test Accuracy:0.92010\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 22, Train Err.:0.01114, Validation Err.:0.12042, Test Accuracy:0.96770, Max Test Accuracy:0.96770\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 22, Train Err.:2.30200, Validation Err.:2.30173, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 22, Train Err.:0.10626, Validation Err.:0.13905, Test Accuracy:0.97260, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 22, Train Err.:0.82653, Validation Err.:0.51527, Test Accuracy:0.91690, Max Test Accuracy:0.91690\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 23, Train Err.:0.02988, Validation Err.:0.11515, Test Accuracy:0.96320, Max Test Accuracy:0.96350\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 23, Train Err.:2.30202, Validation Err.:2.30174, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 23, Train Err.:0.11614, Validation Err.:0.15444, Test Accuracy:0.97010, Max Test Accuracy:0.97110\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 23, Train Err.:0.83400, Validation Err.:0.49397, Test Accuracy:0.91360, Max Test Accuracy:0.92010\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 23, Train Err.:0.01051, Validation Err.:0.12190, Test Accuracy:0.96730, Max Test Accuracy:0.96770\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 23, Train Err.:2.30201, Validation Err.:2.30174, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 23, Train Err.:0.10991, Validation Err.:0.13836, Test Accuracy:0.97350, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 23, Train Err.:0.83660, Validation Err.:0.50814, Test Accuracy:0.91490, Max Test Accuracy:0.91690\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 24, Train Err.:0.02947, Validation Err.:0.11581, Test Accuracy:0.96360, Max Test Accuracy:0.96360\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 24, Train Err.:2.30203, Validation Err.:2.30174, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 24, Train Err.:0.08387, Validation Err.:0.15127, Test Accuracy:0.97080, Max Test Accuracy:0.97110\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 24, Train Err.:0.85187, Validation Err.:0.49241, Test Accuracy:0.91370, Max Test Accuracy:0.92010\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 24, Train Err.:0.00974, Validation Err.:0.12338, Test Accuracy:0.96730, Max Test Accuracy:0.96770\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 24, Train Err.:2.30202, Validation Err.:2.30174, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 24, Train Err.:0.10394, Validation Err.:0.13859, Test Accuracy:0.97390, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 24, Train Err.:0.82701, Validation Err.:0.48585, Test Accuracy:0.92060, Max Test Accuracy:0.92060\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 25, Train Err.:0.02903, Validation Err.:0.11707, Test Accuracy:0.96340, Max Test Accuracy:0.96360\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 25, Train Err.:2.30203, Validation Err.:2.30174, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 25, Train Err.:0.10049, Validation Err.:0.15287, Test Accuracy:0.97040, Max Test Accuracy:0.97110\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 25, Train Err.:0.83557, Validation Err.:0.47571, Test Accuracy:0.92080, Max Test Accuracy:0.92080\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 25, Train Err.:0.00903, Validation Err.:0.12447, Test Accuracy:0.96720, Max Test Accuracy:0.96770\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 25, Train Err.:2.30203, Validation Err.:2.30174, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 25, Train Err.:0.09259, Validation Err.:0.13785, Test Accuracy:0.97320, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 25, Train Err.:0.81372, Validation Err.:0.49756, Test Accuracy:0.91600, Max Test Accuracy:0.92060\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 26, Train Err.:0.02854, Validation Err.:0.11856, Test Accuracy:0.96380, Max Test Accuracy:0.96380\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 26, Train Err.:2.30204, Validation Err.:2.30174, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 26, Train Err.:0.14197, Validation Err.:0.14850, Test Accuracy:0.97100, Max Test Accuracy:0.97110\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 26, Train Err.:0.85281, Validation Err.:0.47576, Test Accuracy:0.91880, Max Test Accuracy:0.92080\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 26, Train Err.:0.00856, Validation Err.:0.12659, Test Accuracy:0.96740, Max Test Accuracy:0.96770\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 26, Train Err.:2.30203, Validation Err.:2.30174, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 26, Train Err.:0.09459, Validation Err.:0.14278, Test Accuracy:0.97340, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 26, Train Err.:0.82435, Validation Err.:0.48598, Test Accuracy:0.91870, Max Test Accuracy:0.92060\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 27, Train Err.:0.02791, Validation Err.:0.11971, Test Accuracy:0.96370, Max Test Accuracy:0.96380\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 27, Train Err.:2.30204, Validation Err.:2.30174, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 27, Train Err.:0.11548, Validation Err.:0.15516, Test Accuracy:0.97110, Max Test Accuracy:0.97110\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 27, Train Err.:0.82072, Validation Err.:0.49670, Test Accuracy:0.91750, Max Test Accuracy:0.92080\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 27, Train Err.:0.00790, Validation Err.:0.12771, Test Accuracy:0.96770, Max Test Accuracy:0.96770\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 27, Train Err.:2.30204, Validation Err.:2.30174, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 27, Train Err.:0.09325, Validation Err.:0.13622, Test Accuracy:0.97290, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 27, Train Err.:0.77039, Validation Err.:0.48875, Test Accuracy:0.91530, Max Test Accuracy:0.92060\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 28, Train Err.:0.02741, Validation Err.:0.12052, Test Accuracy:0.96430, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 28, Train Err.:2.30205, Validation Err.:2.30174, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 28, Train Err.:0.11759, Validation Err.:0.15256, Test Accuracy:0.97080, Max Test Accuracy:0.97110\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 28, Train Err.:0.74735, Validation Err.:0.45561, Test Accuracy:0.92370, Max Test Accuracy:0.92370\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 28, Train Err.:0.00740, Validation Err.:0.12954, Test Accuracy:0.96740, Max Test Accuracy:0.96770\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 28, Train Err.:2.30204, Validation Err.:2.30174, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 28, Train Err.:0.10493, Validation Err.:0.14086, Test Accuracy:0.97350, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 28, Train Err.:0.81902, Validation Err.:0.45419, Test Accuracy:0.92240, Max Test Accuracy:0.92240\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 29, Train Err.:0.02687, Validation Err.:0.12130, Test Accuracy:0.96370, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 29, Train Err.:2.30205, Validation Err.:2.30175, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 29, Train Err.:0.12183, Validation Err.:0.15256, Test Accuracy:0.97150, Max Test Accuracy:0.97150\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 29, Train Err.:0.79489, Validation Err.:0.46061, Test Accuracy:0.92270, Max Test Accuracy:0.92370\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 29, Train Err.:0.00716, Validation Err.:0.13091, Test Accuracy:0.96790, Max Test Accuracy:0.96790\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 29, Train Err.:2.30205, Validation Err.:2.30174, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 29, Train Err.:0.09666, Validation Err.:0.14626, Test Accuracy:0.97330, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 29, Train Err.:0.77891, Validation Err.:0.46848, Test Accuracy:0.92460, Max Test Accuracy:0.92460\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 30, Train Err.:0.02668, Validation Err.:0.12396, Test Accuracy:0.96310, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 30, Train Err.:2.30206, Validation Err.:2.30175, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 30, Train Err.:0.09332, Validation Err.:0.15068, Test Accuracy:0.97180, Max Test Accuracy:0.97180\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 30, Train Err.:0.81743, Validation Err.:0.45840, Test Accuracy:0.92370, Max Test Accuracy:0.92370\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 30, Train Err.:0.00665, Validation Err.:0.13287, Test Accuracy:0.96810, Max Test Accuracy:0.96810\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 30, Train Err.:2.30205, Validation Err.:2.30175, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 30, Train Err.:0.09624, Validation Err.:0.14359, Test Accuracy:0.97330, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 30, Train Err.:0.78664, Validation Err.:0.47670, Test Accuracy:0.91510, Max Test Accuracy:0.92460\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 31, Train Err.:0.02585, Validation Err.:0.12318, Test Accuracy:0.96330, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 31, Train Err.:2.30206, Validation Err.:2.30175, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 31, Train Err.:0.09880, Validation Err.:0.15198, Test Accuracy:0.97210, Max Test Accuracy:0.97210\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 31, Train Err.:0.82315, Validation Err.:0.48926, Test Accuracy:0.92420, Max Test Accuracy:0.92420\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 31, Train Err.:0.00637, Validation Err.:0.13407, Test Accuracy:0.96780, Max Test Accuracy:0.96810\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 31, Train Err.:2.30206, Validation Err.:2.30175, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 31, Train Err.:0.09320, Validation Err.:0.14205, Test Accuracy:0.97340, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 31, Train Err.:0.80341, Validation Err.:0.47072, Test Accuracy:0.92360, Max Test Accuracy:0.92460\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 32, Train Err.:0.02557, Validation Err.:0.12471, Test Accuracy:0.96360, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 32, Train Err.:2.30207, Validation Err.:2.30175, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 32, Train Err.:0.12183, Validation Err.:0.15137, Test Accuracy:0.97070, Max Test Accuracy:0.97210\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 32, Train Err.:0.78921, Validation Err.:0.47842, Test Accuracy:0.91800, Max Test Accuracy:0.92420\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 32, Train Err.:0.00593, Validation Err.:0.13620, Test Accuracy:0.96760, Max Test Accuracy:0.96810\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 32, Train Err.:2.30206, Validation Err.:2.30175, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 32, Train Err.:0.11640, Validation Err.:0.14367, Test Accuracy:0.97310, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 32, Train Err.:0.83033, Validation Err.:0.46300, Test Accuracy:0.92570, Max Test Accuracy:0.92570\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 33, Train Err.:0.02524, Validation Err.:0.12439, Test Accuracy:0.96340, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 33, Train Err.:2.30207, Validation Err.:2.30175, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 33, Train Err.:0.08813, Validation Err.:0.15521, Test Accuracy:0.97060, Max Test Accuracy:0.97210\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 33, Train Err.:0.82039, Validation Err.:0.46760, Test Accuracy:0.92000, Max Test Accuracy:0.92420\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 33, Train Err.:0.00537, Validation Err.:0.13815, Test Accuracy:0.96710, Max Test Accuracy:0.96810\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 33, Train Err.:2.30207, Validation Err.:2.30175, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 33, Train Err.:0.08872, Validation Err.:0.14647, Test Accuracy:0.97310, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 33, Train Err.:0.75595, Validation Err.:0.48779, Test Accuracy:0.92140, Max Test Accuracy:0.92570\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 34, Train Err.:0.02474, Validation Err.:0.12655, Test Accuracy:0.96320, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 34, Train Err.:2.30207, Validation Err.:2.30175, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 34, Train Err.:0.10538, Validation Err.:0.15667, Test Accuracy:0.97080, Max Test Accuracy:0.97210\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 34, Train Err.:0.78176, Validation Err.:0.48189, Test Accuracy:0.91210, Max Test Accuracy:0.92420\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 34, Train Err.:0.00518, Validation Err.:0.13830, Test Accuracy:0.96780, Max Test Accuracy:0.96810\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 34, Train Err.:2.30207, Validation Err.:2.30175, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 34, Train Err.:0.07772, Validation Err.:0.14550, Test Accuracy:0.97370, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 34, Train Err.:0.79627, Validation Err.:0.46411, Test Accuracy:0.92060, Max Test Accuracy:0.92570\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 35, Train Err.:0.02484, Validation Err.:0.12715, Test Accuracy:0.96310, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 35, Train Err.:2.30208, Validation Err.:2.30175, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 35, Train Err.:0.11088, Validation Err.:0.15794, Test Accuracy:0.97040, Max Test Accuracy:0.97210\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 35, Train Err.:0.79851, Validation Err.:0.49409, Test Accuracy:0.90700, Max Test Accuracy:0.92420\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 35, Train Err.:0.00498, Validation Err.:0.14064, Test Accuracy:0.96840, Max Test Accuracy:0.96840\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 35, Train Err.:2.30207, Validation Err.:2.30175, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 35, Train Err.:0.08045, Validation Err.:0.14433, Test Accuracy:0.97390, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 35, Train Err.:0.75077, Validation Err.:0.45994, Test Accuracy:0.92370, Max Test Accuracy:0.92570\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 36, Train Err.:0.02378, Validation Err.:0.12801, Test Accuracy:0.96370, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 36, Train Err.:2.30208, Validation Err.:2.30175, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 36, Train Err.:0.10799, Validation Err.:0.15810, Test Accuracy:0.97010, Max Test Accuracy:0.97210\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 36, Train Err.:0.75672, Validation Err.:0.44402, Test Accuracy:0.92710, Max Test Accuracy:0.92710\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 36, Train Err.:0.00456, Validation Err.:0.14191, Test Accuracy:0.96780, Max Test Accuracy:0.96840\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 36, Train Err.:2.30208, Validation Err.:2.30175, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 36, Train Err.:0.09534, Validation Err.:0.14627, Test Accuracy:0.97400, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 36, Train Err.:0.76016, Validation Err.:0.47235, Test Accuracy:0.92130, Max Test Accuracy:0.92570\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 37, Train Err.:0.02332, Validation Err.:0.12848, Test Accuracy:0.96310, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 37, Train Err.:2.30209, Validation Err.:2.30175, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 37, Train Err.:0.10390, Validation Err.:0.15573, Test Accuracy:0.97100, Max Test Accuracy:0.97210\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 37, Train Err.:0.78619, Validation Err.:0.45698, Test Accuracy:0.92090, Max Test Accuracy:0.92710\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 37, Train Err.:0.00449, Validation Err.:0.14412, Test Accuracy:0.96780, Max Test Accuracy:0.96840\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 37, Train Err.:2.30208, Validation Err.:2.30175, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 37, Train Err.:0.08379, Validation Err.:0.14299, Test Accuracy:0.97410, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 37, Train Err.:0.78095, Validation Err.:0.47764, Test Accuracy:0.92160, Max Test Accuracy:0.92570\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 38, Train Err.:0.02283, Validation Err.:0.12965, Test Accuracy:0.96370, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 38, Train Err.:2.30209, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 38, Train Err.:0.09165, Validation Err.:0.15788, Test Accuracy:0.97210, Max Test Accuracy:0.97210\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 38, Train Err.:0.76456, Validation Err.:0.44789, Test Accuracy:0.92100, Max Test Accuracy:0.92710\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 38, Train Err.:0.00435, Validation Err.:0.14546, Test Accuracy:0.96740, Max Test Accuracy:0.96840\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 38, Train Err.:2.30209, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 38, Train Err.:0.09101, Validation Err.:0.14619, Test Accuracy:0.97390, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 38, Train Err.:0.76584, Validation Err.:0.45711, Test Accuracy:0.92470, Max Test Accuracy:0.92570\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 39, Train Err.:0.02253, Validation Err.:0.13105, Test Accuracy:0.96360, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 39, Train Err.:2.30209, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 39, Train Err.:0.09278, Validation Err.:0.15766, Test Accuracy:0.97090, Max Test Accuracy:0.97210\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 39, Train Err.:0.76528, Validation Err.:0.45830, Test Accuracy:0.92280, Max Test Accuracy:0.92710\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 39, Train Err.:0.00422, Validation Err.:0.14755, Test Accuracy:0.96760, Max Test Accuracy:0.96840\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 39, Train Err.:2.30209, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 39, Train Err.:0.10576, Validation Err.:0.14046, Test Accuracy:0.97300, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 39, Train Err.:0.77053, Validation Err.:0.46132, Test Accuracy:0.92560, Max Test Accuracy:0.92570\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 40, Train Err.:0.02174, Validation Err.:0.13249, Test Accuracy:0.96360, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 40, Train Err.:2.30210, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 40, Train Err.:0.09345, Validation Err.:0.15959, Test Accuracy:0.97060, Max Test Accuracy:0.97210\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 40, Train Err.:0.76008, Validation Err.:0.45072, Test Accuracy:0.91920, Max Test Accuracy:0.92710\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 40, Train Err.:0.00410, Validation Err.:0.14803, Test Accuracy:0.96760, Max Test Accuracy:0.96840\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 40, Train Err.:2.30209, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 40, Train Err.:0.09406, Validation Err.:0.14202, Test Accuracy:0.97360, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 40, Train Err.:0.75307, Validation Err.:0.44983, Test Accuracy:0.92510, Max Test Accuracy:0.92570\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 41, Train Err.:0.02108, Validation Err.:0.13261, Test Accuracy:0.96350, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 41, Train Err.:2.30210, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 41, Train Err.:0.10029, Validation Err.:0.15863, Test Accuracy:0.97110, Max Test Accuracy:0.97210\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 41, Train Err.:0.77962, Validation Err.:0.46149, Test Accuracy:0.91990, Max Test Accuracy:0.92710\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 41, Train Err.:0.00389, Validation Err.:0.14917, Test Accuracy:0.96760, Max Test Accuracy:0.96840\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 41, Train Err.:2.30210, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 41, Train Err.:0.11435, Validation Err.:0.14163, Test Accuracy:0.97330, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 41, Train Err.:0.73249, Validation Err.:0.43985, Test Accuracy:0.92790, Max Test Accuracy:0.92790\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 42, Train Err.:0.02075, Validation Err.:0.13546, Test Accuracy:0.96300, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 42, Train Err.:2.30210, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 42, Train Err.:0.11548, Validation Err.:0.15778, Test Accuracy:0.97090, Max Test Accuracy:0.97210\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 42, Train Err.:0.74738, Validation Err.:0.45527, Test Accuracy:0.92160, Max Test Accuracy:0.92710\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 42, Train Err.:0.00369, Validation Err.:0.15035, Test Accuracy:0.96750, Max Test Accuracy:0.96840\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 42, Train Err.:2.30210, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 42, Train Err.:0.08925, Validation Err.:0.14735, Test Accuracy:0.97310, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 42, Train Err.:0.75036, Validation Err.:0.44144, Test Accuracy:0.92800, Max Test Accuracy:0.92800\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 43, Train Err.:0.02038, Validation Err.:0.13555, Test Accuracy:0.96380, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 43, Train Err.:2.30210, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 43, Train Err.:0.12330, Validation Err.:0.16419, Test Accuracy:0.97140, Max Test Accuracy:0.97210\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 43, Train Err.:0.79532, Validation Err.:0.47139, Test Accuracy:0.92000, Max Test Accuracy:0.92710\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 43, Train Err.:0.00353, Validation Err.:0.15187, Test Accuracy:0.96730, Max Test Accuracy:0.96840\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 43, Train Err.:2.30210, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 43, Train Err.:0.09232, Validation Err.:0.14578, Test Accuracy:0.97300, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 43, Train Err.:0.73588, Validation Err.:0.45091, Test Accuracy:0.92470, Max Test Accuracy:0.92800\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 44, Train Err.:0.01975, Validation Err.:0.13591, Test Accuracy:0.96340, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 44, Train Err.:2.30211, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 44, Train Err.:0.10269, Validation Err.:0.16424, Test Accuracy:0.97070, Max Test Accuracy:0.97210\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 44, Train Err.:0.76162, Validation Err.:0.44338, Test Accuracy:0.92660, Max Test Accuracy:0.92710\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 44, Train Err.:0.00344, Validation Err.:0.15366, Test Accuracy:0.96730, Max Test Accuracy:0.96840\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 44, Train Err.:2.30211, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 44, Train Err.:0.09973, Validation Err.:0.14968, Test Accuracy:0.97360, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 44, Train Err.:0.73445, Validation Err.:0.43995, Test Accuracy:0.92510, Max Test Accuracy:0.92800\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 45, Train Err.:0.01924, Validation Err.:0.13757, Test Accuracy:0.96260, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 45, Train Err.:2.30211, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 45, Train Err.:0.10515, Validation Err.:0.16936, Test Accuracy:0.96990, Max Test Accuracy:0.97210\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 45, Train Err.:0.78057, Validation Err.:0.45462, Test Accuracy:0.92620, Max Test Accuracy:0.92710\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 45, Train Err.:0.00333, Validation Err.:0.15473, Test Accuracy:0.96780, Max Test Accuracy:0.96840\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 45, Train Err.:2.30211, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 45, Train Err.:0.08702, Validation Err.:0.14920, Test Accuracy:0.97370, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 45, Train Err.:0.73885, Validation Err.:0.43294, Test Accuracy:0.92690, Max Test Accuracy:0.92800\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 46, Train Err.:0.01877, Validation Err.:0.13899, Test Accuracy:0.96270, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 46, Train Err.:2.30211, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 46, Train Err.:0.09501, Validation Err.:0.16902, Test Accuracy:0.97140, Max Test Accuracy:0.97210\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 46, Train Err.:0.76163, Validation Err.:0.42911, Test Accuracy:0.92380, Max Test Accuracy:0.92710\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 46, Train Err.:0.00323, Validation Err.:0.15645, Test Accuracy:0.96720, Max Test Accuracy:0.96840\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 46, Train Err.:2.30211, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 46, Train Err.:0.09000, Validation Err.:0.14467, Test Accuracy:0.97230, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 46, Train Err.:0.70916, Validation Err.:0.43805, Test Accuracy:0.92670, Max Test Accuracy:0.92800\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 47, Train Err.:0.01827, Validation Err.:0.13953, Test Accuracy:0.96280, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 47, Train Err.:2.30212, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 47, Train Err.:0.11106, Validation Err.:0.16601, Test Accuracy:0.97090, Max Test Accuracy:0.97210\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 47, Train Err.:0.83054, Validation Err.:0.48447, Test Accuracy:0.90550, Max Test Accuracy:0.92710\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 47, Train Err.:0.00314, Validation Err.:0.15705, Test Accuracy:0.96760, Max Test Accuracy:0.96840\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 47, Train Err.:2.30211, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 47, Train Err.:0.08314, Validation Err.:0.14881, Test Accuracy:0.97270, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 47, Train Err.:0.74876, Validation Err.:0.41989, Test Accuracy:0.93020, Max Test Accuracy:0.93020\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 48, Train Err.:0.01729, Validation Err.:0.14112, Test Accuracy:0.96250, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 48, Train Err.:2.30212, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 48, Train Err.:0.09628, Validation Err.:0.16337, Test Accuracy:0.97140, Max Test Accuracy:0.97210\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 48, Train Err.:0.78075, Validation Err.:0.43209, Test Accuracy:0.93140, Max Test Accuracy:0.93140\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 48, Train Err.:0.00303, Validation Err.:0.15849, Test Accuracy:0.96750, Max Test Accuracy:0.96840\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 48, Train Err.:2.30211, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 48, Train Err.:0.09809, Validation Err.:0.14698, Test Accuracy:0.97240, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 48, Train Err.:0.74050, Validation Err.:0.42679, Test Accuracy:0.92910, Max Test Accuracy:0.93020\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 49, Train Err.:0.01730, Validation Err.:0.14292, Test Accuracy:0.96220, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 49, Train Err.:2.30212, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 49, Train Err.:0.09109, Validation Err.:0.16015, Test Accuracy:0.97020, Max Test Accuracy:0.97210\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 49, Train Err.:0.75810, Validation Err.:0.43874, Test Accuracy:0.92660, Max Test Accuracy:0.93140\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0, No_Dropout-Epoch: 49, Train Err.:0.00292, Validation Err.:0.16025, Test Accuracy:0.96740, Max Test Accuracy:0.96840\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.1, Dropout   -Epoch: 49, Train Err.:2.30212, Validation Err.:2.30176, Test Accuracy:0.11350, Max Test Accuracy:0.11350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0, Dropout      -Epoch: 49, Train Err.:0.08921, Validation Err.:0.14939, Test Accuracy:0.97320, Max Test Accuracy:0.97430\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1, Dropout      -Epoch: 49, Train Err.:0.72479, Validation Err.:0.43674, Test Accuracy:0.92830, Max Test Accuracy:0.93020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch_list = []\n",
    "\n",
    "num_batch = math.ceil(train_size / batch_size)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    epoch_list.append(i)\n",
    "    for key in markers.keys():\n",
    "        for k in range(num_batch):\n",
    "            x_batch = img_train[k * batch_size : k * batch_size + batch_size]\n",
    "            t_batch = label_train[k * batch_size : k * batch_size + batch_size]\n",
    "            networks[key].learning(x_batch, t_batch)\n",
    "\n",
    "        train_loss = networks[key].loss(x_batch, t_batch, is_train=True)\n",
    "        train_errors[key].append(train_loss)\n",
    "\n",
    "        validation_loss = networks[key].loss(img_validation, label_validation, is_train=False)\n",
    "        validation_errors[key].append(validation_loss)    \n",
    "\n",
    "        test_accuracy = networks[key].accuracy(img_test, label_test)\n",
    "        test_accuracy_values[key].append(test_accuracy)\n",
    "        if test_accuracy > max_test_accuracy_value[key]:\n",
    "            max_test_accuracy_epoch[key] = i\n",
    "            max_test_accuracy_value[key] = test_accuracy\n",
    "        print(\"{0:50s}-Epoch:{1:3d}, Train Err.:{2:7.5f}, Validation Err.:{3:7.5f}, Test Accuracy:{4:7.5f}, Max Test Accuracy:{5:7.5f}\".format(\n",
    "            key,\n",
    "            i,\n",
    "            train_loss,\n",
    "            validation_loss,\n",
    "            test_accuracy,\n",
    "            max_test_accuracy_value[key]\n",
    "        ))\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (49,) and (53,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8cb5fc411f1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmarkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_errors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkevery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train - Total Error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yhhan/anaconda/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1890\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1891\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1892\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1893\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yhhan/anaconda/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1407\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yhhan/anaconda/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yhhan/anaconda/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yhhan/anaconda/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 244\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (49,) and (53,)"
     ]
    }
   ],
   "source": [
    "f, axarr = plt.subplots(2, 2, figsize=(20, 12))\n",
    "for key in markers.keys():\n",
    "    axarr[0, 0].plot(epoch_list[1:], train_errors[key][1:], marker=markers[key], markevery=2, label=key)\n",
    "axarr[0, 0].set_ylabel('Train - Total Error')\n",
    "axarr[0, 0].set_xlabel('Epochs')\n",
    "axarr[0, 0].grid(True)\n",
    "axarr[0, 0].set_title('Train Error')\n",
    "axarr[0, 0].legend(loc='upper right')\n",
    "\n",
    "for key in markers.keys():\n",
    "    axarr[0, 1].plot(epoch_list[1:], validation_errors[key][1:], marker=markers[key], markevery=2, label=key)\n",
    "axarr[0, 1].set_ylabel('Validation - Total Error')\n",
    "axarr[0, 1].set_xlabel('Epochs')\n",
    "axarr[0, 1].grid(True)\n",
    "axarr[0, 1].set_title('Validation Error')\n",
    "axarr[0, 1].legend(loc='upper right')\n",
    "\n",
    "for key in markers.keys():\n",
    "    axarr[1, 0].plot(epoch_list[1:], train_errors[key][1:], marker=markers[key], markevery=2, label=key)\n",
    "axarr[1, 0].set_ylabel('Train - Total Error')\n",
    "axarr[1, 0].set_xlabel('Epochs')\n",
    "axarr[1, 0].grid(True)\n",
    "axarr[1, 0].set_ylim(0, 0.2)\n",
    "axarr[1, 0].set_title('Train Error (0.00 ~ 3.00)')\n",
    "axarr[1, 0].legend(loc='upper right')\n",
    "\n",
    "for key in markers.keys():\n",
    "    axarr[1, 1].plot(epoch_list[1:], validation_errors[key][1:], marker=markers[key], markevery=2, label=key)\n",
    "axarr[1, 1].set_ylabel('Validation - Total Error')\n",
    "axarr[1, 1].set_xlabel('Epochs')\n",
    "axarr[1, 1].grid(True)\n",
    "axarr[1, 1].set_ylim(0, 0.2)\n",
    "axarr[1, 1].set_title('Validation Error (0.00 ~ 1.00)')\n",
    "axarr[1, 1].legend(loc='upper right')\n",
    "\n",
    "f.subplots_adjust(hspace=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(2, 1, figsize=(15,10))\n",
    "for key in markers.keys():\n",
    "    axarr[0].plot(epoch_list[1:], test_accuracy_values[key][1:], marker=markers[key], markevery=1, label=key)\n",
    "axarr[0].set_ylabel('Test Accuracy')\n",
    "axarr[0].set_xlabel('Epochs')\n",
    "axarr[0].grid(True)\n",
    "axarr[0].set_title('Test Accuracy')\n",
    "axarr[0].legend(loc='lower right')\n",
    "\n",
    "for key in markers.keys():\n",
    "    axarr[1].plot(epoch_list[1:], test_accuracy_values[key][1:], marker=markers[key], markevery=1, label=key)\n",
    "axarr[1].set_ylabel('Test Accuracy')\n",
    "axarr[1].set_xlabel('Epochs')\n",
    "axarr[1].grid(True)\n",
    "axarr[1].set_ylim(0.94, 0.99)\n",
    "axarr[1].set_title('Test Accuracy (0.7 ~ 1.0)')\n",
    "axarr[1].legend(loc='lower right')\n",
    "\n",
    "f.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in markers.keys():\n",
    "    print(\"{0:26s} - Epoch:{1:3d}, Max Test Accuracy: {2:7.5f}\".format(key, max_test_accuracy_epoch[key], max_test_accuracy_value[key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
