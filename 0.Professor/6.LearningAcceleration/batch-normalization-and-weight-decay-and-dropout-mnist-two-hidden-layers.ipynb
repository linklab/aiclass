{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST-Neural Network-Batch Normalization, Weight Decay and Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "from common.mnist import *\n",
    "from common.networks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Neural Network Model (Two Hidden Layers) and Learing/Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "\n",
    "class Initializer:\n",
    "    def __init__(self, params, params_size_list, use_batch_normalization=False):\n",
    "        self.params = params\n",
    "        self.params_size_list = params_size_list\n",
    "        self.use_batch_normalization = use_batch_normalization\n",
    "\n",
    "    def initialize_params(self):\n",
    "        pass\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "\n",
    "\n",
    "class Zero_Initializer(Initializer):\n",
    "    def initialize_params(self, use_batch_normalization):\n",
    "        for idx in range(1, len(self.params_size_list)):\n",
    "            self.params['W' + str(idx)] = np.zeros(self.params_size_list[idx - 1], self.params_size_list[idx])\n",
    "            self.params['b' + str(idx)] = np.zeros(self.params_size_list[idx])\n",
    "            if self.use_batch_normalization and idx < len(self.params_size_list) - 1:\n",
    "                self.params['gamma' + str(idx)] = np.zeros(self.params_size_list[idx])\n",
    "                self.params['beta' + str(idx)] = np.zeros(self.params_size_list[idx])\n",
    "\n",
    "class N1_Initializer(Initializer):\n",
    "    def initialize_params(self):\n",
    "        for idx in range(1, len(self.params_size_list)):\n",
    "            self.params['W' + str(idx)] = np.random.randn(self.params_size_list[idx - 1], self.params_size_list[idx])\n",
    "            self.params['b' + str(idx)] = np.random.randn(self.params_size_list[idx])\n",
    "            if self.use_batch_normalization and idx < len(self.params_size_list) - 1:\n",
    "                self.params['gamma' + str(idx)] = np.random.randn(self.params_size_list[idx])\n",
    "                self.params['beta' + str(idx)] = np.random.randn(self.params_size_list[idx])\n",
    "\n",
    "class N2_Initializer(Initializer):\n",
    "    def initialize_params(self):\n",
    "        for idx in range(1, len(self.params_size_list)):\n",
    "            self.params['W' + str(idx)] = np.random.randn(self.params_size_list[idx - 1], self.params_size_list[idx]) * 0.01\n",
    "            self.params['b' + str(idx)] = np.random.randn(self.params_size_list[idx]) * 0.01\n",
    "            if self.use_batch_normalization and idx < len(self.params_size_list) - 1:\n",
    "                self.params['gamma' + str(idx)] = np.random.randn(self.params_size_list[idx]) * 0.01\n",
    "                self.params['beta' + str(idx)] = np.random.randn(self.params_size_list[idx]) * 0.01\n",
    "\n",
    "class Xavier_Initializer(Initializer):\n",
    "    def initialize_params(self):\n",
    "        for idx in range(1, len(self.params_size_list)):\n",
    "            self.params['W' + str(idx)] = np.random.randn(self.params_size_list[idx - 1], self.params_size_list[idx]) / np.sqrt(self.params_size_list[idx - 1])\n",
    "            self.params['b' + str(idx)] = np.random.randn(self.params_size_list[idx]) / np.sqrt(self.params_size_list[idx - 1])\n",
    "            if self.use_batch_normalization and idx < len(self.params_size_list) - 1:\n",
    "                self.params['gamma' + str(idx)] = np.random.randn(self.params_size_list[idx]) / np.sqrt(self.params_size_list[idx - 1])\n",
    "                self.params['beta' + str(idx)] = np.random.randn(self.params_size_list[idx]) / np.sqrt(self.params_size_list[idx - 1])\n",
    "\n",
    "\n",
    "class He_Initializer(Initializer):\n",
    "    def initialize_params(self):\n",
    "        for idx in range(1, len(self.params_size_list)):\n",
    "            self.params['W' + str(idx)] = np.random.randn(self.params_size_list[idx - 1], self.params_size_list[idx]) * np.sqrt(2) / np.sqrt(self.params_size_list[idx - 1])\n",
    "            self.params['b' + str(idx)] = np.random.randn(self.params_size_list[idx]) * np.sqrt(2) / np.sqrt(self.params_size_list[idx - 1])\n",
    "            if self.use_batch_normalization and idx < len(self.params_size_list) - 1:\n",
    "                self.params['gamma' + str(idx)] = np.random.randn(self.params_size_list[idx]) * np.sqrt(2) / np.sqrt(self.params_size_list[idx - 1])\n",
    "                self.params['beta' + str(idx)] = np.random.randn(self.params_size_list[idx]) * np.sqrt(2) / np.sqrt(self.params_size_list[idx - 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Layer - Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.momentum = momentum\n",
    "        self.input_shape = None\n",
    "\n",
    "        self.running_mean = running_mean\n",
    "        self.running_var = running_var  \n",
    "        \n",
    "        self.batch_size = None\n",
    "        self.xc = None\n",
    "        self.std = None\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "\n",
    "    def forward(self, x, is_train=True):\n",
    "        self.input_shape = x.shape\n",
    "        if x.ndim != 2:\n",
    "            N, C, H, W = x.shape\n",
    "            x = x.reshape(N, -1)\n",
    "\n",
    "        out = self.__forward(x, is_train)\n",
    "        \n",
    "        return out.reshape(*self.input_shape)\n",
    "            \n",
    "    def __forward(self, x, is_train):\n",
    "        if self.running_mean is None:\n",
    "            N, D = x.shape\n",
    "            self.running_mean = np.zeros(D)\n",
    "            self.running_var = np.zeros(D)\n",
    "                        \n",
    "        if is_train:\n",
    "            mu = x.mean(axis=0)\n",
    "            xc = x - mu\n",
    "            var = np.mean(xc**2, axis=0)\n",
    "            std = np.sqrt(var + 10e-7)\n",
    "            xn = xc / std\n",
    "            \n",
    "            self.batch_size = x.shape[0]\n",
    "            self.xc = xc\n",
    "            self.xn = xn\n",
    "            self.std = std\n",
    "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
    "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
    "        else:\n",
    "            xc = x - self.running_mean\n",
    "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
    "            \n",
    "        out = self.gamma * xn + self.beta \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        if dout.ndim != 2:\n",
    "            N, C, H, W = dout.shape\n",
    "            dout = dout.reshape(N, -1)\n",
    "\n",
    "        dx = self.__backward(dout)\n",
    "\n",
    "        dx = dx.reshape(*self.input_shape)\n",
    "        return dx\n",
    "\n",
    "    def __backward(self, dout):\n",
    "        dbeta = dout.sum(axis=0)\n",
    "        dgamma = np.sum(self.xn * dout, axis=0)\n",
    "        dxn = self.gamma * dout\n",
    "        dxc = dxn / self.std\n",
    "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
    "        dvar = 0.5 * dstd / self.std\n",
    "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
    "        dmu = np.sum(dxc, axis=0)\n",
    "        dx = dxc - dmu / self.batch_size\n",
    "        \n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "activation_layers = {\n",
    "    'Sigmoid': Sigmoid,\n",
    "    'ReLU': ReLU\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "    \"SGD\": SGD,\n",
    "    \"Momentum\": Momentum,\n",
    "    \"Nesterov\": Nesterov,\n",
    "    \"AdaGrad\": AdaGrad,\n",
    "    \"RMSprop\": RMSprop,\n",
    "    \"Adam\": Adam\n",
    "}\n",
    "\n",
    "initializers = {\n",
    "    'Zero': Zero_Initializer,\n",
    "    'N1': N1_Initializer,\n",
    "    'N2': N2_Initializer, # We will use this as a new initializer for supporting Batch Normalization\n",
    "    'Xavier': Xavier_Initializer,\n",
    "    'He': He_Initializer\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiLayerNetExtended(MultiLayerNet):\n",
    "    def __init__(self, input_size, hidden_size_list, output_size, activation='ReLU', initializer='N2', \n",
    "                 optimizer='AdaGrad', learning_rate=0.01, use_batch_normalization=False, weight_decay_lambda=0.0):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size_list = hidden_size_list\n",
    "        self.hidden_layer_num = len(hidden_size_list)\n",
    "        \n",
    "        self.use_batch_normalization = use_batch_normalization\n",
    "\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "            \n",
    "        # Weight Initialization\n",
    "        self.params = {}\n",
    "        self.weight_initialization(initializer)\n",
    "        \n",
    "        # Layering\n",
    "        self.layers = OrderedDict()\n",
    "        self.last_layer = None\n",
    "        self.layering(activation)\n",
    "\n",
    "        # Optimization Method\n",
    "        self.optimizer = optimizers[optimizer](lr=learning_rate)\n",
    "    \n",
    "    def weight_initialization(self, initializer):\n",
    "        params_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
    "        initializer_obj = initializers[initializer](self.params, \n",
    "                                                    params_size_list, \n",
    "                                                    self.use_batch_normalization)\n",
    "        initializer_obj.initialize_params();\n",
    "        \n",
    "    def layering(self, activation):\n",
    "        for idx in range(1, self.hidden_layer_num + 1):\n",
    "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
    "            if self.use_batch_normalization:\n",
    "                self.layers['Batch_Normalization' + str(idx)] = BatchNormalization(self.params['gamma' + str(idx)], \n",
    "                                                                                   self.params['beta' + str(idx)])\n",
    "            self.layers['Activation' + str(idx)] = activation_layers[activation]()\n",
    "\n",
    "        idx = self.hidden_layer_num + 1\n",
    "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
    "\n",
    "        self.last_layer = SoftmaxWithCrossEntropyLoss()  \n",
    "\n",
    "    def predict(self, x, is_train=False):\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"BatchNorm\" in key:\n",
    "                x = layer.forward(x, is_train)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t, is_train=False):\n",
    "        y = self.predict(x, is_train)\n",
    "\n",
    "        weight_decay = 0.0\n",
    "        for idx in range(1, self.hidden_layer_num + 2):\n",
    "            W = self.params['W' + str(idx)]\n",
    "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
    "            \n",
    "        return self.last_layer.forward(y, t) + weight_decay\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x, is_train=False)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy        \n",
    "\n",
    "    def backpropagation_gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t, is_train=True)\n",
    "\n",
    "        # backward\n",
    "        din = 1\n",
    "        din = self.last_layer.backward(din)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            din = layer.backward(din)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num + 2):\n",
    "            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.params['W' + str(idx)]\n",
    "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
    "\n",
    "            if self.use_batch_normalization and idx <= self.hidden_layer_num:\n",
    "                grads['gamma' + str(idx)] = self.layers['Batch_Normalization' + str(idx)].dgamma\n",
    "                grads['beta' + str(idx)] = self.layers['Batch_Normalization' + str(idx)].dbeta\n",
    "                \n",
    "        return grads\n",
    "\n",
    "    def learning(self, x_batch, t_batch):\n",
    "        grads = self.backpropagation_gradient(x_batch, t_batch)\n",
    "        self.optimizer.update(self.params, grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = mnist_data(\"/Users/yhhan/git/aiclass/0.Professor/data/MNIST_data/.\")\n",
    "(img_train, label_train), (img_validation, label_validation), (img_test, label_test) = data.load_mnist(flatten=True, normalize=True, one_hot_label=True)\n",
    "\n",
    "input_size=784\n",
    "hidden_layer1_size=128\n",
    "hidden_layer2_size=128\n",
    "output_size=10\n",
    "\n",
    "num_epochs = 50\n",
    "train_size = img_train.shape[0]\n",
    "batch_size = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "markers = {\"N2, AdaGrad, No_Batch_Norm, lambda=0.0\": \"+\", \n",
    "           \"N2, AdaGrad, Batch_Norm, lambda=0.0\": \"*\", \n",
    "           \"N2, AdaGrad, Batch_Norm, lambda=0.1\": \"o\",\n",
    "           \"He, AdaGrad, No_Batch_Norm, lambda=0.0\": \"x\", \n",
    "           \"He, AdaGrad, Batch_Norm, lambda=0.0\": \"h\", \n",
    "           \"He, AdaGrad, Batch_Norm, lambda=0.1\": \"H\"}\n",
    "\n",
    "networks = {}\n",
    "train_errors = {}\n",
    "validation_errors = {}\n",
    "test_accuracy_values = {}\n",
    "max_test_accuracy_epoch = {}\n",
    "max_test_accuracy_value = {}\n",
    "\n",
    "for key in markers.keys():\n",
    "    if key == \"N2, AdaGrad, No_Batch_Norm, lambda=0.0\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='N2',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=False, weight_decay_lambda=0.0)\n",
    "    elif key == \"N2, AdaGrad, Batch_Norm, lambda=0.0\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='N2',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=True, weight_decay_lambda=0.0)\n",
    "    elif key == \"N2, AdaGrad, Batch_Norm, lambda=0.1\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='N2',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=True, weight_decay_lambda=0.1)        \n",
    "    elif key == \"He, AdaGrad, No_Batch_Norm, lambda=0.0\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='He',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=False, weight_decay_lambda=0.0)\n",
    "    elif key == \"He, AdaGrad, Batch_Norm, lambda=0.0\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='He',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=True, weight_decay_lambda=0.0)\n",
    "    elif key == \"He, AdaGrad, Batch_Norm, lambda=0.1\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='He',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=True, weight_decay_lambda=0.1)        \n",
    "        \n",
    "    train_errors[key] = [] \n",
    "    validation_errors[key] = []\n",
    "    test_accuracy_values[key] = []\n",
    "    max_test_accuracy_epoch[key] = 0\n",
    "    max_test_accuracy_value[key] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  0, Train Err.:0.06041, Validation Err.:0.11439, Test Accuracy:0.96100, Max Test Accuracy:0.96100\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  0, Train Err.:0.01168, Validation Err.:0.07741, Test Accuracy:0.97520, Max Test Accuracy:0.97520\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  0, Train Err.:0.55573, Validation Err.:0.58598, Test Accuracy:0.90100, Max Test Accuracy:0.90380\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  0, Train Err.:0.12401, Validation Err.:0.14348, Test Accuracy:0.94780, Max Test Accuracy:0.94780\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  0, Train Err.:0.02844, Validation Err.:0.07994, Test Accuracy:0.97280, Max Test Accuracy:0.97280\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  0, Train Err.:0.68769, Validation Err.:0.70762, Test Accuracy:0.89880, Max Test Accuracy:0.89880\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  1, Train Err.:0.05216, Validation Err.:0.10956, Test Accuracy:0.96310, Max Test Accuracy:0.96310\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  1, Train Err.:0.00850, Validation Err.:0.07827, Test Accuracy:0.97570, Max Test Accuracy:0.97570\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  1, Train Err.:0.49540, Validation Err.:0.54307, Test Accuracy:0.90880, Max Test Accuracy:0.90880\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  1, Train Err.:0.10311, Validation Err.:0.13668, Test Accuracy:0.95250, Max Test Accuracy:0.95250\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  1, Train Err.:0.01782, Validation Err.:0.07669, Test Accuracy:0.97460, Max Test Accuracy:0.97460\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  1, Train Err.:0.55961, Validation Err.:0.59761, Test Accuracy:0.90380, Max Test Accuracy:0.90380\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  2, Train Err.:0.04593, Validation Err.:0.10712, Test Accuracy:0.96310, Max Test Accuracy:0.96310\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  2, Train Err.:0.00605, Validation Err.:0.07968, Test Accuracy:0.97570, Max Test Accuracy:0.97570\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  2, Train Err.:0.46216, Validation Err.:0.50615, Test Accuracy:0.91620, Max Test Accuracy:0.91620\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  2, Train Err.:0.08939, Validation Err.:0.12966, Test Accuracy:0.95530, Max Test Accuracy:0.95530\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  2, Train Err.:0.01199, Validation Err.:0.07801, Test Accuracy:0.97460, Max Test Accuracy:0.97460\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  2, Train Err.:0.53538, Validation Err.:0.55485, Test Accuracy:0.90340, Max Test Accuracy:0.90380\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  3, Train Err.:0.04035, Validation Err.:0.10591, Test Accuracy:0.96420, Max Test Accuracy:0.96420\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  3, Train Err.:0.00454, Validation Err.:0.08142, Test Accuracy:0.97640, Max Test Accuracy:0.97640\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  3, Train Err.:0.44171, Validation Err.:0.49057, Test Accuracy:0.92050, Max Test Accuracy:0.92050\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  3, Train Err.:0.07839, Validation Err.:0.12428, Test Accuracy:0.95700, Max Test Accuracy:0.95700\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  3, Train Err.:0.00854, Validation Err.:0.08004, Test Accuracy:0.97610, Max Test Accuracy:0.97610\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  3, Train Err.:0.49754, Validation Err.:0.51410, Test Accuracy:0.90990, Max Test Accuracy:0.90990\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  4, Train Err.:0.03636, Validation Err.:0.10582, Test Accuracy:0.96430, Max Test Accuracy:0.96430\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  4, Train Err.:0.00359, Validation Err.:0.08254, Test Accuracy:0.97670, Max Test Accuracy:0.97670\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  4, Train Err.:0.42633, Validation Err.:0.46511, Test Accuracy:0.92320, Max Test Accuracy:0.92320\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  4, Train Err.:0.06981, Validation Err.:0.12016, Test Accuracy:0.95840, Max Test Accuracy:0.95840\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  4, Train Err.:0.00594, Validation Err.:0.08243, Test Accuracy:0.97740, Max Test Accuracy:0.97740\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  4, Train Err.:0.46589, Validation Err.:0.48235, Test Accuracy:0.91320, Max Test Accuracy:0.91320\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  5, Train Err.:0.03335, Validation Err.:0.10535, Test Accuracy:0.96510, Max Test Accuracy:0.96510\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  5, Train Err.:0.00284, Validation Err.:0.08465, Test Accuracy:0.97740, Max Test Accuracy:0.97740\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  5, Train Err.:0.41592, Validation Err.:0.45249, Test Accuracy:0.92320, Max Test Accuracy:0.92320\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  5, Train Err.:0.06403, Validation Err.:0.11812, Test Accuracy:0.95910, Max Test Accuracy:0.95910\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  5, Train Err.:0.00476, Validation Err.:0.08494, Test Accuracy:0.97730, Max Test Accuracy:0.97740\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  5, Train Err.:0.44399, Validation Err.:0.46606, Test Accuracy:0.91590, Max Test Accuracy:0.91590\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  6, Train Err.:0.03095, Validation Err.:0.10593, Test Accuracy:0.96600, Max Test Accuracy:0.96600\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  6, Train Err.:0.00226, Validation Err.:0.08648, Test Accuracy:0.97700, Max Test Accuracy:0.97740\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  6, Train Err.:0.43398, Validation Err.:0.46241, Test Accuracy:0.91900, Max Test Accuracy:0.92320\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  6, Train Err.:0.05827, Validation Err.:0.11613, Test Accuracy:0.95980, Max Test Accuracy:0.95980\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  6, Train Err.:0.00385, Validation Err.:0.08805, Test Accuracy:0.97740, Max Test Accuracy:0.97740\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  6, Train Err.:0.40772, Validation Err.:0.44445, Test Accuracy:0.92310, Max Test Accuracy:0.92310\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  7, Train Err.:0.02889, Validation Err.:0.10672, Test Accuracy:0.96570, Max Test Accuracy:0.96600\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  7, Train Err.:0.00187, Validation Err.:0.08821, Test Accuracy:0.97680, Max Test Accuracy:0.97740\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  7, Train Err.:0.36979, Validation Err.:0.41517, Test Accuracy:0.92940, Max Test Accuracy:0.92940\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  7, Train Err.:0.05311, Validation Err.:0.11441, Test Accuracy:0.96020, Max Test Accuracy:0.96020\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  7, Train Err.:0.00313, Validation Err.:0.08994, Test Accuracy:0.97720, Max Test Accuracy:0.97740\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  7, Train Err.:0.38940, Validation Err.:0.43249, Test Accuracy:0.92390, Max Test Accuracy:0.92390\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  8, Train Err.:0.02641, Validation Err.:0.10658, Test Accuracy:0.96600, Max Test Accuracy:0.96600\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  8, Train Err.:0.00157, Validation Err.:0.09041, Test Accuracy:0.97690, Max Test Accuracy:0.97740\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  8, Train Err.:0.36202, Validation Err.:0.41017, Test Accuracy:0.93240, Max Test Accuracy:0.93240\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  8, Train Err.:0.04865, Validation Err.:0.11374, Test Accuracy:0.96040, Max Test Accuracy:0.96040\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  8, Train Err.:0.00259, Validation Err.:0.09267, Test Accuracy:0.97770, Max Test Accuracy:0.97770\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  8, Train Err.:0.37690, Validation Err.:0.41706, Test Accuracy:0.92750, Max Test Accuracy:0.92750\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  9, Train Err.:0.02423, Validation Err.:0.10787, Test Accuracy:0.96630, Max Test Accuracy:0.96630\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  9, Train Err.:0.00132, Validation Err.:0.09227, Test Accuracy:0.97750, Max Test Accuracy:0.97750\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  9, Train Err.:0.38505, Validation Err.:0.42015, Test Accuracy:0.92470, Max Test Accuracy:0.93240\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  9, Train Err.:0.04551, Validation Err.:0.11252, Test Accuracy:0.96080, Max Test Accuracy:0.96080\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  9, Train Err.:0.00218, Validation Err.:0.09427, Test Accuracy:0.97770, Max Test Accuracy:0.97770\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  9, Train Err.:0.36565, Validation Err.:0.40450, Test Accuracy:0.92800, Max Test Accuracy:0.92800\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 10, Train Err.:0.02277, Validation Err.:0.10895, Test Accuracy:0.96630, Max Test Accuracy:0.96630\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 10, Train Err.:0.00113, Validation Err.:0.09397, Test Accuracy:0.97770, Max Test Accuracy:0.97770\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 10, Train Err.:0.35580, Validation Err.:0.39945, Test Accuracy:0.93280, Max Test Accuracy:0.93280\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 10, Train Err.:0.04215, Validation Err.:0.11159, Test Accuracy:0.96200, Max Test Accuracy:0.96200\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 10, Train Err.:0.00183, Validation Err.:0.09764, Test Accuracy:0.97810, Max Test Accuracy:0.97810\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 10, Train Err.:0.34770, Validation Err.:0.39170, Test Accuracy:0.92710, Max Test Accuracy:0.92800\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 11, Train Err.:0.02114, Validation Err.:0.10927, Test Accuracy:0.96650, Max Test Accuracy:0.96650\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 11, Train Err.:0.00099, Validation Err.:0.09548, Test Accuracy:0.97800, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 11, Train Err.:0.35673, Validation Err.:0.40182, Test Accuracy:0.92970, Max Test Accuracy:0.93280\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 11, Train Err.:0.03813, Validation Err.:0.11003, Test Accuracy:0.96220, Max Test Accuracy:0.96220\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 11, Train Err.:0.00157, Validation Err.:0.10043, Test Accuracy:0.97780, Max Test Accuracy:0.97810\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 11, Train Err.:0.34636, Validation Err.:0.38585, Test Accuracy:0.92860, Max Test Accuracy:0.92860\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 12, Train Err.:0.01963, Validation Err.:0.11079, Test Accuracy:0.96680, Max Test Accuracy:0.96680\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 12, Train Err.:0.00087, Validation Err.:0.09661, Test Accuracy:0.97800, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 12, Train Err.:0.33497, Validation Err.:0.38160, Test Accuracy:0.93260, Max Test Accuracy:0.93280\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 12, Train Err.:0.03624, Validation Err.:0.10949, Test Accuracy:0.96270, Max Test Accuracy:0.96270\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 12, Train Err.:0.00135, Validation Err.:0.10096, Test Accuracy:0.97800, Max Test Accuracy:0.97810\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 12, Train Err.:0.34901, Validation Err.:0.38715, Test Accuracy:0.92830, Max Test Accuracy:0.92860\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 13, Train Err.:0.01857, Validation Err.:0.11180, Test Accuracy:0.96680, Max Test Accuracy:0.96680\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 13, Train Err.:0.00078, Validation Err.:0.09755, Test Accuracy:0.97790, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 13, Train Err.:0.32111, Validation Err.:0.36978, Test Accuracy:0.93540, Max Test Accuracy:0.93540\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 13, Train Err.:0.03257, Validation Err.:0.10977, Test Accuracy:0.96350, Max Test Accuracy:0.96350\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 13, Train Err.:0.00117, Validation Err.:0.10295, Test Accuracy:0.97790, Max Test Accuracy:0.97810\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 13, Train Err.:0.33375, Validation Err.:0.37321, Test Accuracy:0.93070, Max Test Accuracy:0.93070\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 14, Train Err.:0.01857, Validation Err.:0.11330, Test Accuracy:0.96640, Max Test Accuracy:0.96680\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 14, Train Err.:0.00071, Validation Err.:0.09857, Test Accuracy:0.97770, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 14, Train Err.:0.32400, Validation Err.:0.37271, Test Accuracy:0.93190, Max Test Accuracy:0.93540\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 14, Train Err.:0.03068, Validation Err.:0.11039, Test Accuracy:0.96390, Max Test Accuracy:0.96390\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 14, Train Err.:0.00104, Validation Err.:0.10400, Test Accuracy:0.97810, Max Test Accuracy:0.97810\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 14, Train Err.:0.33732, Validation Err.:0.37638, Test Accuracy:0.93010, Max Test Accuracy:0.93070\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 15, Train Err.:0.01671, Validation Err.:0.11389, Test Accuracy:0.96700, Max Test Accuracy:0.96700\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 15, Train Err.:0.00064, Validation Err.:0.09928, Test Accuracy:0.97740, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 15, Train Err.:0.30741, Validation Err.:0.35774, Test Accuracy:0.93720, Max Test Accuracy:0.93720\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 15, Train Err.:0.02872, Validation Err.:0.11122, Test Accuracy:0.96470, Max Test Accuracy:0.96470\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 15, Train Err.:0.00093, Validation Err.:0.10539, Test Accuracy:0.97840, Max Test Accuracy:0.97840\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 15, Train Err.:0.32128, Validation Err.:0.36582, Test Accuracy:0.93260, Max Test Accuracy:0.93260\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 16, Train Err.:0.01569, Validation Err.:0.11567, Test Accuracy:0.96740, Max Test Accuracy:0.96740\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 16, Train Err.:0.00059, Validation Err.:0.10013, Test Accuracy:0.97730, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 16, Train Err.:0.29793, Validation Err.:0.34993, Test Accuracy:0.93800, Max Test Accuracy:0.93800\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 16, Train Err.:0.02745, Validation Err.:0.11086, Test Accuracy:0.96580, Max Test Accuracy:0.96580\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 16, Train Err.:0.00085, Validation Err.:0.10657, Test Accuracy:0.97870, Max Test Accuracy:0.97870\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 16, Train Err.:0.30992, Validation Err.:0.35831, Test Accuracy:0.93280, Max Test Accuracy:0.93280\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 17, Train Err.:0.01496, Validation Err.:0.11755, Test Accuracy:0.96780, Max Test Accuracy:0.96780\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 17, Train Err.:0.00054, Validation Err.:0.10085, Test Accuracy:0.97730, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 17, Train Err.:0.29077, Validation Err.:0.34561, Test Accuracy:0.93640, Max Test Accuracy:0.93800\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 17, Train Err.:0.02646, Validation Err.:0.11166, Test Accuracy:0.96520, Max Test Accuracy:0.96580\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 17, Train Err.:0.00077, Validation Err.:0.10766, Test Accuracy:0.97850, Max Test Accuracy:0.97870\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 17, Train Err.:0.31117, Validation Err.:0.35711, Test Accuracy:0.93310, Max Test Accuracy:0.93310\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 18, Train Err.:0.01446, Validation Err.:0.11952, Test Accuracy:0.96780, Max Test Accuracy:0.96780\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 18, Train Err.:0.00050, Validation Err.:0.10155, Test Accuracy:0.97730, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 18, Train Err.:0.30347, Validation Err.:0.35260, Test Accuracy:0.93420, Max Test Accuracy:0.93800\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 18, Train Err.:0.02519, Validation Err.:0.11295, Test Accuracy:0.96530, Max Test Accuracy:0.96580\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 18, Train Err.:0.00070, Validation Err.:0.10878, Test Accuracy:0.97870, Max Test Accuracy:0.97870\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 18, Train Err.:0.30661, Validation Err.:0.35264, Test Accuracy:0.93440, Max Test Accuracy:0.93440\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 19, Train Err.:0.01357, Validation Err.:0.12138, Test Accuracy:0.96780, Max Test Accuracy:0.96780\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 19, Train Err.:0.00047, Validation Err.:0.10222, Test Accuracy:0.97740, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 19, Train Err.:0.28546, Validation Err.:0.34025, Test Accuracy:0.94080, Max Test Accuracy:0.94080\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 19, Train Err.:0.02426, Validation Err.:0.11418, Test Accuracy:0.96540, Max Test Accuracy:0.96580\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 19, Train Err.:0.00065, Validation Err.:0.11001, Test Accuracy:0.97870, Max Test Accuracy:0.97870\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 19, Train Err.:0.29900, Validation Err.:0.34730, Test Accuracy:0.93490, Max Test Accuracy:0.93490\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 20, Train Err.:0.01288, Validation Err.:0.12286, Test Accuracy:0.96790, Max Test Accuracy:0.96790\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 20, Train Err.:0.00044, Validation Err.:0.10290, Test Accuracy:0.97760, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 20, Train Err.:0.29053, Validation Err.:0.34380, Test Accuracy:0.93850, Max Test Accuracy:0.94080\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 20, Train Err.:0.02338, Validation Err.:0.11525, Test Accuracy:0.96630, Max Test Accuracy:0.96630\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 20, Train Err.:0.00060, Validation Err.:0.11102, Test Accuracy:0.97860, Max Test Accuracy:0.97870\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 20, Train Err.:0.29751, Validation Err.:0.34336, Test Accuracy:0.93430, Max Test Accuracy:0.93490\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 21, Train Err.:0.01246, Validation Err.:0.12535, Test Accuracy:0.96840, Max Test Accuracy:0.96840\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 21, Train Err.:0.00041, Validation Err.:0.10355, Test Accuracy:0.97760, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 21, Train Err.:0.28172, Validation Err.:0.33433, Test Accuracy:0.94080, Max Test Accuracy:0.94080\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 21, Train Err.:0.02245, Validation Err.:0.11669, Test Accuracy:0.96680, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 21, Train Err.:0.00055, Validation Err.:0.11200, Test Accuracy:0.97860, Max Test Accuracy:0.97870\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 21, Train Err.:0.28842, Validation Err.:0.33846, Test Accuracy:0.93620, Max Test Accuracy:0.93620\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 22, Train Err.:0.01202, Validation Err.:0.12763, Test Accuracy:0.96800, Max Test Accuracy:0.96840\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 22, Train Err.:0.00039, Validation Err.:0.10414, Test Accuracy:0.97780, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 22, Train Err.:0.28924, Validation Err.:0.34156, Test Accuracy:0.93750, Max Test Accuracy:0.94080\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 22, Train Err.:0.02136, Validation Err.:0.11811, Test Accuracy:0.96610, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 22, Train Err.:0.00051, Validation Err.:0.11299, Test Accuracy:0.97890, Max Test Accuracy:0.97890\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 22, Train Err.:0.29010, Validation Err.:0.33798, Test Accuracy:0.93610, Max Test Accuracy:0.93620\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 23, Train Err.:0.01158, Validation Err.:0.12922, Test Accuracy:0.96850, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 23, Train Err.:0.00037, Validation Err.:0.10475, Test Accuracy:0.97770, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 23, Train Err.:0.28477, Validation Err.:0.34366, Test Accuracy:0.93610, Max Test Accuracy:0.94080\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 23, Train Err.:0.02080, Validation Err.:0.11929, Test Accuracy:0.96640, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 23, Train Err.:0.00048, Validation Err.:0.11397, Test Accuracy:0.97890, Max Test Accuracy:0.97890\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 23, Train Err.:0.27909, Validation Err.:0.33027, Test Accuracy:0.93670, Max Test Accuracy:0.93670\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 24, Train Err.:0.01105, Validation Err.:0.13181, Test Accuracy:0.96840, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 24, Train Err.:0.00035, Validation Err.:0.10532, Test Accuracy:0.97750, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 24, Train Err.:0.27989, Validation Err.:0.33286, Test Accuracy:0.93800, Max Test Accuracy:0.94080\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 24, Train Err.:0.01984, Validation Err.:0.12114, Test Accuracy:0.96620, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 24, Train Err.:0.00045, Validation Err.:0.11477, Test Accuracy:0.97900, Max Test Accuracy:0.97900\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 24, Train Err.:0.27777, Validation Err.:0.32986, Test Accuracy:0.93610, Max Test Accuracy:0.93670\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 25, Train Err.:0.01068, Validation Err.:0.13320, Test Accuracy:0.96800, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 25, Train Err.:0.00033, Validation Err.:0.10590, Test Accuracy:0.97740, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 25, Train Err.:0.27398, Validation Err.:0.33323, Test Accuracy:0.93840, Max Test Accuracy:0.94080\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 25, Train Err.:0.01891, Validation Err.:0.12193, Test Accuracy:0.96620, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 25, Train Err.:0.00042, Validation Err.:0.11568, Test Accuracy:0.97890, Max Test Accuracy:0.97900\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 25, Train Err.:0.27610, Validation Err.:0.32644, Test Accuracy:0.93680, Max Test Accuracy:0.93680\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 26, Train Err.:0.01042, Validation Err.:0.13537, Test Accuracy:0.96750, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 26, Train Err.:0.00031, Validation Err.:0.10651, Test Accuracy:0.97740, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 26, Train Err.:0.26224, Validation Err.:0.32239, Test Accuracy:0.94080, Max Test Accuracy:0.94080\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 26, Train Err.:0.01843, Validation Err.:0.12320, Test Accuracy:0.96600, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 26, Train Err.:0.00040, Validation Err.:0.11649, Test Accuracy:0.97870, Max Test Accuracy:0.97900\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 26, Train Err.:0.27477, Validation Err.:0.32387, Test Accuracy:0.93690, Max Test Accuracy:0.93690\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 27, Train Err.:0.01008, Validation Err.:0.13575, Test Accuracy:0.96740, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 27, Train Err.:0.00030, Validation Err.:0.10699, Test Accuracy:0.97730, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 27, Train Err.:0.26714, Validation Err.:0.32760, Test Accuracy:0.93950, Max Test Accuracy:0.94080\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 27, Train Err.:0.01751, Validation Err.:0.12337, Test Accuracy:0.96580, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 27, Train Err.:0.00038, Validation Err.:0.11714, Test Accuracy:0.97890, Max Test Accuracy:0.97900\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 27, Train Err.:0.27319, Validation Err.:0.32133, Test Accuracy:0.93700, Max Test Accuracy:0.93700\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 28, Train Err.:0.00965, Validation Err.:0.13740, Test Accuracy:0.96730, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 28, Train Err.:0.00028, Validation Err.:0.10749, Test Accuracy:0.97720, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 28, Train Err.:0.26257, Validation Err.:0.32570, Test Accuracy:0.94120, Max Test Accuracy:0.94120\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 28, Train Err.:0.01678, Validation Err.:0.12429, Test Accuracy:0.96590, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 28, Train Err.:0.00036, Validation Err.:0.11791, Test Accuracy:0.97900, Max Test Accuracy:0.97900\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 28, Train Err.:0.26352, Validation Err.:0.31839, Test Accuracy:0.93780, Max Test Accuracy:0.93780\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 29, Train Err.:0.00927, Validation Err.:0.13921, Test Accuracy:0.96740, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 29, Train Err.:0.00027, Validation Err.:0.10801, Test Accuracy:0.97710, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 29, Train Err.:0.26048, Validation Err.:0.32180, Test Accuracy:0.94140, Max Test Accuracy:0.94140\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 29, Train Err.:0.01610, Validation Err.:0.12545, Test Accuracy:0.96620, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 29, Train Err.:0.00034, Validation Err.:0.11858, Test Accuracy:0.97890, Max Test Accuracy:0.97900\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 29, Train Err.:0.26346, Validation Err.:0.31569, Test Accuracy:0.93770, Max Test Accuracy:0.93780\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 30, Train Err.:0.00903, Validation Err.:0.14111, Test Accuracy:0.96720, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 30, Train Err.:0.00026, Validation Err.:0.10851, Test Accuracy:0.97700, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 30, Train Err.:0.26720, Validation Err.:0.32476, Test Accuracy:0.93950, Max Test Accuracy:0.94140\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 30, Train Err.:0.01562, Validation Err.:0.12711, Test Accuracy:0.96550, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 30, Train Err.:0.00032, Validation Err.:0.11930, Test Accuracy:0.97890, Max Test Accuracy:0.97900\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 30, Train Err.:0.26779, Validation Err.:0.31440, Test Accuracy:0.93800, Max Test Accuracy:0.93800\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 31, Train Err.:0.00879, Validation Err.:0.14300, Test Accuracy:0.96700, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 31, Train Err.:0.00025, Validation Err.:0.10899, Test Accuracy:0.97700, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 31, Train Err.:0.25544, Validation Err.:0.31661, Test Accuracy:0.94270, Max Test Accuracy:0.94270\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 31, Train Err.:0.01477, Validation Err.:0.12728, Test Accuracy:0.96540, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 31, Train Err.:0.00031, Validation Err.:0.11993, Test Accuracy:0.97900, Max Test Accuracy:0.97900\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 31, Train Err.:0.26684, Validation Err.:0.30973, Test Accuracy:0.93920, Max Test Accuracy:0.93920\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 32, Train Err.:0.00828, Validation Err.:0.14419, Test Accuracy:0.96690, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 32, Train Err.:0.00024, Validation Err.:0.10942, Test Accuracy:0.97700, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 32, Train Err.:0.25294, Validation Err.:0.31536, Test Accuracy:0.94170, Max Test Accuracy:0.94270\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 32, Train Err.:0.01444, Validation Err.:0.12820, Test Accuracy:0.96550, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 32, Train Err.:0.00030, Validation Err.:0.12056, Test Accuracy:0.97910, Max Test Accuracy:0.97910\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 32, Train Err.:0.26104, Validation Err.:0.31096, Test Accuracy:0.93760, Max Test Accuracy:0.93920\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 33, Train Err.:0.00771, Validation Err.:0.14581, Test Accuracy:0.96750, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 33, Train Err.:0.00023, Validation Err.:0.10988, Test Accuracy:0.97700, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 33, Train Err.:0.24934, Validation Err.:0.30897, Test Accuracy:0.94410, Max Test Accuracy:0.94410\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 33, Train Err.:0.01331, Validation Err.:0.12924, Test Accuracy:0.96590, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 33, Train Err.:0.00028, Validation Err.:0.12126, Test Accuracy:0.97910, Max Test Accuracy:0.97910\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 33, Train Err.:0.26261, Validation Err.:0.31289, Test Accuracy:0.93580, Max Test Accuracy:0.93920\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 34, Train Err.:0.00720, Validation Err.:0.14858, Test Accuracy:0.96760, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 34, Train Err.:0.00022, Validation Err.:0.11034, Test Accuracy:0.97690, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 34, Train Err.:0.25553, Validation Err.:0.31494, Test Accuracy:0.94200, Max Test Accuracy:0.94410\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 34, Train Err.:0.01300, Validation Err.:0.13060, Test Accuracy:0.96580, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 34, Train Err.:0.00027, Validation Err.:0.12188, Test Accuracy:0.97900, Max Test Accuracy:0.97910\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 34, Train Err.:0.25960, Validation Err.:0.31031, Test Accuracy:0.93730, Max Test Accuracy:0.93920\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 35, Train Err.:0.00680, Validation Err.:0.15029, Test Accuracy:0.96750, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 35, Train Err.:0.00021, Validation Err.:0.11076, Test Accuracy:0.97690, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 35, Train Err.:0.25156, Validation Err.:0.30839, Test Accuracy:0.94270, Max Test Accuracy:0.94410\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 35, Train Err.:0.01221, Validation Err.:0.13080, Test Accuracy:0.96580, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 35, Train Err.:0.00026, Validation Err.:0.12242, Test Accuracy:0.97890, Max Test Accuracy:0.97910\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 35, Train Err.:0.25813, Validation Err.:0.30820, Test Accuracy:0.93840, Max Test Accuracy:0.93920\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 36, Train Err.:0.00641, Validation Err.:0.15121, Test Accuracy:0.96720, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 36, Train Err.:0.00021, Validation Err.:0.11117, Test Accuracy:0.97690, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 36, Train Err.:0.26677, Validation Err.:0.32253, Test Accuracy:0.93910, Max Test Accuracy:0.94410\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 36, Train Err.:0.01155, Validation Err.:0.13117, Test Accuracy:0.96530, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 36, Train Err.:0.00025, Validation Err.:0.12303, Test Accuracy:0.97880, Max Test Accuracy:0.97910\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 36, Train Err.:0.25358, Validation Err.:0.30582, Test Accuracy:0.93910, Max Test Accuracy:0.93920\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 37, Train Err.:0.00613, Validation Err.:0.15291, Test Accuracy:0.96680, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 37, Train Err.:0.00020, Validation Err.:0.11156, Test Accuracy:0.97680, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 37, Train Err.:0.25006, Validation Err.:0.30793, Test Accuracy:0.94210, Max Test Accuracy:0.94410\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 37, Train Err.:0.01109, Validation Err.:0.13240, Test Accuracy:0.96500, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 37, Train Err.:0.00024, Validation Err.:0.12347, Test Accuracy:0.97870, Max Test Accuracy:0.97910\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 37, Train Err.:0.25122, Validation Err.:0.30497, Test Accuracy:0.93920, Max Test Accuracy:0.93920\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 38, Train Err.:0.00576, Validation Err.:0.15613, Test Accuracy:0.96740, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 38, Train Err.:0.00019, Validation Err.:0.11196, Test Accuracy:0.97680, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 38, Train Err.:0.24775, Validation Err.:0.30694, Test Accuracy:0.94330, Max Test Accuracy:0.94410\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 38, Train Err.:0.01067, Validation Err.:0.13327, Test Accuracy:0.96510, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 38, Train Err.:0.00023, Validation Err.:0.12410, Test Accuracy:0.97870, Max Test Accuracy:0.97910\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 38, Train Err.:0.25283, Validation Err.:0.30387, Test Accuracy:0.93970, Max Test Accuracy:0.93970\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 39, Train Err.:0.00518, Validation Err.:0.15634, Test Accuracy:0.96750, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 39, Train Err.:0.00019, Validation Err.:0.11237, Test Accuracy:0.97680, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 39, Train Err.:0.24810, Validation Err.:0.30319, Test Accuracy:0.94250, Max Test Accuracy:0.94410\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 39, Train Err.:0.01016, Validation Err.:0.13365, Test Accuracy:0.96470, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 39, Train Err.:0.00022, Validation Err.:0.12451, Test Accuracy:0.97870, Max Test Accuracy:0.97910\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 39, Train Err.:0.25065, Validation Err.:0.30192, Test Accuracy:0.93990, Max Test Accuracy:0.93990\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 40, Train Err.:0.00500, Validation Err.:0.15897, Test Accuracy:0.96680, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 40, Train Err.:0.00018, Validation Err.:0.11272, Test Accuracy:0.97690, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 40, Train Err.:0.25013, Validation Err.:0.30352, Test Accuracy:0.94300, Max Test Accuracy:0.94410\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 40, Train Err.:0.00953, Validation Err.:0.13484, Test Accuracy:0.96500, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 40, Train Err.:0.00022, Validation Err.:0.12510, Test Accuracy:0.97860, Max Test Accuracy:0.97910\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 40, Train Err.:0.24753, Validation Err.:0.29966, Test Accuracy:0.94130, Max Test Accuracy:0.94130\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 41, Train Err.:0.00478, Validation Err.:0.16151, Test Accuracy:0.96690, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 41, Train Err.:0.00018, Validation Err.:0.11312, Test Accuracy:0.97670, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 41, Train Err.:0.24772, Validation Err.:0.30235, Test Accuracy:0.94200, Max Test Accuracy:0.94410\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 41, Train Err.:0.00914, Validation Err.:0.13555, Test Accuracy:0.96490, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 41, Train Err.:0.00021, Validation Err.:0.12551, Test Accuracy:0.97860, Max Test Accuracy:0.97910\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 41, Train Err.:0.24258, Validation Err.:0.29955, Test Accuracy:0.93960, Max Test Accuracy:0.94130\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 42, Train Err.:0.00438, Validation Err.:0.16236, Test Accuracy:0.96720, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 42, Train Err.:0.00017, Validation Err.:0.11347, Test Accuracy:0.97680, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 42, Train Err.:0.23925, Validation Err.:0.29688, Test Accuracy:0.94440, Max Test Accuracy:0.94440\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 42, Train Err.:0.00857, Validation Err.:0.13738, Test Accuracy:0.96460, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 42, Train Err.:0.00020, Validation Err.:0.12605, Test Accuracy:0.97870, Max Test Accuracy:0.97910\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 42, Train Err.:0.24173, Validation Err.:0.29587, Test Accuracy:0.94100, Max Test Accuracy:0.94130\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 43, Train Err.:0.00407, Validation Err.:0.16536, Test Accuracy:0.96660, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 43, Train Err.:0.00017, Validation Err.:0.11385, Test Accuracy:0.97670, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 43, Train Err.:0.24288, Validation Err.:0.29512, Test Accuracy:0.94250, Max Test Accuracy:0.94440\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 43, Train Err.:0.00833, Validation Err.:0.13757, Test Accuracy:0.96490, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 43, Train Err.:0.00020, Validation Err.:0.12647, Test Accuracy:0.97860, Max Test Accuracy:0.97910\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 43, Train Err.:0.23990, Validation Err.:0.29752, Test Accuracy:0.94010, Max Test Accuracy:0.94130\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 44, Train Err.:0.00402, Validation Err.:0.16709, Test Accuracy:0.96690, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 44, Train Err.:0.00016, Validation Err.:0.11417, Test Accuracy:0.97660, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 44, Train Err.:0.24861, Validation Err.:0.29623, Test Accuracy:0.94250, Max Test Accuracy:0.94440\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 44, Train Err.:0.00770, Validation Err.:0.13927, Test Accuracy:0.96550, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 44, Train Err.:0.00019, Validation Err.:0.12692, Test Accuracy:0.97850, Max Test Accuracy:0.97910\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 44, Train Err.:0.23697, Validation Err.:0.29623, Test Accuracy:0.94010, Max Test Accuracy:0.94130\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 45, Train Err.:0.00368, Validation Err.:0.16701, Test Accuracy:0.96640, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 45, Train Err.:0.00016, Validation Err.:0.11452, Test Accuracy:0.97670, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 45, Train Err.:0.23586, Validation Err.:0.28870, Test Accuracy:0.94630, Max Test Accuracy:0.94630\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 45, Train Err.:0.00756, Validation Err.:0.13909, Test Accuracy:0.96570, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 45, Train Err.:0.00018, Validation Err.:0.12740, Test Accuracy:0.97850, Max Test Accuracy:0.97910\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 45, Train Err.:0.23659, Validation Err.:0.29649, Test Accuracy:0.94080, Max Test Accuracy:0.94130\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 46, Train Err.:0.00356, Validation Err.:0.16917, Test Accuracy:0.96650, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 46, Train Err.:0.00015, Validation Err.:0.11483, Test Accuracy:0.97660, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 46, Train Err.:0.23903, Validation Err.:0.28935, Test Accuracy:0.94510, Max Test Accuracy:0.94630\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 46, Train Err.:0.00706, Validation Err.:0.14038, Test Accuracy:0.96500, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 46, Train Err.:0.00018, Validation Err.:0.12780, Test Accuracy:0.97840, Max Test Accuracy:0.97910\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 46, Train Err.:0.23588, Validation Err.:0.29494, Test Accuracy:0.94060, Max Test Accuracy:0.94130\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 47, Train Err.:0.00337, Validation Err.:0.17024, Test Accuracy:0.96650, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 47, Train Err.:0.00015, Validation Err.:0.11515, Test Accuracy:0.97660, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 47, Train Err.:0.24100, Validation Err.:0.29041, Test Accuracy:0.94320, Max Test Accuracy:0.94630\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 47, Train Err.:0.00687, Validation Err.:0.14143, Test Accuracy:0.96520, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 47, Train Err.:0.00017, Validation Err.:0.12824, Test Accuracy:0.97840, Max Test Accuracy:0.97910\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 47, Train Err.:0.23342, Validation Err.:0.29409, Test Accuracy:0.94060, Max Test Accuracy:0.94130\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 48, Train Err.:0.00318, Validation Err.:0.17146, Test Accuracy:0.96630, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 48, Train Err.:0.00014, Validation Err.:0.11550, Test Accuracy:0.97660, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 48, Train Err.:0.22546, Validation Err.:0.28130, Test Accuracy:0.94600, Max Test Accuracy:0.94630\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 48, Train Err.:0.00648, Validation Err.:0.14273, Test Accuracy:0.96540, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 48, Train Err.:0.00017, Validation Err.:0.12867, Test Accuracy:0.97840, Max Test Accuracy:0.97910\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 48, Train Err.:0.23261, Validation Err.:0.29273, Test Accuracy:0.94050, Max Test Accuracy:0.94130\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 49, Train Err.:0.00311, Validation Err.:0.17322, Test Accuracy:0.96650, Max Test Accuracy:0.96850\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 49, Train Err.:0.00014, Validation Err.:0.11583, Test Accuracy:0.97660, Max Test Accuracy:0.97800\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 49, Train Err.:0.23309, Validation Err.:0.28606, Test Accuracy:0.94460, Max Test Accuracy:0.94630\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch: 49, Train Err.:0.00623, Validation Err.:0.14365, Test Accuracy:0.96510, Max Test Accuracy:0.96680\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch: 49, Train Err.:0.00017, Validation Err.:0.12912, Test Accuracy:0.97840, Max Test Accuracy:0.97910\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch: 49, Train Err.:0.23221, Validation Err.:0.29177, Test Accuracy:0.94180, Max Test Accuracy:0.94180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch_list = []\n",
    "\n",
    "num_batch = math.ceil(train_size / batch_size)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    epoch_list.append(i)\n",
    "    for key in markers.keys():\n",
    "        for k in range(num_batch):\n",
    "            x_batch = img_train[k * batch_size : k * batch_size + batch_size]\n",
    "            t_batch = label_train[k * batch_size : k * batch_size + batch_size]\n",
    "            networks[key].learning(x_batch, t_batch)\n",
    "\n",
    "        train_loss = networks[key].loss(x_batch, t_batch, is_train=True)\n",
    "        train_errors[key].append(train_loss)\n",
    "\n",
    "        validation_loss = networks[key].loss(img_validation, label_validation, is_train=False)\n",
    "        validation_errors[key].append(validation_loss)    \n",
    "\n",
    "        test_accuracy = networks[key].accuracy(img_test, label_test)\n",
    "        test_accuracy_values[key].append(test_accuracy)\n",
    "        if test_accuracy > max_test_accuracy_value[key]:\n",
    "            max_test_accuracy_epoch[key] = i\n",
    "            max_test_accuracy_value[key] = test_accuracy\n",
    "        print(\"{0:38s}-Epoch:{1:3d}, Train Err.:{2:7.5f}, Validation Err.:{3:7.5f}, Test Accuracy:{4:7.5f}, Max Test Accuracy:{5:7.5f}\".format(\n",
    "            key,\n",
    "            i,\n",
    "            train_loss,\n",
    "            validation_loss,\n",
    "            test_accuracy,\n",
    "            max_test_accuracy_value[key]\n",
    "        ))\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (49,) and (53,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8cb5fc411f1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmarkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_errors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkevery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train - Total Error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yhhan/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1890\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1891\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1892\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1893\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yhhan/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1407\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yhhan/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yhhan/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yhhan/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 244\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (49,) and (53,)"
     ]
    }
   ],
   "source": [
    "f, axarr = plt.subplots(2, 2, figsize=(20, 12))\n",
    "for key in markers.keys():\n",
    "    axarr[0, 0].plot(epoch_list[1:], train_errors[key][1:], marker=markers[key], markevery=2, label=key)\n",
    "axarr[0, 0].set_ylabel('Train - Total Error')\n",
    "axarr[0, 0].set_xlabel('Epochs')\n",
    "axarr[0, 0].grid(True)\n",
    "axarr[0, 0].set_title('Train Error')\n",
    "axarr[0, 0].legend(loc='upper right')\n",
    "\n",
    "for key in markers.keys():\n",
    "    axarr[0, 1].plot(epoch_list[1:], validation_errors[key][1:], marker=markers[key], markevery=2, label=key)\n",
    "axarr[0, 1].set_ylabel('Validation - Total Error')\n",
    "axarr[0, 1].set_xlabel('Epochs')\n",
    "axarr[0, 1].grid(True)\n",
    "axarr[0, 1].set_title('Validation Error')\n",
    "axarr[0, 1].legend(loc='upper right')\n",
    "\n",
    "for key in markers.keys():\n",
    "    axarr[1, 0].plot(epoch_list[1:], train_errors[key][1:], marker=markers[key], markevery=2, label=key)\n",
    "axarr[1, 0].set_ylabel('Train - Total Error')\n",
    "axarr[1, 0].set_xlabel('Epochs')\n",
    "axarr[1, 0].grid(True)\n",
    "axarr[1, 0].set_ylim(0, 0.2)\n",
    "axarr[1, 0].set_title('Train Error (0.00 ~ 3.00)')\n",
    "axarr[1, 0].legend(loc='upper right')\n",
    "\n",
    "for key in markers.keys():\n",
    "    axarr[1, 1].plot(epoch_list[1:], validation_errors[key][1:], marker=markers[key], markevery=2, label=key)\n",
    "axarr[1, 1].set_ylabel('Validation - Total Error')\n",
    "axarr[1, 1].set_xlabel('Epochs')\n",
    "axarr[1, 1].grid(True)\n",
    "axarr[1, 1].set_ylim(0, 0.2)\n",
    "axarr[1, 1].set_title('Validation Error (0.00 ~ 1.00)')\n",
    "axarr[1, 1].legend(loc='upper right')\n",
    "\n",
    "f.subplots_adjust(hspace=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (49,) and (53,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-632223a15c99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmarkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkevery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yhhan/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1890\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1891\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1892\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1893\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yhhan/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1407\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yhhan/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yhhan/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yhhan/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 244\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (49,) and (53,)"
     ]
    }
   ],
   "source": [
    "f, axarr = plt.subplots(2, 1, figsize=(15,10))\n",
    "for key in markers.keys():\n",
    "    axarr[0].plot(epoch_list[1:], test_accuracy_values[key][1:], marker=markers[key], markevery=1, label=key)\n",
    "axarr[0].set_ylabel('Test Accuracy')\n",
    "axarr[0].set_xlabel('Epochs')\n",
    "axarr[0].grid(True)\n",
    "axarr[0].set_title('Test Accuracy')\n",
    "axarr[0].legend(loc='lower right')\n",
    "\n",
    "for key in markers.keys():\n",
    "    axarr[1].plot(epoch_list[1:], test_accuracy_values[key][1:], marker=markers[key], markevery=1, label=key)\n",
    "axarr[1].set_ylabel('Test Accuracy')\n",
    "axarr[1].set_xlabel('Epochs')\n",
    "axarr[1].grid(True)\n",
    "axarr[1].set_ylim(0.94, 0.99)\n",
    "axarr[1].set_title('Test Accuracy (0.7 ~ 1.0)')\n",
    "axarr[1].legend(loc='lower right')\n",
    "\n",
    "f.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in markers.keys():\n",
    "    print(\"{0:26s} - Epoch:{1:3d}, Max Test Accuracy: {2:7.5f}\".format(key, max_test_accuracy_epoch[key], max_test_accuracy_value[key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
