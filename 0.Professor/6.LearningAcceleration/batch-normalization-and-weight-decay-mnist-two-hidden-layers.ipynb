{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST-Neural Network-Batch Normalization and Weight Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "from common.mnist import *\n",
    "from common.networks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Neural Network Model (Two Hidden Layers) and Learing/Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "\n",
    "class Initializer:\n",
    "    def __init__(self, params, params_size_list, use_batch_normalization=False):\n",
    "        self.params = params\n",
    "        self.params_size_list = params_size_list\n",
    "        self.use_batch_normalization = use_batch_normalization\n",
    "\n",
    "    def initialize_params(self):\n",
    "        pass\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "\n",
    "\n",
    "class Zero_Initializer(Initializer):\n",
    "    def initialize_params(self, use_batch_normalization):\n",
    "        for idx in range(1, len(self.params_size_list)):\n",
    "            self.params['W' + str(idx)] = np.zeros(self.params_size_list[idx - 1], self.params_size_list[idx])\n",
    "            self.params['b' + str(idx)] = np.zeros(self.params_size_list[idx])\n",
    "            if self.use_batch_normalization and idx < len(self.params_size_list) - 1:\n",
    "                self.params['gamma' + str(idx)] = np.zeros(self.params_size_list[idx])\n",
    "                self.params['beta' + str(idx)] = np.zeros(self.params_size_list[idx])\n",
    "\n",
    "class N1_Initializer(Initializer):\n",
    "    def initialize_params(self):\n",
    "        for idx in range(1, len(self.params_size_list)):\n",
    "            self.params['W' + str(idx)] = np.random.randn(self.params_size_list[idx - 1], self.params_size_list[idx])\n",
    "            self.params['b' + str(idx)] = np.random.randn(self.params_size_list[idx])\n",
    "            if self.use_batch_normalization and idx < len(self.params_size_list) - 1:\n",
    "                self.params['gamma' + str(idx)] = np.random.randn(self.params_size_list[idx])\n",
    "                self.params['beta' + str(idx)] = np.random.randn(self.params_size_list[idx])\n",
    "\n",
    "class N2_Initializer(Initializer):\n",
    "    def initialize_params(self):\n",
    "        for idx in range(1, len(self.params_size_list)):\n",
    "            self.params['W' + str(idx)] = np.random.randn(self.params_size_list[idx - 1], self.params_size_list[idx]) * 0.01\n",
    "            self.params['b' + str(idx)] = np.random.randn(self.params_size_list[idx]) * 0.01\n",
    "            if self.use_batch_normalization and idx < len(self.params_size_list) - 1:\n",
    "                self.params['gamma' + str(idx)] = np.random.randn(self.params_size_list[idx]) * 0.01\n",
    "                self.params['beta' + str(idx)] = np.random.randn(self.params_size_list[idx]) * 0.01\n",
    "\n",
    "class Xavier_Initializer(Initializer):\n",
    "    def initialize_params(self):\n",
    "        for idx in range(1, len(self.params_size_list)):\n",
    "            self.params['W' + str(idx)] = np.random.randn(self.params_size_list[idx - 1], self.params_size_list[idx]) / np.sqrt(self.params_size_list[idx - 1])\n",
    "            self.params['b' + str(idx)] = np.random.randn(self.params_size_list[idx]) / np.sqrt(self.params_size_list[idx - 1])\n",
    "            if self.use_batch_normalization and idx < len(self.params_size_list) - 1:\n",
    "                self.params['gamma' + str(idx)] = np.random.randn(self.params_size_list[idx]) / np.sqrt(self.params_size_list[idx - 1])\n",
    "                self.params['beta' + str(idx)] = np.random.randn(self.params_size_list[idx]) / np.sqrt(self.params_size_list[idx - 1])\n",
    "\n",
    "\n",
    "class He_Initializer(Initializer):\n",
    "    def initialize_params(self):\n",
    "        for idx in range(1, len(self.params_size_list)):\n",
    "            self.params['W' + str(idx)] = np.random.randn(self.params_size_list[idx - 1], self.params_size_list[idx]) * np.sqrt(2) / np.sqrt(self.params_size_list[idx - 1])\n",
    "            self.params['b' + str(idx)] = np.random.randn(self.params_size_list[idx]) * np.sqrt(2) / np.sqrt(self.params_size_list[idx - 1])\n",
    "            if self.use_batch_normalization and idx < len(self.params_size_list) - 1:\n",
    "                self.params['gamma' + str(idx)] = np.random.randn(self.params_size_list[idx]) * np.sqrt(2) / np.sqrt(self.params_size_list[idx - 1])\n",
    "                self.params['beta' + str(idx)] = np.random.randn(self.params_size_list[idx]) * np.sqrt(2) / np.sqrt(self.params_size_list[idx - 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Layer - Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.momentum = momentum\n",
    "        self.input_shape = None\n",
    "\n",
    "        self.running_mean = running_mean\n",
    "        self.running_var = running_var  \n",
    "        \n",
    "        self.batch_size = None\n",
    "        self.xc = None\n",
    "        self.std = None\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "\n",
    "    def forward(self, x, is_train=True):\n",
    "        self.input_shape = x.shape\n",
    "        if x.ndim != 2:\n",
    "            N, C, H, W = x.shape\n",
    "            x = x.reshape(N, -1)\n",
    "\n",
    "        out = self.__forward(x, is_train)\n",
    "        \n",
    "        return out.reshape(*self.input_shape)\n",
    "            \n",
    "    def __forward(self, x, is_train):\n",
    "        if self.running_mean is None:\n",
    "            N, D = x.shape\n",
    "            self.running_mean = np.zeros(D)\n",
    "            self.running_var = np.zeros(D)\n",
    "                        \n",
    "        if is_train:\n",
    "            mu = x.mean(axis=0)\n",
    "            xc = x - mu\n",
    "            var = np.mean(xc**2, axis=0)\n",
    "            std = np.sqrt(var + 10e-7)\n",
    "            xn = xc / std\n",
    "            \n",
    "            self.batch_size = x.shape[0]\n",
    "            self.xc = xc\n",
    "            self.xn = xn\n",
    "            self.std = std\n",
    "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
    "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
    "        else:\n",
    "            xc = x - self.running_mean\n",
    "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
    "            \n",
    "        out = self.gamma * xn + self.beta \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        if dout.ndim != 2:\n",
    "            N, C, H, W = dout.shape\n",
    "            dout = dout.reshape(N, -1)\n",
    "\n",
    "        dx = self.__backward(dout)\n",
    "\n",
    "        dx = dx.reshape(*self.input_shape)\n",
    "        return dx\n",
    "\n",
    "    def __backward(self, dout):\n",
    "        dbeta = dout.sum(axis=0)\n",
    "        dgamma = np.sum(self.xn * dout, axis=0)\n",
    "        dxn = self.gamma * dout\n",
    "        dxc = dxn / self.std\n",
    "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
    "        dvar = 0.5 * dstd / self.std\n",
    "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
    "        dmu = np.sum(dxc, axis=0)\n",
    "        dx = dxc - dmu / self.batch_size\n",
    "        \n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "activation_layers = {\n",
    "    'Sigmoid': Sigmoid,\n",
    "    'ReLU': ReLU\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "    \"SGD\": SGD,\n",
    "    \"Momentum\": Momentum,\n",
    "    \"Nesterov\": Nesterov,\n",
    "    \"AdaGrad\": AdaGrad,\n",
    "    \"RMSprop\": RMSprop,\n",
    "    \"Adam\": Adam\n",
    "}\n",
    "\n",
    "initializers = {\n",
    "    'Zero': Zero_Initializer,\n",
    "    'N1': N1_Initializer,\n",
    "    'N2': N2_Initializer, # We will use this as a new initializer for supporting Batch Normalization\n",
    "    'Xavier': Xavier_Initializer,\n",
    "    'He': He_Initializer\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiLayerNetExtended(MultiLayerNet):\n",
    "    def __init__(self, input_size, hidden_size_list, output_size, activation='ReLU', initializer='N2', \n",
    "                 optimizer='AdaGrad', learning_rate=0.01, \n",
    "                 use_batch_normalization=False, \n",
    "                 use_weight_decay=False, weight_decay_lambda=0.0):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size_list = hidden_size_list\n",
    "        self.hidden_layer_num = len(hidden_size_list)\n",
    "        \n",
    "        self.use_batch_normalization = use_batch_normalization\n",
    "\n",
    "        self.use_weight_decay = use_weight_decay\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "            \n",
    "        # Weight Initialization\n",
    "        self.params = {}\n",
    "        self.weight_initialization(initializer)\n",
    "        \n",
    "        # Layering\n",
    "        self.layers = OrderedDict()\n",
    "        self.last_layer = None\n",
    "        self.layering(activation)\n",
    "\n",
    "        # Optimization Method\n",
    "        self.optimizer = optimizers[optimizer](lr=learning_rate)\n",
    "    \n",
    "    def weight_initialization(self, initializer):\n",
    "        params_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
    "        initializer_obj = initializers[initializer](self.params, \n",
    "                                                    params_size_list, \n",
    "                                                    self.use_batch_normalization)\n",
    "        initializer_obj.initialize_params();\n",
    "        \n",
    "    def layering(self, activation):\n",
    "        for idx in range(1, self.hidden_layer_num + 1):\n",
    "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
    "            if self.use_batch_normalization:\n",
    "                self.layers['Batch_Normalization' + str(idx)] = BatchNormalization(self.params['gamma' + str(idx)], \n",
    "                                                                                   self.params['beta' + str(idx)])\n",
    "            self.layers['Activation' + str(idx)] = activation_layers[activation]()\n",
    "\n",
    "        idx = self.hidden_layer_num + 1\n",
    "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
    "\n",
    "        self.last_layer = SoftmaxWithCrossEntropyLoss()  \n",
    "\n",
    "    def predict(self, x, is_train=False):\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"BatchNorm\" in key:\n",
    "                x = layer.forward(x, is_train)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t, is_train=False):\n",
    "        y = self.predict(x, is_train)\n",
    "\n",
    "        if self.use_weight_decay:\n",
    "            weight_decay = 0.0\n",
    "            for idx in range(1, self.hidden_layer_num + 2):\n",
    "                W = self.params['W' + str(idx)]\n",
    "                weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
    "            return self.last_layer.forward(y, t) + weight_decay\n",
    "        else:\n",
    "            return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x, is_train=False)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy        \n",
    "\n",
    "    def backpropagation_gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t, is_train=True)\n",
    "\n",
    "        # backward\n",
    "        din = 1\n",
    "        din = self.last_layer.backward(din)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            din = layer.backward(din)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num + 2):\n",
    "            if self.use_weight_decay:\n",
    "                grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.params['W' + str(idx)]\n",
    "            else:\n",
    "                grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW\n",
    "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
    "\n",
    "            if self.use_batch_normalization and idx <= self.hidden_layer_num:\n",
    "                grads['gamma' + str(idx)] = self.layers['Batch_Normalization' + str(idx)].dgamma\n",
    "                grads['beta' + str(idx)] = self.layers['Batch_Normalization' + str(idx)].dbeta\n",
    "                \n",
    "        return grads\n",
    "\n",
    "    def learning(self, x_batch, t_batch):\n",
    "        grads = self.backpropagation_gradient(x_batch, t_batch)\n",
    "        self.optimizer.update(self.params, grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = mnist_data(\"/Users/yhhan/git/aiclass/0.Professor/data/MNIST_data/.\")\n",
    "(img_train, label_train), (img_validation, label_validation), (img_test, label_test) = data.load_mnist(flatten=True, normalize=True, one_hot_label=True)\n",
    "\n",
    "input_size=784\n",
    "hidden_layer1_size=128\n",
    "hidden_layer2_size=128\n",
    "output_size=10\n",
    "\n",
    "num_epochs = 50\n",
    "train_size = img_train.shape[0]\n",
    "batch_size = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "markers = {\"N2, AdaGrad, No_Batch_Norm, lambda=0.0\": \"+\", \n",
    "           \"N2, AdaGrad, Batch_Norm, lambda=0.0\": \"*\", \n",
    "           \"N2, AdaGrad, Batch_Norm, lambda=0.1\": \"o\",\n",
    "           \"He, AdaGrad, No_Batch_Norm, lambda=0.0\": \"x\", \n",
    "           \"He, AdaGrad, Batch_Norm, lambda=0.0\": \"h\", \n",
    "           \"He, AdaGrad, Batch_Norm, lambda=0.1\": \"H\"}\n",
    "\n",
    "networks = {}\n",
    "train_errors = {}\n",
    "validation_errors = {}\n",
    "test_accuracy_values = {}\n",
    "max_test_accuracy_epoch = {}\n",
    "max_test_accuracy_value = {}\n",
    "\n",
    "for key in markers.keys():\n",
    "    if key == \"N2, AdaGrad, No_Batch_Norm, lambda=0.0\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='N2',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=False, \n",
    "                                use_weight_decay=False, weight_decay_lambda=0.0)\n",
    "    elif key == \"N2, AdaGrad, Batch_Norm, lambda=0.0\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='N2',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=True,\n",
    "                                use_weight_decay=False, weight_decay_lambda=0.0)\n",
    "    elif key == \"N2, AdaGrad, Batch_Norm, lambda=0.1\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='N2',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=True,\n",
    "                                use_weight_decay=True, weight_decay_lambda=0.1)        \n",
    "    elif key == \"He, AdaGrad, No_Batch_Norm, lambda=0.0\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='He',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=False,\n",
    "                                use_weight_decay=False, weight_decay_lambda=0.0)\n",
    "    elif key == \"He, AdaGrad, Batch_Norm, lambda=0.0\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='He',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=True,\n",
    "                                use_weight_decay=False, weight_decay_lambda=0.0)\n",
    "    elif key == \"He, AdaGrad, Batch_Norm, lambda=0.1\":\n",
    "        networks[key] = MultiLayerNetExtended(input_size, [hidden_layer1_size, hidden_layer2_size], output_size, \n",
    "                                activation='ReLU', \n",
    "                                initializer='He',\n",
    "                                optimizer='AdaGrad', learning_rate=learning_rate,\n",
    "                                use_batch_normalization=True,\n",
    "                                use_weight_decay=True, weight_decay_lambda=0.1)        \n",
    "        \n",
    "    train_errors[key] = [] \n",
    "    validation_errors[key] = []\n",
    "    test_accuracy_values[key] = []\n",
    "    max_test_accuracy_epoch[key] = 0\n",
    "    max_test_accuracy_value[key] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  0, Train Err.:0.29420, Validation Err.:0.27180, Test Accuracy:0.90270, Max Test Accuracy:0.90270\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  0, Train Err.:0.09101, Validation Err.:0.11990, Test Accuracy:0.95630, Max Test Accuracy:0.95630\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  0, Train Err.:1.17575, Validation Err.:1.21322, Test Accuracy:0.77510, Max Test Accuracy:0.77510\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  0, Train Err.:0.24563, Validation Err.:0.21871, Test Accuracy:0.91570, Max Test Accuracy:0.91570\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  0, Train Err.:0.07612, Validation Err.:0.11275, Test Accuracy:0.95690, Max Test Accuracy:0.95690\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  0, Train Err.:1.06066, Validation Err.:1.08043, Test Accuracy:0.81700, Max Test Accuracy:0.81700\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  1, Train Err.:0.18584, Validation Err.:0.19378, Test Accuracy:0.93030, Max Test Accuracy:0.93030\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  1, Train Err.:0.04513, Validation Err.:0.09069, Test Accuracy:0.96600, Max Test Accuracy:0.96600\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  1, Train Err.:0.76966, Validation Err.:0.80488, Test Accuracy:0.87300, Max Test Accuracy:0.87300\n",
      "He, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  1, Train Err.:0.15998, Validation Err.:0.15808, Test Accuracy:0.93800, Max Test Accuracy:0.93800\n",
      "He, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  1, Train Err.:0.03489, Validation Err.:0.08802, Test Accuracy:0.96620, Max Test Accuracy:0.96620\n",
      "He, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  1, Train Err.:0.78362, Validation Err.:0.80771, Test Accuracy:0.87490, Max Test Accuracy:0.87490\n",
      "\n",
      "N2, AdaGrad, No_Batch_Norm, lambda=0.0-Epoch:  2, Train Err.:0.13651, Validation Err.:0.17047, Test Accuracy:0.93700, Max Test Accuracy:0.93700\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.0   -Epoch:  2, Train Err.:0.02978, Validation Err.:0.08239, Test Accuracy:0.97130, Max Test Accuracy:0.97130\n",
      "N2, AdaGrad, Batch_Norm, lambda=0.1   -Epoch:  2, Train Err.:0.57870, Validation Err.:0.62202, Test Accuracy:0.90760, Max Test Accuracy:0.90760\n"
     ]
    }
   ],
   "source": [
    "epoch_list = []\n",
    "\n",
    "num_batch = math.ceil(train_size / batch_size)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    epoch_list.append(i)\n",
    "    for key in markers.keys():\n",
    "        for k in range(num_batch):\n",
    "            x_batch = img_train[k * batch_size : k * batch_size + batch_size]\n",
    "            t_batch = label_train[k * batch_size : k * batch_size + batch_size]\n",
    "            networks[key].learning(x_batch, t_batch)\n",
    "\n",
    "        train_loss = networks[key].loss(x_batch, t_batch, is_train=True)\n",
    "        train_errors[key].append(train_loss)\n",
    "\n",
    "        validation_loss = networks[key].loss(img_validation, label_validation, is_train=False)\n",
    "        validation_errors[key].append(validation_loss)    \n",
    "\n",
    "        test_accuracy = networks[key].accuracy(img_test, label_test)\n",
    "        test_accuracy_values[key].append(test_accuracy)\n",
    "        if test_accuracy > max_test_accuracy_value[key]:\n",
    "            max_test_accuracy_epoch[key] = i\n",
    "            max_test_accuracy_value[key] = test_accuracy\n",
    "        print(\"{0:38s}-Epoch:{1:3d}, Train Err.:{2:7.5f}, Validation Err.:{3:7.5f}, Test Accuracy:{4:7.5f}, Max Test Accuracy:{5:7.5f}\".format(\n",
    "            key,\n",
    "            i,\n",
    "            train_loss,\n",
    "            validation_loss,\n",
    "            test_accuracy,\n",
    "            max_test_accuracy_value[key]\n",
    "        ))\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(2, 2, figsize=(20, 12))\n",
    "for key in markers.keys():\n",
    "    axarr[0, 0].plot(epoch_list[1:], train_errors[key][1:], marker=markers[key], markevery=2, label=key)\n",
    "axarr[0, 0].set_ylabel('Train - Total Error')\n",
    "axarr[0, 0].set_xlabel('Epochs')\n",
    "axarr[0, 0].grid(True)\n",
    "axarr[0, 0].set_title('Train Error')\n",
    "axarr[0, 0].legend(loc='upper right')\n",
    "\n",
    "for key in markers.keys():\n",
    "    axarr[0, 1].plot(epoch_list[1:], validation_errors[key][1:], marker=markers[key], markevery=2, label=key)\n",
    "axarr[0, 1].set_ylabel('Validation - Total Error')\n",
    "axarr[0, 1].set_xlabel('Epochs')\n",
    "axarr[0, 1].grid(True)\n",
    "axarr[0, 1].set_title('Validation Error')\n",
    "axarr[0, 1].legend(loc='upper right')\n",
    "\n",
    "for key in markers.keys():\n",
    "    axarr[1, 0].plot(epoch_list[1:], train_errors[key][1:], marker=markers[key], markevery=2, label=key)\n",
    "axarr[1, 0].set_ylabel('Train - Total Error')\n",
    "axarr[1, 0].set_xlabel('Epochs')\n",
    "axarr[1, 0].grid(True)\n",
    "axarr[1, 0].set_ylim(0, 0.1)\n",
    "axarr[1, 0].set_title('Train Error (0.00 ~ 0.10)')\n",
    "axarr[1, 0].legend(loc='upper right')\n",
    "\n",
    "for key in markers.keys():\n",
    "    axarr[1, 1].plot(epoch_list[1:], validation_errors[key][1:], marker=markers[key], markevery=2, label=key)\n",
    "axarr[1, 1].set_ylabel('Validation - Total Error')\n",
    "axarr[1, 1].set_xlabel('Epochs')\n",
    "axarr[1, 1].grid(True)\n",
    "axarr[1, 1].set_ylim(0.05, 0.2)\n",
    "axarr[1, 1].set_title('Validation Error (0.05 ~ 0.20)')\n",
    "axarr[1, 1].legend(loc='upper right')\n",
    "\n",
    "f.subplots_adjust(hspace=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(2, 1, figsize=(15,10))\n",
    "for key in markers.keys():\n",
    "    axarr[0].plot(epoch_list[1:], test_accuracy_values[key][1:], marker=markers[key], markevery=1, label=key)\n",
    "axarr[0].set_ylabel('Test Accuracy')\n",
    "axarr[0].set_xlabel('Epochs')\n",
    "axarr[0].grid(True)\n",
    "axarr[0].set_title('Test Accuracy')\n",
    "axarr[0].legend(loc='lower right')\n",
    "\n",
    "for key in markers.keys():\n",
    "    axarr[1].plot(epoch_list[1:], test_accuracy_values[key][1:], marker=markers[key], markevery=1, label=key)\n",
    "axarr[1].set_ylabel('Test Accuracy')\n",
    "axarr[1].set_xlabel('Epochs')\n",
    "axarr[1].grid(True)\n",
    "axarr[1].set_ylim(0.92, 0.99)\n",
    "axarr[1].set_title('Test Accuracy (0.92 ~ 0.99)')\n",
    "axarr[1].legend(loc='lower right')\n",
    "\n",
    "f.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in markers.keys():\n",
    "    print(\"{0:26s} - Epoch:{1:3d}, Max Test Accuracy: {2:7.5f}\".format(key, max_test_accuracy_epoch[key], max_test_accuracy_value[key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
