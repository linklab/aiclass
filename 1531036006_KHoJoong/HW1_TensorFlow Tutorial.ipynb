{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# TensorFlow Tutorial\n",
    "(*이 자료는\n",
    "[TensorFlow Tutorial](https://www.slideshare.net/nmhkahn/tensorflow-tutorial-71896086 \"링크\")을 참조하여 만들어짐*)\n",
    "\n",
    "----\n",
    "## TensorFlow의 장점\n",
    "1. Easy numerical computation\n",
    "2. Automaticderivatives computation\n",
    "3. Deploy computation to multiple CPUs or GPUs\n",
    "4. Powerful data visualization toolkit\n",
    "\n",
    "## Tensor란?\n",
    "1. vector와 matrix의 확장형\n",
    "2. N-Dimension array로 볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Numpy (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.zeros((2,2))\n",
    "b = np.ones((2,2))\n",
    "np.sum(b, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(a, (1,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TensorFlow (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "a = tf.zeros((2,2))\n",
    "b = tf.ones((2,2))\n",
    "sess.run(tf.reduce_sum(b, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(2)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.reshape(a, (1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sess.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "a = np.zeros((2,2))\n",
    "ta = tf.zeros((2,2))\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros_1:0\", shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(ta) # TnsorFlow는 처음에 값이 없는 Computation graph를 정의함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(ta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tf.constant(5.0)\n",
    "b = tf.constant(6.0)\n",
    "c = a * b           # 이 과정은 Computation graph를 정의하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(c))  # 이 과정은 이전에 정의된 Computation graph에 따라 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n",
      "30.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))\n",
    "    print(c.eval())    # c.eval()은 sess 지역안에 있어야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow Variables (no initialize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.zeros((2,2)), name=\"weight\")\n",
    "#with tf.Session() as sess:\n",
    "    #print(sess.run(w))    # 변수가 초기화 되지 않았으므로 에러!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables need ** *Initialization* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1119767  -0.09501576]\n",
      " [ 0.08057523  0.18241394]\n",
      " [-0.0219014  -0.08727607]\n",
      " [-0.0773242  -0.08091877]\n",
      " [ 0.10698026  0.05581374]]\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(tf.random_normal([5,2], stddev = 0.1), name=\"weight\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "state = tf.Variable(0, name=\"counter\")\n",
    "new_value = tf.add(state, tf.constant(1))\n",
    "update = tf.assign(state, new_value)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(state))\n",
    "    \n",
    "    for _ in range(3):\n",
    "        sess.run(update)\n",
    "        print(sess.run(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.constant(1)\n",
    "x2 = tf.constant(2)\n",
    "x3 = tf.constant(3)\n",
    "temp = tf.add(x2, x3)\n",
    "mul = tf.multiply(x1, temp)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result1, result2 = sess.run([mul, temp])\n",
    "    print(result1, result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow Placeholder**  \n",
    "이전까지의 예제는 모두 수동으로 정의한 Tensors.  \n",
    "외부데이터를 Tensorflow에 넣기 위해서 placeholder와 feed_dict를 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.int16)\n",
    "b = tf.placeholder(tf.int16)\n",
    "\n",
    "add = tf.add(a,b)\n",
    "mul = tf.multiply(a,b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(add, feed_dict={a:2, b:3}))   # feed_dict : python dictionary\n",
    "    print(sess.run(mul, feed_dict={a:2, b:3}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n"
     ]
    }
   ],
   "source": [
    "#using tf.constant\n",
    "mat1 = tf.constant([[3., 3.]])\n",
    "mat2 = tf.constant([[2.], [2.]])\n",
    "product = tf.matmul(mat1, mat2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(product)\n",
    "    print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n"
     ]
    }
   ],
   "source": [
    "#using placeholder\n",
    "import numpy as np\n",
    "\n",
    "mat1 = tf.placeholder(tf.float32, [1, 2])\n",
    "mat2 = tf.placeholder(tf.float32, [2, 1])\n",
    "product = tf.matmul(mat1, mat2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    mv1 = np.array([[3., 3.]])\n",
    "    mv2 = np.array([[2.], [2.]])\n",
    "    result = sess.run(product, feed_dict={mat1 : mv1, mat2 : mv2})\n",
    "    print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - MNIST with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "max_steps = 15000\n",
    "batch_size =128\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MLP(inputs):\n",
    "    w_1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "    b_1 = tf.Variable(tf.zeros([256]))\n",
    "    \n",
    "    w_2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "    b_2 = tf.Variable(tf.zeros([256]))\n",
    "    \n",
    "    w_out = tf.Variable(tf.random_normal([256, 10]))\n",
    "    b_out = tf.Variable(tf.zeros([10]))\n",
    "    \n",
    "    h_1 = tf.add(tf.matmul(inputs, w_1), b_1)\n",
    "    h_1 = tf.nn.relu(h_1)\n",
    "    \n",
    "    h_2 = tf.add(tf.matmul(h_1, w_2), b_2)\n",
    "    h_2 = tf.nn.relu(h_2)\n",
    "    \n",
    "    out = tf.add(tf.matmul(h_2, w_out), b_out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "net = MLP(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000/15000] loss : 58.110\n",
      "[2000/15000] loss : 18.055\n",
      "[3000/15000] loss : 6.305\n",
      "[4000/15000] loss : 3.923\n",
      "[5000/15000] loss : 1.069\n",
      "[6000/15000] loss : 0.387\n",
      "[7000/15000] loss : 0.063\n",
      "[8000/15000] loss : 0.019\n",
      "[9000/15000] loss : 0.150\n",
      "[10000/15000] loss : 0.799\n",
      "[11000/15000] loss : 0.142\n",
      "[12000/15000] loss : 0.000\n",
      "[13000/15000] loss : 0.158\n",
      "[14000/15000] loss : 0.000\n",
      "[15000/15000] loss : 0.343\n",
      "Optimization Finished!\n",
      "Train accuracy :0.996\n",
      "Test accuracy :0.954\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=net, labels=y))         # 수정\n",
    "opt = tf.train.AdamOptimizer(learning_rate).minimize(loss_op)\n",
    "\n",
    "\n",
    "# initializing the variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "#train model\n",
    "for step in range(max_steps):\n",
    "    batch_X, batch_y = mnist.train.next_batch(batch_size)\n",
    "    _, loss = sess.run([opt, loss_op], feed_dict={x:batch_X, y:batch_y})\n",
    "    \n",
    "    if (step +1) % 1000 == 0:\n",
    "        print(\"[{}/{}] loss : {:.3f}\".format(step+1, max_steps, loss))\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "#test model\n",
    "correct_prediction = tf.equal(tf.argmax(net, 1), tf.argmax(y, 1))\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Train accuracy :{:.3f}\".format(sess.run(accuracy, feed_dict={x:mnist.train.images, y:mnist.train.labels})))\n",
    "print(\"Test accuracy :{:.3f}\".format(sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable Scope - variable_scope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1: var_2:0\n",
      "var2: foo_2/d/var:0\n",
      "var3: foo_2/d/b:0\n"
     ]
    }
   ],
   "source": [
    "var1 = tf.Variable([1], name=\"var\") \n",
    "with tf.variable_scope(\"foo\"): \n",
    "    with tf.variable_scope(\"d\"): \n",
    "        var2 = tf.Variable([1], name=\"var\") \n",
    "        var3 = tf.Variable([1], name=\"b\") \n",
    "print(\"var1: {}\".format(var1.name)) \n",
    "print(\"var2: {}\".format(var2.name)) \n",
    "print(\"var3: {}\".format(var3.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable Scope - get_variable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1: var_8:0\n",
      "var2: foo_8/bar/var:0\n",
      "var3: foo_8/bar/var_1:0\n"
     ]
    }
   ],
   "source": [
    "var1 = tf.Variable([1], name=\"var\") \n",
    "with tf.variable_scope(\"foo\"): \n",
    "    with tf.variable_scope(\"bar\") as scp: \n",
    "        var2 = tf.Variable([1], name=\"var\") \n",
    "        scp.reuse_variables() # allow reuse variables\n",
    "        var3 = tf.Variable([1], name=\"var\") \n",
    "print(\"var1: {}\".format(var1.name)) \n",
    "print(\"var2: {}\".format(var2.name)) \n",
    "print(\"var3: {}\".format(var3.name))    # var3의 이름을 foo/bar/var:0로 하고 싶으나 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1: var_9:0\n",
      "var2: foo/bar/var_2:0\n",
      "var3: foo/bar/var_2:0\n"
     ]
    }
   ],
   "source": [
    "var1 = tf.get_variable(\"var\", [1])  \n",
    "with tf.variable_scope(\"foo\"): \n",
    "    with tf.variable_scope(\"bar\") as scp: \n",
    "        var2 = tf.get_variable(\"var\", [1]) \n",
    "        scp.reuse_variables() # allow reuse variables\n",
    "        var3 = tf.get_variable(\"var\", [1]) \n",
    "print(\"var1: {}\".format(var1.name)) \n",
    "print(\"var2: {}\".format(var2.name)) \n",
    "print(\"var3: {}\".format(var3.name))    # var3의 이름을 foo/bar/var:0로 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1: foo2/bar2/var2:0\n",
      "var2: foo2/bar2/var2:0\n",
      "var3: foo2/bar2/var2:0\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"foo2\"):\n",
    "    with tf.variable_scope(\"bar2\") as scp:\n",
    "        var1 = tf.get_variable(\"var2\", [1])\n",
    "        scp.reuse_variables()\n",
    "        var2 = tf.get_variable(\"var2\", [1])\n",
    "        \n",
    "    with tf.variable_scope(\"bar2\", reuse=True):\n",
    "        var3 = tf.get_variable(\"var2\", [1])\n",
    "    \n",
    "print(\"var1: {}\".format(var1.name)) \n",
    "print(\"var2: {}\".format(var2.name)) \n",
    "print(\"var3: {}\".format(var3.name))     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@@ Tip @@  \n",
    "tf.Variable보다는 tf.get_variable을 사용하자.  \n",
    "tf.Variable은 파라미터 sharing 불가  \n",
    "tf.Variable은 initializer과 regularizer을 사용할 수 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - MNIST with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import variance_scaling_initializer \n",
    "he_init = variance_scaling_initializer() \n",
    "def conv(bottom, num_filter, ksize=3, stride=1, padding=\"SAME\", scope=None): \n",
    "    bottom_shape = bottom.get_shape().as_list()[3] \n",
    "    \n",
    "    with tf.variable_scope(scope or \"conv\"): \n",
    "        W = tf.get_variable(\"W\", \n",
    "                            [ksize, ksize, bottom_shape, num_filter], \n",
    "                            initializer=he_init) \n",
    "        b = tf.get_variable(\"b\", [num_filter], \n",
    "                            initializer=tf.constant_initializer(0)) \n",
    "        x = tf.nn.conv2d(bottom, W, \n",
    "                         strides=[1, stride, stride, 1], \n",
    "                         padding=padding) \n",
    "        x = tf.nn.relu(tf.nn.bias_add(x, b)) \n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxpool(bottom, ksize=2, stride=2, padding=\"SAME\", scope=None): \n",
    "    with tf.variable_scope(scope or \"maxpool\"): \n",
    "        pool = tf.nn.max_pool(bottom, ksize=[1, ksize, ksize, 1], strides=[1, stride, stride, 1], padding=padding) \n",
    "        \n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc(bottom, num_dims, scope=None): \n",
    "    bottom_shape = bottom.get_shape().as_list() \n",
    "    if len(bottom_shape) > 2: \n",
    "        bottom = tf.reshape(bottom, [-1, reduce(lambda x, y: x*y, bottom_shape[1:])]) \n",
    "        bottom_shape = bottom.get_shape().as_list() \n",
    "        \n",
    "    with tf.variable_scope(scope or \"fc\"): \n",
    "        W = tf.get_variable(\"W\", [bottom_shape[1], num_dims], initializer=he_init) \n",
    "        b = tf.get_variable(\"b\", [num_dims], initializer=tf.constant_initializer(0)) \n",
    "        out = tf.nn.bias_add(tf.matmul(bottom, W), b) \n",
    "        \n",
    "    return out \n",
    "\n",
    "def fc_relu(bottom, num_dims, scope=None): \n",
    "    with tf.variable_scope(scope or \"fc\"): \n",
    "        out = fc(bottom, num_dims, scope=\"fc\") \n",
    "        relu = tf.nn.relu(out) \n",
    "        \n",
    "    return relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32, None) \n",
    "\n",
    "def conv_net(x, keep_prob): \n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1]) \n",
    "    \n",
    "    conv1 = conv(x, 32, 5, scope=\"conv_1\") \n",
    "    conv1 = maxpool(conv1, scope=\"maxpool_1\") \n",
    "    conv2 = conv(conv1, 64, 5, scope=\"conv_2\") \n",
    "    conv2 = maxpool(conv2, scope=\"maxpool_2\") \n",
    "    \n",
    "    fc1 = fc_relu(conv2, 1024, scope=\"fc_1\") \n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob) \n",
    "    out = fc(fc1, 10, scope=\"out\") \n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto( gpu_options=tf.GPUOptions(allow_growth=True)) \n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.slim  \n",
    "#### tf.contrib.slim의 장점  \n",
    "building, training, evaluation neural networks가 쉬움  \n",
    "몇몇의 많이 사용되는 모델들이 slim으로 만들어짐  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#input = ...  #...에 무엇이 들어가야하는지.\n",
    "#with tf.name_scope('conv1_1') as scope: \n",
    "    #kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32, stddev=1e-1), name='weights') \n",
    "    #conv = tf.nn.conv2d(input, kernel, [1, 1, 1, 1], padding='SAME') \n",
    "    #biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32), trainable=True, name='biases') \n",
    "    #bias = tf.nn.bias_add(conv, biases) \n",
    "    #conv1 = tf.nn.relu(bias, name=scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#input = ... #...에 무엇이 들어가야 하는지..\n",
    "#net = slim.conv2d(input, 128, [3, 3], padding= 'SAME', scope= 'conv1_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "#import tf.contrib.slim\n",
    "\n",
    "# 1. simple network generation with slim \n",
    "#net = ... #...에 무엇이 들어가야 하는지..\n",
    "#net = slim.conv2d(net, 256, [3, 3], scope='conv3_1') \n",
    "#net = slim.conv2d(net, 256, [3, 3], scope='conv3_2') \n",
    "#net = slim.conv2d(net, 256, [3, 3], scope='conv3_3') \n",
    "#net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "\n",
    "# 1. cleaner by repeat operation: \n",
    "#net = ... #...에 무엇이 들어가야 하는지..\n",
    "#net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3') \n",
    "#net = slim.max_pool(net, [2, 2], scope= 'pool3')\n",
    "                                        \n",
    "# 2. Verbose way: \n",
    "#x = slim.fully_connected(x, 32, scope='fc/fc_1') \n",
    "#x = slim.fully_connected(x, 64, scope='fc/fc_2') \n",
    "#x = slim.fully_connected(x, 128, scope='fc/fc_3')\n",
    "\n",
    "# 2. Equivalent, TF-Slim way using slim.stack: \n",
    "#slim.stack(x, slim.fully_connected, [32, 64, 128], scope='fc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializer와 Regularizer에는 다음과 같은 것들이 있음  \n",
    "Initializer  \n",
    "1. tf.truncated_noraml_initialzier (Included in tf)  \n",
    "2. slim.xavier_initialzier  \n",
    "3. slim.variance_scaling_initializer  \n",
    "\n",
    "Regularizer  \n",
    "1. slim.l1_regularizer\n",
    "2. slim.l2_regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-7509178c5bd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m net = slim.conv2d(inputs, 64, [11, 11], 4, padding='SAME', \n\u001b[0m\u001b[1;32m      2\u001b[0m                   \u001b[0mweights_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxavier_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   weights_regularizer=slim.l2_regularizer(0.0005), scope='conv1')\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "net = slim.conv2d(inputs, 64, [11, 11], 4, padding='SAME', \n",
    "                  weights_initializer=slim.xavier_initializer(), \n",
    "                  weights_regularizer=slim.l2_regularizer(0.0005), scope='conv1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "argscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-dd0cb0411178>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SAME'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'conv1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxavier_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'conv2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfully_connected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'fc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "he_init = slim.variance_scaling_initializer() \n",
    "xavier_init = slim.xavier_initializer() \n",
    "\n",
    "with slim.arg_scope([slim.conv2d, slim.fully_connected], \n",
    "                    activation_fn=tf.nn.relu, weights_initializer=he_init, \n",
    "                    weights_regularizer=slim.l2_regularizer(0.0005)): \n",
    "    \n",
    "    with slim.arg_scope([slim.conv2d], stride=1, padding='SAME'): \n",
    "        net = slim.conv2d(inputs, 64, [11, 11], 4, scope='conv1') \n",
    "        net = slim.conv2d(net, 256, [5, 5], weights_initializer=xavier_init, scope='conv2') \n",
    "        net = slim.fully_connected(net, 1000, activation_fn=None, scope='fc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Losses  \n",
    "Multiple loss와 regularize loss를 다루기 쉬움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-2e8276a73177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_total_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_regularization_losses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred1' is not defined"
     ]
    }
   ],
   "source": [
    "loss1 = slim.losses.softmax_cross_entropy(pred1, label1) \n",
    "loss2 = slim.losses.mean_squared_error(pred2, label2) \n",
    "\n",
    "total_loss = loss1 + loss2 \n",
    "slim.losses.get_total_loss(add_regularization_losses=False) \n",
    "\n",
    "reg_loss = tf.add_n(slim.losses.get_regularization_losses()) \n",
    "total_loss = loss1 + loss2 + reg_loss \n",
    "\n",
    "total_loss = slim.losses.get_total_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save / Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(self, ckpt_dir, global_step=None): \n",
    "    if self.config.get(\"saver\") is None: \n",
    "        self.config[\"saver\"] = tf.train.Saver(max_to_keep=30) \n",
    "        \n",
    "    saver = self.config[\"saver\"] \n",
    "    sess = self.config[\"sess\"] \n",
    "    \n",
    "    dirname = os.path.join(ckpt_dir, self.name) \n",
    "    \n",
    "    if not os.path.exists(dirname): \n",
    "        os.makedirs(dirname) \n",
    "        \n",
    "    saver.save(sess, dirname, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_latest_checkpoint(self, ckpt_dir, exclude=None): \n",
    "    path = tf.train.latest_checkpoint(ckpt_dir) \n",
    "    if path is None: \n",
    "        raise AssertionError(\"No ckpt exists in {0}.\".format(ckpt_dir)) \n",
    "        \n",
    "    print(\"Load {} save file\".format(path)) \n",
    "    self._load(path, exclude) \n",
    "\n",
    "def vggdef load_from_path(self, ckpt_path, exclude=None): \n",
    "    self._load(ckpt_path, exclude) \n",
    "    \n",
    "def _load(self, ckpt_path, exclude): \n",
    "    init_fn = slim.assign_from_checkpoint_fn(ckpt_path, slim.get_variables_to_restore(exclude=exclude), \n",
    "                                             ignore_missing_vars=True) \n",
    "    init_fn(self.config[\"sess\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vgg_16(inputs, num_classes=1000, is_training=True, dropout_keep_prob=0.5,\n",
    "          spatial_squeeze=True, scope='vgg_16'):\n",
    "    with tf.variable_scope(scope, 'vgg_16', [inputs]) as sc:\n",
    "        end_points_collection = sc.name + '_end_points'\n",
    "        \n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d], outputs_collections = end_points_collection):\n",
    "            \n",
    "            net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "            net = slim.max_pool2d(net, [2,2], scope='pool1')\n",
    "            \n",
    "            net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "            net = slim.max_pool2d(net, [2,2], scope='pool2')\n",
    "            \n",
    "            net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "            net = slim.max_pool2d(net, [2,2], scope='pool3')\n",
    "            \n",
    "            net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "            net = slim.max_pool2d(net, [2,2], scope='pool4')\n",
    "            \n",
    "            net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "            net = slim.max_pool2d(net, [2,2], scope='pool5')\n",
    "            \n",
    "            net = slim.conv2d(net, 4096, [7,7], padding='VALID', scope='fc6')\n",
    "            net = slim.dropout(net, dropout_keep_prob, is_training = is_training, scope='dropout6')\n",
    "            \n",
    "            net = slim.conv2d(net, 4096, [1,1], scope='fc7')\n",
    "            net = slim.dropout(net, dropout_keep_prob, is_training = is_training, scope='dropout7')\n",
    "            \n",
    "            net = slim.conv2d(net, num_classes, [1,1], activation_fn=None, normalizer_fn=None, scope='fc8')\n",
    "            \n",
    "            end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "            \n",
    "            if spatial_squeeze:\n",
    "                net = tf.squeeze(net, [1,2], name='fc8/squeezed')\n",
    "                end_points[sc.name +'/fc8'] = net\n",
    "                \n",
    "            return net, end_points\n",
    "    \n",
    "vgg_16.default_image_size = 224\n",
    "\n",
    "def vgg_arg_scope(weight_decay=0.0005):\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected], \n",
    "                       activation_fn=tf.nn.relu,\n",
    "                       weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "                       biases_initializer=tf.zeros_initializer()):\n",
    "        with slim.arg_scope([slim.conv2d], padding='SAME') as arg_sc:\n",
    "            return arg_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-137-d25a5578cb0b>:9: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (?, 1000) and (?, 8) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-d25a5578cb0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"losses\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcls_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mreg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_n\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_regularization_losses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreg_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m           \u001b[0m_call_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorator_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_qualified_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m           func.__module__, date, instructions)\n\u001b[0;32m--> 117\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     new_func.__doc__ = _add_deprecated_function_notice_to_docstring(\n\u001b[1;32m    119\u001b[0m         func.__doc__, date, instructions)\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy\u001b[0;34m(logits, onehot_labels, weights, label_smoothing, scope)\u001b[0m\n\u001b[1;32m    378\u001b[0m   with ops.name_scope(scope, \"softmax_cross_entropy_loss\",\n\u001b[1;32m    379\u001b[0m                       [logits, onehot_labels, weights]) as scope:\n\u001b[0;32m--> 380\u001b[0;31m     \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0monehot_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0monehot_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0monehot_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    733\u001b[0m     \"\"\"\n\u001b[1;32m    734\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mis_fully_defined\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (?, 1000) and (?, 8) are incompatible"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 224, 224, 3], name=\"X\") \n",
    "y = tf.placeholder(tf.int32, [None, 8], name=\"y\") \n",
    "is_training = tf.placeholder(tf.bool, name=\"is_training\") \n",
    "\n",
    "with slim.arg_scope(vgg_arg_scope()): \n",
    "    net, end_pts = vgg_16(X, is_training=is_training, num_classes=1000) \n",
    "    \n",
    "with tf.variable_scope(\"losses\"): \n",
    "    cls_loss = slim.losses.softmax_cross_entropy(net, y) \n",
    "    reg_loss = tf.add_n(slim.losses.get_regularization_losses()) \n",
    "    loss_op = type_loss + reg_loss \n",
    "    \n",
    "with tf.variable_scope(\"opt\"): \n",
    "    opt = tf.train.AdamOptimizer(0.001).minimize(loss_op) \n",
    "    \n",
    "self.load_from_path(ckpt_path=VGG_PATH, exclude=[\"vgg_16/fc8\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard & slim example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MINST data \n",
    "from tensorflow.examples.tutorials.mnist import input_data \n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True) \n",
    "\n",
    "max_steps = 10000 \n",
    "batch_size = 128 \n",
    "lr = 0.001 \n",
    "keep_prob = 0.5 \n",
    "weight_decay = 0.0004 \n",
    "logs_path = \"/tmp/tensorflow_logs/example\" \n",
    "\n",
    "def my_arg_scope(is_training, weight_decay): \n",
    "    with slim.arg_scope([slim.conv2d], \n",
    "                        activation_fn=tf.nn.relu, \n",
    "                        weights_regularizer=slim.l2_regularizer(weight_decay), \n",
    "                        weights_initializer=slim.variance_scaling_initializer(), \n",
    "                        biases_initializer=tf.zeros_initializer(), stride=1, \n",
    "                        padding=\"SAME\"): \n",
    "        with slim.arg_scope([slim.dropout], is_training=is_training) as arg_sc: \n",
    "            return arg_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_net(x, keep_prob, outputs_collections=\"my_net\"): \n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1]) \n",
    "    \n",
    "    with slim.arg_scope([slim.conv2d, slim.max_pool2d], \n",
    "                        outputs_collections=outputs_collections): \n",
    "        \n",
    "        net = slim.conv2d(x, 64, [3, 3], scope=\"conv1\") \n",
    "        net = slim.max_pool2d(net, [2, 2], scope=\"pool1\") \n",
    "        \n",
    "        net = slim.conv2d(net, 128, [3, 3], scope=\"conv2\") \n",
    "        net = slim.max_pool2d(net, [2, 2], scope=\"pool2\") \n",
    "        \n",
    "        net = slim.conv2d(net, 256, [3, 3], scope=\"conv3\") \n",
    "        \n",
    "        # global average pooling \n",
    "        net = tf.reduce_mean(net, [1, 2], name=\"pool3\", keep_dims=True) \n",
    "        net = slim.dropout(net, keep_prob, scope=\"dropout3\") \n",
    "        net = slim.conv2d(net, 1024, [1, 1], scope=\"fc4\") \n",
    "        net = slim.dropout(net, keep_prob, scope=\"dropout4\") \n",
    "        net = slim.conv2d(net, 10, [1, 1], activation_fn=None, scope=\"fc5\") \n",
    "        \n",
    "    end_points = slim.utils.convert_collection_to_dict(outputs_collections) \n",
    "    \n",
    "    return tf.reshape(net, [-1, 10]), end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784]) \n",
    "y = tf.placeholder(tf.float32, [None, 10]) \n",
    "is_training = tf.placeholder(tf.bool) \n",
    "\n",
    "with slim.arg_scope(my_arg_scope(is_training, weight_decay)): \n",
    "    net, end_pts = my_net(x, keep_prob) \n",
    "    pred = slim.softmax(net, scope=\"prediction\") \n",
    "    \n",
    "with tf.variable_scope(\"losses\"): \n",
    "    cls_loss = tf.losses.softmax_cross_entropy(net, y) \n",
    "    reg_loss = tf.add_n(tf.losses.get_regularization_losses()) \n",
    "    loss_op = cls_loss + reg_loss \n",
    "\n",
    "with tf.variable_scope(\"Adam2\"):\n",
    "    opt = tf.train.AdamOptimizer(lr) \n",
    "    \n",
    "    # Op to calculate every variable gradient \n",
    "    grads = tf.gradients(loss_op, tf.trainable_variables()) \n",
    "    grads = list(zip(grads, tf.trainable_variables())) \n",
    "    \n",
    "    # Op to update all variables according to their gradient \n",
    "    apply_grads = opt.apply_gradients(grads_and_vars=grads) \n",
    "    \n",
    "with tf.variable_scope(\"accuracy\"): \n",
    "    correct_op = tf.equal(tf.argmax(net, 1), tf.argmax(y, 1)) \n",
    "    acc_op = tf.reduce_mean(tf.cast(correct_op, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name weight:0 is illegal; using weight_0 instead.\n",
      "INFO:tensorflow:Summary name weight_1:0 is illegal; using weight_1_0 instead.\n",
      "INFO:tensorflow:Summary name counter:0 is illegal; using counter_0 instead.\n",
      "INFO:tensorflow:Summary name Variable:0 is illegal; using Variable_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_1:0 is illegal; using Variable_1_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_2:0 is illegal; using Variable_2_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_3:0 is illegal; using Variable_3_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_4:0 is illegal; using Variable_4_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_5:0 is illegal; using Variable_5_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_6:0 is illegal; using Variable_6_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_7:0 is illegal; using Variable_7_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_8:0 is illegal; using Variable_8_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_9:0 is illegal; using Variable_9_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_10:0 is illegal; using Variable_10_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_11:0 is illegal; using Variable_11_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_12:0 is illegal; using Variable_12_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_13:0 is illegal; using Variable_13_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_14:0 is illegal; using Variable_14_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_15:0 is illegal; using Variable_15_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_16:0 is illegal; using Variable_16_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_17:0 is illegal; using Variable_17_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_18:0 is illegal; using Variable_18_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_19:0 is illegal; using Variable_19_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_20:0 is illegal; using Variable_20_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_21:0 is illegal; using Variable_21_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_22:0 is illegal; using Variable_22_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_23:0 is illegal; using Variable_23_0 instead.\n",
      "INFO:tensorflow:Summary name var:0 is illegal; using var_0 instead.\n",
      "INFO:tensorflow:Summary name foo/bar/var:0 is illegal; using foo/bar/var_0 instead.\n",
      "INFO:tensorflow:Summary name foo/bar/var_1:0 is illegal; using foo/bar/var_1_0 instead.\n",
      "INFO:tensorflow:Summary name var_1:0 is illegal; using var_1_0 instead.\n",
      "INFO:tensorflow:Summary name foo_1/bar/var:0 is illegal; using foo_1/bar/var_0 instead.\n",
      "INFO:tensorflow:Summary name foo_1/bar/b:0 is illegal; using foo_1/bar/b_0 instead.\n",
      "INFO:tensorflow:Summary name var_2:0 is illegal; using var_2_0 instead.\n",
      "INFO:tensorflow:Summary name foo_2/d/var:0 is illegal; using foo_2/d/var_0 instead.\n",
      "INFO:tensorflow:Summary name foo_2/d/b:0 is illegal; using foo_2/d/b_0 instead.\n",
      "INFO:tensorflow:Summary name var_3:0 is illegal; using var_3_0 instead.\n",
      "INFO:tensorflow:Summary name foo_3/bar/var:0 is illegal; using foo_3/bar/var_0 instead.\n",
      "INFO:tensorflow:Summary name foo_3/bar/var_1:0 is illegal; using foo_3/bar/var_1_0 instead.\n",
      "INFO:tensorflow:Summary name var_4:0 is illegal; using var_4_0 instead.\n",
      "INFO:tensorflow:Summary name foo_4/bar/var:0 is illegal; using foo_4/bar/var_0 instead.\n",
      "INFO:tensorflow:Summary name foo_4/bar/var_1:0 is illegal; using foo_4/bar/var_1_0 instead.\n",
      "INFO:tensorflow:Summary name var_5:0 is illegal; using var_5_0 instead.\n",
      "INFO:tensorflow:Summary name foo_5/bar/var:0 is illegal; using foo_5/bar/var_0 instead.\n",
      "INFO:tensorflow:Summary name foo_5/bar/var_1:0 is illegal; using foo_5/bar/var_1_0 instead.\n",
      "INFO:tensorflow:Summary name var_6:0 is illegal; using var_6_0 instead.\n",
      "INFO:tensorflow:Summary name foo_6/bar/var:0 is illegal; using foo_6/bar/var_0 instead.\n",
      "INFO:tensorflow:Summary name foo_6/bar/var_1:0 is illegal; using foo_6/bar/var_1_0 instead.\n",
      "INFO:tensorflow:Summary name var_7:0 is illegal; using var_7_0 instead.\n",
      "INFO:tensorflow:Summary name foo_7/bar/var:0 is illegal; using foo_7/bar/var_0 instead.\n",
      "INFO:tensorflow:Summary name foo_7/bar/var_1:0 is illegal; using foo_7/bar/var_1_0 instead.\n",
      "INFO:tensorflow:Summary name var_8:0 is illegal; using var_8_0 instead.\n",
      "INFO:tensorflow:Summary name foo_8/bar/var:0 is illegal; using foo_8/bar/var_0 instead.\n",
      "INFO:tensorflow:Summary name foo_8/bar/var_1:0 is illegal; using foo_8/bar/var_1_0 instead.\n",
      "INFO:tensorflow:Summary name var_9:0 is illegal; using var_9_0 instead.\n",
      "INFO:tensorflow:Summary name foo/bar/var_2:0 is illegal; using foo/bar/var_2_0 instead.\n",
      "INFO:tensorflow:Summary name foo2/bar2/var2:0 is illegal; using foo2/bar2/var2_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1/weights:0 is illegal; using conv1_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_1/weights:0 is illegal; using conv1_1_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_2/weights:0 is illegal; using conv1_1_2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_3/weights:0 is illegal; using conv1_1_3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_4/weights:0 is illegal; using conv1_1_4/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_5/weights:0 is illegal; using conv1_1_5/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_6/weights:0 is illegal; using conv1_1_6/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_7/weights:0 is illegal; using conv1_1_7/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_8/weights:0 is illegal; using conv1_1_8/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_9/weights:0 is illegal; using conv1_1_9/weights_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_24:0 is illegal; using Variable_24_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_10/weights:0 is illegal; using conv1_1_10/weights_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_25:0 is illegal; using Variable_25_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_11/weights:0 is illegal; using conv1_1_11/weights_0 instead.\n",
      "INFO:tensorflow:Summary name asdf:0 is illegal; using asdf_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_12/weights:0 is illegal; using conv1_1_12/weights_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_26:0 is illegal; using Variable_26_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_13/weights:0 is illegal; using conv1_1_13/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_14/weights:0 is illegal; using conv1_1_14/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_14/biases:0 is illegal; using conv1_1_14/biases_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_15/weights:0 is illegal; using conv1_1_15/weights_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv1/conv1_1/weights:0 is illegal; using vgg_16/conv1/conv1_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1/weights:0 is illegal; using conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1/biases_1:0 is illegal; using conv1/biases_1_0 instead.\n",
      "INFO:tensorflow:Summary name conv2/weights:0 is illegal; using conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv2/biases:0 is illegal; using conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name conv3/weights:0 is illegal; using conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv3/biases:0 is illegal; using conv3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name fc4/weights:0 is illegal; using fc4/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc4/biases:0 is illegal; using fc4/biases_0 instead.\n",
      "INFO:tensorflow:Summary name fc5/weights:0 is illegal; using fc5/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc5/biases:0 is illegal; using fc5/biases_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv1/conv1_1/biases_2:0 is illegal; using vgg_16/conv1/conv1_1/biases_2_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv1/conv1_2/weights:0 is illegal; using vgg_16/conv1/conv1_2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv1/conv1_2/biases:0 is illegal; using vgg_16/conv1/conv1_2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv2/conv2_1/weights:0 is illegal; using vgg_16/conv2/conv2_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv2/conv2_1/biases:0 is illegal; using vgg_16/conv2/conv2_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv2/conv2_2/weights:0 is illegal; using vgg_16/conv2/conv2_2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv2/conv2_2/biases:0 is illegal; using vgg_16/conv2/conv2_2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv3/conv3_1/weights:0 is illegal; using vgg_16/conv3/conv3_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv3/conv3_1/biases:0 is illegal; using vgg_16/conv3/conv3_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv3/conv3_2/weights:0 is illegal; using vgg_16/conv3/conv3_2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv3/conv3_2/biases:0 is illegal; using vgg_16/conv3/conv3_2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv3/conv3_3/weights:0 is illegal; using vgg_16/conv3/conv3_3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv3/conv3_3/biases:0 is illegal; using vgg_16/conv3/conv3_3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv4/conv4_1/weights:0 is illegal; using vgg_16/conv4/conv4_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv4/conv4_1/biases:0 is illegal; using vgg_16/conv4/conv4_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv4/conv4_2/weights:0 is illegal; using vgg_16/conv4/conv4_2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv4/conv4_2/biases:0 is illegal; using vgg_16/conv4/conv4_2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv4/conv4_3/weights:0 is illegal; using vgg_16/conv4/conv4_3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv4/conv4_3/biases:0 is illegal; using vgg_16/conv4/conv4_3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv5/conv5_1/weights:0 is illegal; using vgg_16/conv5/conv5_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv5/conv5_1/biases:0 is illegal; using vgg_16/conv5/conv5_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv5/conv5_2/weights:0 is illegal; using vgg_16/conv5/conv5_2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv5/conv5_2/biases:0 is illegal; using vgg_16/conv5/conv5_2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv5/conv5_3/weights:0 is illegal; using vgg_16/conv5/conv5_3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/conv5/conv5_3/biases:0 is illegal; using vgg_16/conv5/conv5_3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/fc6/weights:0 is illegal; using vgg_16/fc6/weights_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/fc6/biases:0 is illegal; using vgg_16/fc6/biases_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/fc7/weights:0 is illegal; using vgg_16/fc7/weights_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/fc7/biases:0 is illegal; using vgg_16/fc7/biases_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/fc8/weights:0 is illegal; using vgg_16/fc8/weights_0 instead.\n",
      "INFO:tensorflow:Summary name vgg_16/fc8/biases:0 is illegal; using vgg_16/fc8/biases_0 instead.\n",
      "INFO:tensorflow:Summary name weight:0/gradient is illegal; using weight_0/gradient instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    492\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m           \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    109\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m     98\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m---> 99\u001b[0;31m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    100\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"None values not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m     \u001b[1;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-161cfc3c1e61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/gradient\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"my_summ\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msumm_wg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"my_summ\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\summary\\summary.py\u001b[0m in \u001b[0;36mhistogram\u001b[0;34m(name, values, collections)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     val = _gen_logging_ops._histogram_summary(\n\u001b[0;32m--> 203\u001b[0;31m         tag=scope.rstrip('/'), values=values, name=scope)\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0m_collect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSUMMARIES\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_logging_ops.py\u001b[0m in \u001b[0;36m_histogram_summary\u001b[0;34m(tag, values, name)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \"\"\"\n\u001b[1;32m    138\u001b[0m   result = _op_def_lib.apply_op(\"HistogramSummary\", tag=tag, values=values,\n\u001b[0;32m--> 139\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m    140\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[1;31m# What type does convert_to_tensor think it has?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             observed = ops.internal_convert_to_tensor(\n\u001b[0;32m--> 504\u001b[0;31m                 values, as_ref=input_arg.is_ref).dtype.name\n\u001b[0m\u001b[1;32m    505\u001b[0m             prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\n\u001b[1;32m    506\u001b[0m                       (input_name, op_type_name, observed))\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m           \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    108\u001b[0m                                          as_ref=False):\n\u001b[1;32m    109\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mtensor_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m---> 99\u001b[0;31m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    100\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    358\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"None values not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m     \u001b[1;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[1;31m# provided if possible.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "# Create a summary to monitor loss and accuracy \n",
    "summ_loss = tf.summary.scalar(\"loss\", loss_op) \n",
    "summ_acc = tf.summary.scalar(\"accuracy_test\", acc_op) \n",
    "\n",
    "# Create summaries to visualize weights and grads \n",
    "for var in tf.trainable_variables(): \n",
    "    tf.summary.histogram(var.name, var, collections=[\"my_summ\"]) \n",
    "    \n",
    "for grad, var in grads: \n",
    "    tf.summary.histogram(var.name + \"/gradient\", grad, collections=[\"my_summ\"]) \n",
    "    \n",
    "summ_wg = tf.summary.merge_all(key=\"my_summ\") \n",
    "\n",
    "sess = tf.Session() \n",
    "sess.run(tf.global_variables_initializer()) \n",
    "\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summ_wg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-4dfc21c9911d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     _, loss, plot_loss, plot_wg = sess.run([apply_grads, loss_op, summ_loss, summ_wg], \n\u001b[0m\u001b[1;32m      4\u001b[0m                                            feed_dict={x: batch_X, y: batch_y, is_training: True}) \n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summ_wg' is not defined"
     ]
    }
   ],
   "source": [
    "for step in range(max_steps): \n",
    "    batch_X, batch_y = mnist.train.next_batch(batch_size) \n",
    "    _, loss, plot_loss, plot_wg = sess.run([apply_grads, loss_op, summ_loss, summ_wg], \n",
    "                                           feed_dict={x: batch_X, y: batch_y, is_training: True}) \n",
    "    \n",
    "    summary_writer.add_summary(plot_loss, step) \n",
    "    summary_writer.add_summary(plot_wg, step) \n",
    "    \n",
    "    if (step+1) % 100 == 0: \n",
    "        plot_acc = sess.run(summ_acc, feed_dict={x: mnist.test.images, y: mnist.test.labels, is_training: False}) \n",
    "        summary_writer.add_summary(plot_acc, step) \n",
    "        \n",
    "print(\"Optimization Finished!\") \n",
    "\n",
    "test_acc = sess.run(acc_op, feed_dict={x: mnist.test.images, y: mnist.test.labels, is_training: False}) \n",
    "\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
