{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_steps = 10000\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "keep_prob = 0.5\n",
    "weight_decay = 0.0004\n",
    "logs_path = \"/tmp/tensorflow_logs/example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_arg_scope(is_training, weight_decay):\n",
    "    with slim.arg_scope([slim.conv2d],\n",
    "                       activation_fn = tf.nn.relu,\n",
    "                       weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "                       weights_initializer=slim.variance_scaling_initializer(),\n",
    "                       biases_initializer = tf.zeros_initializer(),\n",
    "                       stride=1, padding=\"SAME\"):\n",
    "                        with slim.arg_scope([slim.dropout], \n",
    "                                            is_training=is_training) as arg_sc:\n",
    "                            return arg_sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_net(x, keep_prob, outputs_collections=\"my_net\"):\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n",
    "                       outputs_collections = outputs_collections):\n",
    "        net = slim.conv2d(x, 64, [3,3], scope=\"conv1\")\n",
    "        net = slim.max_pool2d(net, [2,2], scope=\"pool1\")\n",
    "        net = slim.conv2d(net, 128, [3,3], scope=\"conv2\")\n",
    "        net = slim.max_pool2d(net, [2,2], scope=\"pool2\")\n",
    "        net = slim.conv2d(net, 256, [3,3], scope=\"conv3\")\n",
    "        #global average pooling\n",
    "        net = tf.reduce_mean(net, [1,2], name=\"pool3\", keep_dims=True)\n",
    "        net = slim.dropout(net, keep_prob, scope=\"dropout3\")\n",
    "        net = slim.conv2d(net, 1024, [1,1], scope = \"fc4\")\n",
    "        net = slim.dropout(net, keep_prob, scope=\"dropout4\")\n",
    "        net = slim.conv2d(net, 10, [1,1], activation_fn=None, scope = \"fc5\")\n",
    "    end_points = \\\n",
    "        slim.utils.convert_collection_to_dict(outputs_collections)\n",
    "    return tf.reshape(net, [-1, 10]), end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "is_training = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-be4ddcfb2d18>:6: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead.\n",
      "WARNING:tensorflow:From /Users/KangByungJun/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:394: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From /Users/KangByungJun/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-be4ddcfb2d18>:7: get_regularization_losses (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.get_regularization_losses instead.\n"
     ]
    }
   ],
   "source": [
    "with slim.arg_scope(my_arg_scope(is_training, weight_decay)):\n",
    "    net, end_pts = my_net(x, keep_prob)\n",
    "    pred = slim.softmax(net, scope=\"prediction\")\n",
    "\n",
    "with tf.variable_scope(\"losses\"):\n",
    "    cls_loss = slim.losses.softmax_cross_entropy(net, y)\n",
    "    reg_loss = tf.add_n(slim.losses.get_regularization_losses())\n",
    "    loss_op = cls_loss + reg_loss\n",
    "\n",
    "with tf.variable_scope(\"Adam\"):\n",
    "    opt = tf.train.AdamOptimizer(lr)\n",
    "    grads = tf.gradients(loss_op, tf.trainable_variables())\n",
    "    grads = list(zip(grads, tf.trainable_variables()))\n",
    "    apply_grads = opt.apply_gradients(grads_and_vars = grads)\n",
    "\n",
    "with tf.variable_scope(\"accuracy\"):\n",
    "    correct_op = tf.equal(tf.argmax(net, 1), tf.argmax(y, 1))\n",
    "    acc_op = tf.reduce_mean(tf.cast(correct_op, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summ_loss = tf.summary.scalar(\"loss\", loss_op)\n",
    "summ_acc = tf.summary.scalar(\"accuracy_test\", acc_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv1/weights:0 is illegal; using conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1/biases:0 is illegal; using conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name conv2/weights:0 is illegal; using conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv2/biases:0 is illegal; using conv2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name conv3/weights:0 is illegal; using conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv3/biases:0 is illegal; using conv3/biases_0 instead.\n",
      "INFO:tensorflow:Summary name fc4/weights:0 is illegal; using fc4/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc4/biases:0 is illegal; using fc4/biases_0 instead.\n",
      "INFO:tensorflow:Summary name fc5/weights:0 is illegal; using fc5/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc5/biases:0 is illegal; using fc5/biases_0 instead.\n",
      "INFO:tensorflow:Summary name conv1/weights:0/gradient is illegal; using conv1/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv1/biases:0/gradient is illegal; using conv1/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2/weights:0/gradient is illegal; using conv2/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv2/biases:0/gradient is illegal; using conv2/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv3/weights:0/gradient is illegal; using conv3/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name conv3/biases:0/gradient is illegal; using conv3/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc4/weights:0/gradient is illegal; using fc4/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc4/biases:0/gradient is illegal; using fc4/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc5/weights:0/gradient is illegal; using fc5/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc5/biases:0/gradient is illegal; using fc5/biases_0/gradient instead.\n"
     ]
    }
   ],
   "source": [
    "for var in tf.trainable_variables():\n",
    "    tf.summary.histogram(var.name, var, collections=[\"my_summ\"])\n",
    "for grad, var in grads:\n",
    "    tf.summary.histogram(var.name + \"/gradient\", grad, collections=[\"my_summ\"])\n",
    "summ_wg = tf.summary.merge_all(key=\"my_summ\")\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph = sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_steps):\n",
    "    batch_X, batch_y = mnist.train.next_batch(batch_size)\n",
    "    _, loss, plot_loss, plot_wg = sess.run([apply_grads, loss_op,\n",
    "                                           summ_loss, summ_wg],\n",
    "                                          feed_dict={x: batch_X, y: batch_y, is_training: True})\n",
    "    summary_writer.add_summary(plot_loss, step)\n",
    "    summary_writer.add_summary(plot_wg, step)\n",
    "    \n",
    "    if(step+1) % 100 == 0:\n",
    "        plot_acc = sess.run(summ_acc, feed_dict={x: mnist.test.images,\n",
    "                                                y: mnist.test.labels,\n",
    "                                                is_training: False})\n",
    "        summary_writer.add_summary(plot_acc, step)\n",
    "print(\"Optimization Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.990\n"
     ]
    }
   ],
   "source": [
    "test_acc = sess.run(acc_op, feed_dict = {x: mnist.test.images,\n",
    "                                        y: mnist.test.labels,\n",
    "                                        is_training: False})\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
