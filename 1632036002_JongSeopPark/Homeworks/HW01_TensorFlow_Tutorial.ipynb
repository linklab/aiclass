{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2017. 03. 14. JongSeop Park (kugipark@gmail.com, Koreatech)\n",
    "#### 인공지능특강 (교수: 한연희) HW#1, 한국기술교육대학교 대학원 컴퓨터공학과 / 1632036002 박종섭 (박사과정)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tensorflow Tutorial\n",
    "\n",
    "* 참고1: [강의 홈페이지](http://link.koreatech.ac.kr/?page_id=3021)\n",
    "* 참고2: [튜토리얼 참고 슬라이드](https://www.slideshare.net/nmhkahn/tensorflow-tutorial-71896086)\n",
    "\n",
    "\n",
    "* 개발환경: Windows 10 64bit OS / Anaconda 4.3.1 / IPython (Jupyter notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. 설치\n",
    "\n",
    "### 1.1 Anaconda 설치\n",
    "\n",
    "Tensorflow는 Python 환경에서 사용 가능한 라이브러리이다. Windows 환경에서 Python을 원활하게 이용하기 위해 Anaconda 플랫폼을 설치한다. 본 문서에서 설치하는 버전은 [Anaconda 4.3.1 For Windows (Python 3.6 version 64bit-installer)]이다.\n",
    "  * 참고1: [Anaconda 다운로드 페이지](https://www.continuum.io/downloads)\n",
    "  * 참고2: [설치파일 직접다운로드 링크](https://repo.continuum.io/archive/Anaconda3-4.3.1-Windows-x86_64.exe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* 설치 확인\n",
    "Anaconda를 설치하고 나면 Python을 사용할 수 있다. 본 문서에서는 IPython(Jupyter notebook)을 통해 Python을 사용할 것이므로 Jupyter notebook이 제대로 동작하는지 확인한다. 향후 작성할 IPython 문서들을 저장할 새로운 작업 폴더를 마련하고(예: C:\\Users\\JongSeop\\AI2017_1) 콘솔(cmd) 창을 열어 해당 디렉터리로 이동한다. 콘솔 사용 방법을 모른다면 다음과 같은 방법으로 수행 가능하다.\n",
    "\n",
    "  1. 윈도우 탐색기에서 원하는 폴더로 이동\n",
    "  2. 탐색기 상단 메뉴에서 [파일]-->[명령 프롬프트 열기]-->[관리자 권한으로 명령 프롬프트 열기] 선택\n",
    "  \n",
    "콘솔 창이 열렸으면 다음과 같은 명령어를 입력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    ">jupytor notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "이 명령어는 현재 실행중인 PC에서 내부 호스팅을 시작하여 Jupyter notebook을 사용할 수 있도록 해 준다. 명령어를 입력하고 나면 자동으로 웹 브라우저가 실행되며 Jupyter notebook 메인 페이지로 이동된다. 명령어를 입력한 후 notebook 웹 페이지가 열릴 때 까지 약간의 시간이 걸릴 수 있다. 만약 웹 브라우저가 열리지 않았거나 실수로 웹 브라우저를 닫았다면 다음과 같은 주소로 해당 페이지에 접근할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> http://localhost:8888/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "콘솔 창을 닫으면 호스팅이 중단되어 Jupyter notebook를 이용할 수 없으며 계속 사용하려면 다시 콘솔창을 열어 명령어를 입력하여야 한다.\n",
    "지금은 Tensorflow를 설치할 것이기 때문에 정상적으로 설치된 것을 확인했으면 notebook 페이지를 닫고 콘솔 창에서 [Ctrl + C]를 입력하여 호스팅을 중단시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2 Python 추가 설치 (Optional)\n",
    "\n",
    "본 문서에서 설치하는 Anaconda 4.3.1은 기본적으로 Python 3.6 버전을 사용하도록 되어있다. 그런데 현재 Tensorflow은 Python 3.5.x 버전만 지원하고 있기 때문에 Anaconda 환경에 알맞은 버전의 Python을 추가해주어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " * 사용 가능한 Python 버전 확인\n",
    " \n",
    "콘솔에 다음 명령어를 입력하여 사용 가능한 Python 버전을 확인한다. 이미 설치되어 있다면 버전 옆에 * 표시가 되어있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " > conda search python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " * Python 3.5.3 설치\n",
    " \n",
    "본 문서에서는 Python 3.5.3 버전을 사용한다. 콘솔에 다음 명령어를 입력하여 설치를 수행할 수 있다.\n",
    "\n",
    "이 작업은 인터넷 연결이 필요하며 설치하는 데 15~20 분 정도 시간이 걸릴 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " > conda install python=3.5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "위 명령어를 입력하면 설치할 패키지 정보가 콘솔 화면에 표시되면서 계속 수행할지 묻는다. y를 입력하여 설치를 계속 수행하도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.3 Tensorflow 설치\n",
    "\n",
    "Anaconda에서 제공하는 conda environment를 구성하고 해당 환경 내에서 Tensorflow를 설치하여 사용할 것이다.\n",
    "\n",
    " * conda environment 구성\n",
    "\n",
    "다음과 같은 명령어를 콘솔 창에 입력하여 새로운 conda environment를 생성한다. 이 명령어에서 \"tensorflow\"는 환경의 이름을 설정한 것이며 python 버전을 3.5.3으로 지정해주었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " > conda create -n tensorflow python=3.5.3 ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "위 명령어를 입력하면 다음과 같은 확인 메시지가 표시된다. 내용은 버전 및 환경에 따라 달라질 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "     The following New packages will be INSTALLED:\n",
    "\n",
    "        ...\n",
    "        pip:             9.0.1-py35_1 \n",
    "        python:          3.5.3-0\n",
    "        setuptools:      27.2.0-py35_1\n",
    "        vs2015_runtime:  14.0.25123-0\n",
    "        wheel:           0.29.0-py35_0\n",
    "        ...\n",
    "\n",
    "     Proceed ([y]/n)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "y를 입력하여 설치를 계속 진행한다. 이 작업에는 시간이 거의 걸리지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " * conda environment 활성화\n",
    " \n",
    " 다음 명령어를 콘솔 창에 입력하여 위에서 tensorflow라는 이름으로 생성된 conda environment를 활성화시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " > activate tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "그러면 다음과 같이 콘솔 입력 표시 맨 앞에 현재 환경의 이름인 (tensorflow)가 붙는 것을 확인할 수 있다.\n",
    "\n",
    "예시)\n",
    "\n",
    "    (tensorflow) C:\\Users\\JongSeop\\Jupyter>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "만약 나중에 이 환경을 다시 비활성화 시키고 싶다면 다음과 같이 입력하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " > deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " * Jupyter kernel 변경\n",
    " \n",
    " 기본 환경(Python 3.6)으로 실행되는 Jupyter kernel 대신 지금까지 설정한 가상 작업 환경의 커널을 사용한다.\n",
    " \n",
    " 다음 명령어를 콘솔에 입력하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " > ipython kernel install --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "또는 기존 커널의 버전을 유지한 채 별도의 커널을 만들고 싶다면 다음과 같이 옵션을 추가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " > ipython kernel install --user --name tfkernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " * Tensorflow 설치\n",
    " \n",
    " 여기서부터는 [Tensorflow 홈페이지](https://www.tensorflow.org/install/install_windows)에 나와있는 설치 가이드와 동일하다.\n",
    " \n",
    " 현재 Tensorflow는 CPU-only 버전과 GPU 버전이 있다. 본 문서에서는 CPU-only 버전을 사용할 것이다.\n",
    " \n",
    " 콘솔에 다음 명령어를 입력하여 CPU-only 버전을 설치한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.1-cp35-cp35m-win_amd64.whl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " * Tensorflow 설치 확인\n",
    " \n",
    " 설치가 완료되었으면 Tensorflow 홈페이지의 가이드에 나와있는 것 처럼 Python을 콘솔에서 바로 실행하여 제대로 설치됐는지 확인해볼 수 있다.\n",
    " 먼저 콘솔에 다음과 같이 입력하여 python을 실행시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " > python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "그리고 다음과 같은 코드를 입력하여 실행시켜 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Hello, TensorFlow! 라는 메시지가 출력되지 않는다면 설치가 제대로 되지 않았을 수 있다.\n",
    "\n",
    "이제 이 Tensorflow가 Jupyter(Ipython)에서도 제대로 동작하는지 확인하기 위해 이전에 했던 것과 동일하게 콘솔에 다음과 같이 입력하여 notebook을 실행시킨다. 콘솔에 입력할 때 (tensorflow)와 같이 conda environment 이름이 맨 앞에 붙어있는지 확인하도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " > (tensorflow) C:\\Users\\JongSeop\\Jupyter> jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Jupyter 메인 화면이 열렸으면 New 버튼을 눌러 새 Notebook 페이지를 생성한다. 위에서 IPython 커널을 수정할 때 이름을 따로 지정하지 않았다면 [Python 3] 라는 이름의 Notebook 종류가 하나 표시될 것이다. 별도의 이름을 지정한 경우에는 해당 이름의 Notebook이 목록에 추가로 표시되어 있어야 한다. 이 때에는 [Python 3] 대신 [tfkernel] 등 새로 지정한 항목을 선택한다.\n",
    "\n",
    "다음 코드로 현재 커널의 버전을 확인할 수 있다. 3.6으로 되어있다면 Tensorflow를 사용할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.3 |Continuum Analytics, Inc.| (default, Feb 22 2017, 21:28:42) [MSC v.1900 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "앞서 했던 것처럼 Hello, TensorFlow를 출력하는 코드를 테스트 해 보도록 한다.\n",
    "메시지가 문제 없이 출력 됐다면 설치가 끝난 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Tutorial\n",
    "\n",
    "여기서부터는 Tensorflow 튜토리얼을 수행한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### If you know Numpy - Python libary for numerical computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.zeros((2, 2))\n",
    "b = np.ones((2, 2))\n",
    "np.sum(b, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(a, (1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 위 코드를 Tensorflow로 대체하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "a = tf.zeros((2, 2))\n",
    "b = tf.ones((2, 2))\n",
    "\n",
    "sess.run(tf.reduce_sum(b, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(2)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.reshape(a, (1, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### sess.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "a = np.zeros((2, 2))\n",
    "ta = tf.zeros((2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### TensorFlow first defines a computation graph that has no numerical value until evaluated. (very similar concept to Theano)\n",
    "TensorFlow는 평가 되기 전에는 숫자 값이 없는 계산 그래프를 먼저 정의한다. (Theano와 매우 유사한 개념)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros_2:0\", shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(ta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### In TensorFlow program, it first assembles a graph, and uses a session to execute ops in the graph.\n",
    "TensorFlow 프로그램에서는 먼저 그래프를 구성하고, 그래프의 작업을 수행하기 위해 세션을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Defines a computation graph. At this time, we cannot see real value.\n",
    "# 계산 그래프를 정의한다. 이 단계에서는 실제 값을 볼 수 없다.\n",
    "a = tf.constant(5.0)\n",
    "b = tf.constant(6.0)\n",
    "c = a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate pre-defined computation graph. Graph evaluation must be followed by tf.Session creation.\n",
    "# 사전에 정의된 계산 그래프를 평가한다. 그래프 평가에는 반드시 tf.Session 생성이 뒤따라야 한다.\n",
    "sess = tf.Session()\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TensorFlow Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### A Session object encapsulates the environment in which operation and tensor objects are evaluated.\n",
    "세션 개체는 연산과 tensor 개체 평가가 이루어지는 환경을 캡슐화한다.\n",
    "\n",
    "#### Both type of the session creation has same behavior.\n",
    "다음 두 가지의 세션 생성 유형은 동일한 동작을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n",
      "30.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))\n",
    "    print(c.eval())\n",
    "    # Syntactic sugar for sess.run(c) in the current active session. (Must locate inside of the tf.Session scope)\n",
    "    # sess.run(c)에 대한 Syntax sugar이다. (반드시 tf.Session 범위 내에 위치해야 한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TensorFlow Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### All tensors used previously were constant tensors\n",
    "이전에 사용된 모든 tensor들은 상수 타입의 tensor였다.\n",
    "\n",
    "#### To train a model, we need variable type tensor to hold and update values\n",
    "모델을 학습하기 위해서는 값을 유지하고 갱신하기 위한 변수 타입의 tensor가 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 다음 구문을 실행하면 초기화하지 않은 변수 weight를 사용했다는 에러가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value weight\n\t [[Node: _send_weight_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=-4746070215346870387, tensor_name=\"weight:0\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](weight)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value weight\n\t [[Node: _send_weight_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=-4746070215346870387, tensor_name=\"weight:0\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](weight)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-9b7f6afd2b09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"weight\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1035\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value weight\n\t [[Node: _send_weight_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=-4746070215346870387, tensor_name=\"weight:0\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](weight)]]"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(tf.zeros((2, 2)), name=\"weight\")\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Variables need initialization!\n",
    "\n",
    "#### tf.Variables can be initialized from constants or random values\n",
    "tf.Variables는 상수나 랜덤값으로 초기화할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01616094 -0.06432138]\n",
      " [-0.04075636 -0.07912492]\n",
      " [-0.04852867 -0.00851206]\n",
      " [ 0.08336934 -0.1094206 ]\n",
      " [-0.04157775 -0.09000797]]\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(tf.random_normal([5, 2], stddev=0.1), name=\"weight\")\n",
    "with tf.Session() as sess:\n",
    "    # tf.Variables needs initialization step.\n",
    "    # tf.Variables는 변수 초기화 단계가 필요하다.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Updating Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "state = tf.Variable(0, name=\"counter\")\n",
    "new_value = tf.add(state, tf.constant(1))\n",
    "update = tf.assign(state, new_value)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(state))\n",
    "    for _ in range(3):\n",
    "        sess.run(update)\n",
    "        print(sess.run(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fetching Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.constant(1)\n",
    "x2 = tf.constant(2)\n",
    "x3 = tf.constant(3)\n",
    "temp = tf.add(x2, x3)\n",
    "mul = tf.multiply(x1, temp)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Calling sess.run(var) outputs its value. Of course, we can output multiple tensors simultaneously with sess.run([var1, .., ])\n",
    "    # sess.run(var)를 호출하면 결과값이 출력된다. sess.run([var1, .., ])를 이용해서 여러 Tensor들을 동시에 출력할 수도 있다.\n",
    "    result1, result2 = sess.run([mul, temp])\n",
    "    print(result1, result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TensorFlow Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Create a placeholder to hold data.\n",
    "# tf.placeholder is dummy node to provide entry points for data to computation graph.\n",
    "# 데이터를 유지하기 위해 placeholder를 생성한다.\n",
    "# tf.placeholder는 계산 그래프의 데이터 진입점을 제공하기 위한 더미 노드이다.\n",
    "a = tf.placeholder(tf.int16)\n",
    "b = tf.placeholder(tf.int16)\n",
    "\n",
    "add = tf.add(a, b)\n",
    "mul = tf.multiply(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # A feed_dict is a \n",
    "    print(sess.run(add, feed_dict={a: 2, b: 3}))\n",
    "    print(sess.run(mul, feed_dict={a: 2, b: 3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - MNIST with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "learning_rate = 0.001\n",
    "max_steps = 15000\n",
    "batch_size = 128\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MLP(inputs):\n",
    "    W_1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "    b_1 = tf.Variable(tf.zeros([256]))\n",
    "    \n",
    "    W_2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "    b_2 = tf.Variable(tf.zeros([256]))\n",
    "    \n",
    "    W_out = tf.Variable(tf.random_normal([256, 10]))\n",
    "    b_out = tf.Variable(tf.zeros([10]))\n",
    "    \n",
    "    h_1 = tf.add(tf.matmul(inputs, W_1), b_1)\n",
    "    h_1 = tf.nn.relu(h_1)\n",
    "    \n",
    "    h_2 = tf.add(tf.matmul(h_1, W_2), b_2)\n",
    "    h_2 = tf.nn.relu(h_2)\n",
    "    \n",
    "    out = tf.add(tf.matmul(h_2, W_out), b_out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "net = MLP(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define loss and optimizer\n",
    "\n",
    "# tf.nn.softmax_cross_entropy_with_logits is for calculate cross entropy loss between predictions and lables\n",
    "# (instead, you can make own loss function)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=net, labels=y))\n",
    "\n",
    "# tf.train.####Optimizer create an optimizer such as SGD, RMSProp, Adam etc..\n",
    "# tf.train.####Optimizer.minimize(loss_op) adds optimization operation to computation graph\n",
    "opt = tf.train.AdamOptimizer(learning_rate).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initializing the variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000/15000] loss:32.777\n",
      "[2000/15000] loss:10.373\n",
      "[3000/15000] loss:11.770\n",
      "[4000/15000] loss:6.526\n",
      "[5000/15000] loss:3.427\n",
      "[6000/15000] loss:3.251\n",
      "[7000/15000] loss:3.745\n",
      "[8000/15000] loss:0.530\n",
      "[9000/15000] loss:0.873\n",
      "[10000/15000] loss:4.089\n",
      "[11000/15000] loss:0.267\n",
      "[12000/15000] loss:0.254\n",
      "[13000/15000] loss:0.000\n",
      "[14000/15000] loss:1.022\n",
      "[15000/15000] loss:0.000\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# tran model\n",
    "for step in range(max_steps):\n",
    "    batch_X, batch_y = mnist.train.next_batch(batch_size)\n",
    "    _, loss = sess.run([opt, loss_op], feed_dict={x: batch_X, y: batch_y})\n",
    "    \n",
    "    if (step+1) % 1000 == 0:\n",
    "        print(\"[{}/{}] loss:{:.3f}\".format(step+1, max_steps, loss))\n",
    "print(\"Optimization Finished!\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test model\n",
    "correct_prediction = tf.equal(tf.argmax(net, 1), tf.argmax(y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.996\n",
      "Test accuracy: 0.957\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"Train accuracy: {:.3f}\".format(sess.run(accuracy, feed_dict={x: mnist.train.images, y: mnist.train.labels})))\n",
    "print(\"Test accuracy: {:.3f}\".format(sess.run(accuracy, feed_dict={x: mnist.test.images, y:mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Scope\n",
    "\n",
    "* Complicated TensorFlow models can have hundreds of variables\n",
    "    * tf.variable_scope() provides simple name-spacing to avoid clashes\n",
    "    * tf.get_variables() creates/accesses variables from within a variable scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.variable_scope()\n",
    "   * Variable scope is a simple type of name-spacing that add prefixes to variable names within scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1: var:0\n",
      "var2: foo/bar/var:0\n",
      "var3: foo/bar/var_1:0\n"
     ]
    }
   ],
   "source": [
    "var1 = tf.Variable([1], name=\"var\")\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    with tf.variable_scope(\"bar\"):\n",
    "        var2 = tf.Variable([1], name=\"var\")\n",
    "        var3 = tf.Variable([1], name=\"var\")\n",
    "        \n",
    "print(\"var1: {}\".format(var1.name))\n",
    "print(\"var2: {}\".format(var2.name))\n",
    "print(\"var3: {}\".format(var3.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.get_variable()\n",
    "    * tf.Variable doesn't provide variable reuse\n",
    "        * Variable reuse is necessary to implement RNN or Recursive NN\n",
    "    * Behavior depends on wheter variable reuse enabled\n",
    "        * Case 1: reuse flag set to False\n",
    "            * Create new variable and return\n",
    "            * Raise ValueError if there exists variable with given anem\n",
    "        * Case 2: reuse flag set to True\n",
    "            * Search for existing variable with given name\n",
    "            * Raise ValueError if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1: var_1:0\n",
      "var2: foo_1/bar/var:0\n",
      "var3: foo_1/bar/var_1:0\n"
     ]
    }
   ],
   "source": [
    "var1 = tf.Variable([1], name=\"var\")\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    with tf.variable_scope(\"bar\") as scp:\n",
    "        var2 = tf.Variable([1], name=\"var\")\n",
    "        scp.reuse_variables() # allow reuse variables\n",
    "        var3 = tf.Variable([1], name=\"var\")\n",
    "        \n",
    "print(\"var1: {}\".format(var1.name))\n",
    "print(\"var2: {}\".format(var2.name))\n",
    "print(\"var3: {}\".format(var3.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * We want var3 refer to foo/bar/var:0  \n",
    "      But impossible with tf.Variable\n",
    "      \n",
    "    * This works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1: var_2:0\n",
      "var2: foo/bar/var_2:0\n",
      "var3: foo/bar/var_2:0\n"
     ]
    }
   ],
   "source": [
    "var1 = tf.get_variable(\"var\", [1])\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    with tf.variable_scope(\"bar\") as scp:\n",
    "        var2 = tf.get_variable(\"var\", [1])\n",
    "        scp.reuse_variables() # allow reuse variables\n",
    "        var3 = tf.get_variable(\"var\", [1])\n",
    "        \n",
    "print(\"var1: {}\".format(var1.name))\n",
    "print(\"var2: {}\".format(var2.name))\n",
    "print(\"var3: {}\".format(var3.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter sharinng\n",
    "    * ALWAYS use tf.get_variable instead of tf.Variable\n",
    "        * tf.Variable doesn't provide parameter sharing\n",
    "        * Can't use initializer and regularizer in tf.Variable\n",
    "    *GOOD scoping is essential\n",
    "        * When visualize and recognize network\n",
    "        * Easy to get variables and ops by name or suffix\n",
    "          tf.###.get_variables_by_name(\"my_var\", \"my_scope\")\n",
    "        * This is useful when debug load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1: foo2/bar2/var2:0\n",
      "var2: foo2/bar2/var2:0\n",
      "var3: foo2/bar2/var2:0\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"foo2\"):\n",
    "    with tf.variable_scope(\"bar2\") as scp:\n",
    "        var1 = tf.get_variable(\"var2\", [1])\n",
    "        scp.reuse_variables()\n",
    "        var2 = tf.get_variable(\"var2\", [1])\n",
    "        \n",
    "    with tf.variable_scope(\"bar2\", reuse=True):\n",
    "        var3 = tf.get_variable(\"var2\", [1])\n",
    "        \n",
    "print(\"var1: {}\".format(var1.name))\n",
    "print(\"var2: {}\".format(var2.name))\n",
    "print(\"var3: {}\".format(var3.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - MNIST with CNN\n",
    "\n",
    "* Most of the code is same as previous MLP example\n",
    "  exept code for building network\n",
    "* CNN model needs conv, pool and FC layer\n",
    "    * tf.nn.conv2d(input, filter, strides, padding, name=None)\n",
    "    \n",
    "    \n",
    "        1. input   : A 4D tensor with [batch, height, width, channels] shape\n",
    "        2. filter  : A 4D tensor with [filter_height, filter_width, in_channels, out_channels]\n",
    "        3. stride  : The stride of the sliding window for each dimension of input\n",
    "        4. padding : The type of padding algorithm to use (\"SAME\" or \"VALID\")\n",
    "    \n",
    "    \n",
    "    * tf.nn.##_pool(value, ksize, stride, padding, name=None)\n",
    "        1. value   : A 4D Tensor with shape [batch, height, width, channels]\n",
    "        2. ksize   : A list of ints that has length >= 4. The size of the window for each dimension of the input tensor.\n",
    "        3. stride  : The stride of the sliding window for each dimension of input\n",
    "        4. padding : The type of padding algorithm to use (\"SAME\" or \"VALID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Another init stratege called He init introduced in PReLUNet\n",
    "from tensorflow.contrib.layers import variance_scaling_initializer\n",
    "he_init = variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv(bottom, num_filter, ksize=3, stride=1, padding=\"SAME\", scope=None):\n",
    "    \n",
    "    # To calculate shape of previous(input) layer\n",
    "    bottom_shape = bottom.get_shape().as_list()[3]\n",
    "    \n",
    "    with tf.variable_scope(scope or \"conv\"):\n",
    "        W = tf.get_variable(\"W\",\n",
    "                           [ksize, ksize, bottom_shape, num_filter],\n",
    "                           initializer=he_init)\n",
    "        b = tf.get_variable(\"b\", [num_filter],\n",
    "                           initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        x = tf.nn.conv2d(bottom, W,\n",
    "                        strides=[1, stride, stride, 1],\n",
    "                        padding=padding)\n",
    "        x = tf.nn.relu(tf.nn.bias_add(x, b))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxpool(bottom, ksize=2, stride=2, padding=\"SAME\", scope=None):\n",
    "    \n",
    "    with tf.variable_scope(scope or \"maxpool\"):\n",
    "        pool = tf.nn.max_pool(bottom, ksize=[1, ksize, ksize, 1],\n",
    "                             strides=[1, stride, stride, 1],\n",
    "                             padding=padding)\n",
    "        \n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc(bottom, num_dims, scope=None):\n",
    "    \n",
    "    bottom_shape = bottom.get_shape().as_list()\n",
    "    \n",
    "    # If dims of bottom layer is more than 2, flatten\n",
    "    #   output of conv has (N, H, W, C) shape\n",
    "    #   output of FC has (N, C) shape\n",
    "    if len(bottom_shape) > 2:\n",
    "        bottom = tf.reshape(bottom,\n",
    "                           [-1, reduce(lambda x, y: x*y, bottom_shape[1:])])\n",
    "        bottom_shape = bottom.get_shape().as_list()\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope(scope or \"fc\"):\n",
    "        W = tf.get_variable(\"W\", [bottom_shape[1], num_dims],\n",
    "                           initializer=he_init)\n",
    "        b = tf.get_variable(\"b\", [num_dims],\n",
    "                           initializer=tf.constant_initializer(0))\n",
    "\n",
    "        out = tf.nn.bias_add(tf.matmul(bottom, W), b )\n",
    "\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_relu(bottom, num_dims, scope=None):\n",
    "    \n",
    "    with tf.variable_scope(scope or \"fc\"):\n",
    "        out = fc(bottom, num_dims, scope=\"fc\")\n",
    "        relu = tf.nn.relu(out)\n",
    "        \n",
    "    return relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To hold keeping probability in dropout\n",
    "# Feed value between (0.0, 1.0) in training,\n",
    "# feed 1.0 in test time\n",
    "keep_prob = tf.placeholder(tf.float32, None)\n",
    "\n",
    "def conv_net(x, keep_prob):\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    conv1 = conv(x, 32, 5, scope=\"conv_1\")\n",
    "    conv1 = maxpool(conv1, scope=\"maxpool_1\")\n",
    "    conv2 = conv(conv1, 64, 5, scope=\"conv_2\")\n",
    "    conv2 = maxpool(conv2, scope=\"maxpool_2\")\n",
    "    \n",
    "    fc1 = fc_relu(conv2, 1024, scope=\"fc_1\")\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    out = fc(fc1, 10, scope=\"out\")\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session config\n",
    "\n",
    "* By default, session consume all the GPU resource, no matter how a model capacity is\n",
    "    * With killing all the other process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try this config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(\n",
    "    gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are a bunch of configuration, see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.contrib\n",
    "\n",
    "* 참고: 이하 예제 코드들은 작동하지 않는다. 아마도 입력 값을 따로 지정해주어야 하는 듯.\n",
    "\n",
    "* tf.contrib contains contributed code\n",
    "    * Contain contributions that eventually should get merged into core TensorFlow\n",
    "    * Code in tf.contrib isn't supported by the TensorFlow team.\n",
    "      It is included in the hope that it is helpful, but it might change or be removed at any time\n",
    "    * Many useful interfaces or utils exist\n",
    "    * RNN, Seq2Seq, layers(wrapper) and slim are quite useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.contrib.slim\n",
    "    \n",
    "    * Lightweight library in TensorFlow\n",
    "        * Components of slim can be freely mixed native TensorFlow\n",
    "        * Gather useful tf.contrib component into tf.contrib.slim namespace\n",
    "        \n",
    "    * Why tf.slim?\n",
    "        * Makes building training and evaluation neural networks simple\n",
    "        * Allows to define models compactly by eliminating boilerplate code\n",
    "        * Several widely used models have been developed in slim\n",
    "        * Other useful utilities are provided (initializer, regularizer, losses ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the various components of TF-Slim?\n",
    "\n",
    "TF-Slim is composed of several parts which were design to exist independently. These include the following main pieces (explained in detail below).\n",
    "\n",
    "    * arg_scope: provides a new scope named arg_scope that allows a user to define default arguments for specific operations within that scope.\n",
    "    * data: contains TF-slim's dataset definition, data providers, parallel_reader, and decoding utilities.\n",
    "    * layers: contains high level layers for building models using tensorflow.\n",
    "    * learning: contains routines for training models.\n",
    "    * losses: contains commonly used loss functions.\n",
    "    * metrics: contains popular evaluation metrics.\n",
    "    * nets: contains popular network definitions such as VGG and AlexNet models.\n",
    "    * queues: provides a context manager for easily and safely starting and closing QueueRunners.\n",
    "    * regularizers: contains weight regularizers.\n",
    "    * variables: provides convenience wrappers for variable creation and manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers\n",
    "\n",
    "| Layer                    | TF-Slim               |\n",
    "|--------------------------|-----------------------|\n",
    "| BiasAdd                  | slim.bias.add         |\n",
    "| BatchNorm                | slim.batch_norm       |\n",
    "| Conv2d                   | slim.conv2d           |\n",
    "| Conv2dInPlane            | slim.conv2d_In_Plane  |\n",
    "| Conv2dTranspose (Deconv) | slim.conv2d_transpose |\n",
    "| FullyConnected           | slim.fully_connected  |\n",
    "| AvgPool2D                | slim.avg_pool2d       |\n",
    "| Dropout                  | slim.dropout          |\n",
    "| Flatten                  | slim.flatten          |\n",
    "| MaxPool2D                | slim.max_pool2d       |\n",
    "| OneHotEncoding           | slim.one_hot_encoding |\n",
    "| SeparableConv2           | slim.separable_conv2d |\n",
    "| UnitNorm                 | slim.uint_norm        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 4 but is rank 2 for 'conv1_1_26/Conv2D' (op: 'Conv2D') with input shapes: [?,784], [3,3,64,128].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[0;32m    672\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Shape must be rank 4 but is rank 2 for 'conv1_1_26/Conv2D' (op: 'Conv2D') with input shapes: [?,784], [3,3,64,128].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-59929954e23a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mkernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weights'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     conv = tf.nn.conv2d(input, kernel, strides=[1, 1, 1, 1],\n\u001b[1;32m----> 6\u001b[1;33m                         padding='SAME')\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mbiases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'biases'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[0;32m    394\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                                 \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[0;32m    397\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    761\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    762\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    764\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2327\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2328\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2329\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2330\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2331\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1715\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1717\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1718\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1719\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1666\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1667\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1669\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[0;32m    611\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape must be rank 4 but is rank 2 for 'conv1_1_26/Conv2D' (op: 'Conv2D') with input shapes: [?,784], [3,3,64,128]."
     ]
    }
   ],
   "source": [
    "input = ...\n",
    "with tf.name_scope('conv1_1') as scope:\n",
    "\n",
    "    kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32, stddev=1e-1), name='weights')\n",
    "    conv = tf.nn.conv2d(input, kernel, strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    \n",
    "    biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32), trainable=True, name='biases')\n",
    "    \n",
    "    conv1 = tf.nn.relu(bias, name=scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In TF-Slim code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected binary or unicode string, got Ellipsis",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-0eb2c4b1b204>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SAME'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'conv1_1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_with_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_key_op'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_key_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\u001b[0m in \u001b[0;36mconvolution\u001b[1;34m(inputs, num_outputs, kernel_size, stride, padding, data_format, rate, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope)\u001b[0m\n\u001b[0;32m    872\u001b[0m       \u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Conv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       custom_getter=layer_variable_getter) as sc:\n\u001b[1;32m--> 874\u001b[1;33m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    875\u001b[0m     \u001b[0minput_rank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[0;32m    635\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m       as_ref=False)\n\u001b[0m\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m           \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    108\u001b[0m                                          as_ref=False):\n\u001b[0;32m    109\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[0;32m     97\u001b[0m   \u001b[0mtensor_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m---> 99\u001b[1;33m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[0;32m    100\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    439\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnumpy_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[0mproto_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FlattenToStrings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m     \u001b[0mtensor_proto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    439\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnumpy_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[0mproto_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FlattenToStrings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m     \u001b[0mtensor_proto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[1;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[1;32m---> 65\u001b[1;33m                     (bytes_or_text,))\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected binary or unicode string, got Ellipsis"
     ]
    }
   ],
   "source": [
    "input = ...\n",
    "net = slim.conv2d(input, 128, [3, 3], padding='SAME', scope='conv1_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializer, Regularizer\n",
    "\n",
    "* Initializer:\n",
    "    * tf.truncated_normal_initializer (included in tf)\n",
    "    * slim.xavier_initializer\n",
    "    * slim.variance_scaling_initializer\n",
    "    \n",
    "* Regularizer:\n",
    "    * slim.l1_regularizer\n",
    "    * slim.l2_regularizer\n",
    "    * ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-dcde8f552113>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m net = slim.conv2d(inputs, 64, [11, 11], 4, padding='SAME',\n\u001b[0m\u001b[0;32m      2\u001b[0m                  \u001b[0mweights_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxavier_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                  \u001b[0mweights_regularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_regularizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                  scope='conv1')\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "net = slim.conv2d(inputs, 64, [11, 11], 4, padding='SAME',\n",
    "                 weights_initializer=slim.xavier_initializer(),\n",
    "                 weights_regularizer=slim.l2_regularizer(0.0005),\n",
    "                 scope='conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### argscope\n",
    "\n",
    "* Eliminate boilerp;ate codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-2538277f94df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m                    weights_regularizer=slim.l2_regularizer(0.0005)):\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SAME'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'conv1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         net = slim.conv2d(net, 256, [5, 5],\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "he_init = slim.variance_scaling_initializer()\n",
    "xavier_init = slim.xavier_initializer()\n",
    "\n",
    "with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                   \n",
    "                    # Arguments are injected to [..]\n",
    "                   activation_fn=tf.nn.relu,\n",
    "                   weights_initializer=he_init,\n",
    "                   weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "    with slim.arg_scope([slim.conv2d], stride=1, padding='SAME'):\n",
    "        net = slim.conv2d(inputs, 64, [11, 11], 4, scope='conv1')\n",
    "        net = slim.conv2d(net, 256, [5, 5],\n",
    "                          \n",
    "                         # Arguments are overwritten \n",
    "                         weights_initializer=xavier_init,\n",
    "                         scope='conv2')\n",
    "        net = slim.fully_connected(net, 1000,\n",
    "                                  \n",
    "                                   # Arguments are overwritten\n",
    "                                  activation_fn=None, scope='fc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Losses\n",
    "\n",
    "* Easy to handle multiple loss and regularize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pref1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-e8c8023484da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Define the loss functions and get the total loss.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloss1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpref1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mloss2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# The following tow lines have the same effect:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pref1' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the loss functions and get the total loss.\n",
    "loss1 = slim.losses.softmax_cross_entropy(pref1, label1)\n",
    "loss2 = slim.losses.mean_squared_error(pred2, label2)\n",
    "\n",
    "# The following tow lines have the same effect:\n",
    "total_loss = loss1 + loss2\n",
    "slim.losses.get_total_loss(add_regularization_losses=False)\n",
    "\n",
    "# If you want to add regularization loss\n",
    "reg_loss = tf.add_n(slim.losses.get_regularization_losses())\n",
    "total_loss = loss1 + loss2 + reg_loss\n",
    "# or\n",
    "total_loss = slim.losses.get_total_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection\n",
    "\n",
    "* included in tf (not slim), anyway it's quite useful\n",
    "    * GLOBAL_VARIABLES\n",
    "    * TRAINABLE_VARIABLES\n",
    "    * SUMMARIES\n",
    "    * MOVING_AVERAGE_VARIABLES\n",
    "    * REGULARIZATION_LOSSES\n",
    "    * WEIGHTS\n",
    "    * BIASES\n",
    "    * ACTIVATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'add_arg_scope' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-18a4bc076bb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Add custom loss to LOSSES collection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;33m@\u001b[0m\u001b[0madd_arg_scope\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0madd_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_collection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLOSSES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mloss_collection\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_collection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'add_arg_scope' is not defined"
     ]
    }
   ],
   "source": [
    "# Add custom loss to LOSSES collection\n",
    "@add_arg_scope\n",
    "def add_loss(loss, loss_collection=ops.GraphKeys.LOSSES):\n",
    "    if loss_collection:\n",
    "        ops.add_to_collection(loss_collection, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ops' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-d8650e60576f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Easy to get all losses with graph Collection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mget_losses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_collection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLOSSES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_collection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_regularization_losses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ops' is not defined"
     ]
    }
   ],
   "source": [
    "# Easy to get all losses with graph Collection\n",
    "def get_losses(scope=None, loss_collection=ops.GraphKeys.LOSSES):\n",
    "    return ops.get_collection(loss_collection, scope)\n",
    "\n",
    "def get_regularization_losses(scope=None):\n",
    "    return ops.get_collection(ops.GraphKeys.REGULARIZATION_LOSSES, scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight:0\n",
      "weight_1:0\n",
      "weight_2:0\n",
      "weight_3:0\n",
      "counter:0\n",
      "counter_1:0\n",
      "counter_2:0\n",
      "Variable:0\n",
      "Variable_1:0\n",
      "Variable_2:0\n",
      "Variable_3:0\n",
      "Variable_4:0\n",
      "Variable_5:0\n",
      "Variable_6:0\n",
      "Variable_7:0\n",
      "Variable_8:0\n",
      "Variable_9:0\n",
      "Variable_10:0\n",
      "Variable_11:0\n",
      "var:0\n",
      "foo/bar/var:0\n",
      "foo/bar/var_1:0\n",
      "var_1:0\n",
      "foo_1/bar/var:0\n",
      "foo_1/bar/var_1:0\n",
      "var_2:0\n",
      "foo/bar/var_2:0\n",
      "foo2/bar2/var2:0\n",
      "conv1_1/weights:0\n",
      "conv1_1_1/weights:0\n",
      "conv1_1_2/weights:0\n",
      "conv1_1_3/weights:0\n",
      "conv1_1_4/weights:0\n",
      "conv1_1_5/weights:0\n",
      "conv1_1_6/weights:0\n",
      "conv1_1_7/weights:0\n",
      "conv1_1_8/weights:0\n",
      "conv1_1_9/weights:0\n",
      "conv1_1_10/weights:0\n",
      "conv1_1_11/weights:0\n",
      "conv1_1_12/weights:0\n",
      "conv1_1_13/weights:0\n",
      "conv1_1_14/weights:0\n",
      "conv1_1_15/weights:0\n",
      "conv1_1_16/weights:0\n",
      "conv1_1_17/weights:0\n",
      "conv1_1_18/weights:0\n",
      "conv1_1_19/weights:0\n",
      "conv1_1_20/weights:0\n",
      "conv1_1_21/weights:0\n",
      "conv1_1_22/weights:0\n",
      "conv1_1_23/weights:0\n",
      "conv1_1_24/weights:0\n",
      "conv1_1_26/weights:0\n"
     ]
    }
   ],
   "source": [
    "# Useful to visualize or debugging\n",
    "\n",
    "for var in tf.trainable_variables():\n",
    "    print(var.name)\n",
    "    \n",
    "def trainable_variable():\n",
    "    return ops.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Restore\n",
    "\n",
    "* My save wrapper function in my custom network class (author: NamHyuk Ahn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(self, ckpt_dir, global_step=None):\n",
    "    \n",
    "    # Create saver to save a model\n",
    "    if self.config.get(\"saver\") is None:\n",
    "        self.config[\"saver\"] = \\\n",
    "            tf.train.Saver(max_to_keep=30)\n",
    "            \n",
    "    saver = self.config[\"saver\"]\n",
    "    sess  = self.config[\"sess\"]\n",
    "    \n",
    "    dirname = os.path.join(ckpt_dir, self.name)\n",
    "    \n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "        \n",
    "    # Save a model in dirname directory. global_step is for recording \"current_step\" in filename\n",
    "    saver.save(sess, dirname, global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* My restore wrapper function (author: NamHyuk Ahn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_latest_checkpoint(self, ckpt_dir, exclude=None):\n",
    "    path = tf.train.latest_checkpoint(ckpt_dir)\n",
    "    if path is None:\n",
    "        raise AssertionError(\"No ckpt exist in {0}.\".format(ckpt_dir))\n",
    "        \n",
    "    print(\"Load {} save file\".format(path))\n",
    "    self.load(path, exclude)\n",
    "    \n",
    "def load_from_path(self, ckpt_path, exclude=None):\n",
    "    self._load(ckpt_path, exclude)\n",
    "    \n",
    "def _load(self, ckpt_path, exclude):\n",
    "    init_fn = slim.assign_from_checkpoint_fn(ckpt_path,\n",
    "        # Exclude exclude variables when load a model                                        \n",
    "        slim.get_variables_to_restore(exclude=exclude),\n",
    "                                             \n",
    "        # If False, raise error when missing variables exist in checkpoint (save) file.\n",
    "        # When fine-tune model, have to set True to avoid error\n",
    "        ignore_missing_vars=True)\n",
    "\n",
    "    init_fn(self.config[\"sess\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16\n",
    "Build network\n",
    "\n",
    "https://github.com/tensorflow/models/blob/836ea2720c5a6e2f4136674f1928a4fbd3de9698/slim/nets/vgg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_arg_scope(weight_decay=0.0005):\n",
    "  \"\"\"Defines the VGG arg scope.\n",
    "  Args:\n",
    "    weight_decay: The l2 regularization coefficient.\n",
    "  Returns:\n",
    "    An arg_scope.\n",
    "  \"\"\"\n",
    "  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                      activation_fn=tf.nn.relu,\n",
    "                      weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "                      biases_initializer=tf.zeros_initializer()):\n",
    "    with slim.arg_scope([slim.conv2d], padding='SAME') as arg_sc:\n",
    "      return arg_sc\n",
    "\n",
    "\n",
    "def vgg_16(inputs,\n",
    "           num_classes=1000,\n",
    "           is_training=True,\n",
    "           dropout_keep_prob=0.5,\n",
    "           spatial_squeeze=True,\n",
    "           scope='vgg_16'):\n",
    "  \"\"\"Oxford Net VGG 16-Layers version D Example.\n",
    "  Note: All the fully_connected layers have been transformed to conv2d layers.\n",
    "        To use in classification mode, resize input to 224x224.\n",
    "  Args:\n",
    "    inputs: a tensor of size [batch_size, height, width, channels].\n",
    "    num_classes: number of predicted classes.\n",
    "    is_training: whether or not the model is being trained.\n",
    "    dropout_keep_prob: the probability that activations are kept in the dropout\n",
    "      layers during training.\n",
    "    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n",
    "      outputs. Useful to remove unnecessary dimensions for classification.\n",
    "    scope: Optional scope for the variables.\n",
    "  Returns:\n",
    "    the last op containing the log predictions and end_points dict.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope, 'vgg_16', [inputs]) as sc:\n",
    "    end_points_collection = sc.name + '_end_points'\n",
    "    # Collect outputs for conv2d, fully_connected and max_pool2d.\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n",
    "                        outputs_collections=end_points_collection):\n",
    "      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "      # Use conv2d instead of fully_connected layers.\n",
    "      net = slim.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout6')\n",
    "      net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout7')\n",
    "      net = slim.conv2d(net, num_classes, [1, 1],\n",
    "                        activation_fn=None,\n",
    "                        normalizer_fn=None,\n",
    "                        scope='fc8')\n",
    "      # Convert end_points_collection into a end_point dict.\n",
    "      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "      if spatial_squeeze:\n",
    "        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\n",
    "        end_points[sc.name + '/fc8'] = net\n",
    "      return net, end_points\n",
    "vgg_16.default_image_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vgg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-2c36fed546a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mis_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"is_training\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvgg_arg_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     net, end_pts = vgg.vgg_16(X, is_training=is_training,\n\u001b[0;32m      7\u001b[0m                               num_classes=1000) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'vgg' is not defined"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 224, 224, 3], name=\"X\") \n",
    "y = tf.placeholder(tf.int32, [None, 8], name=\"y\") \n",
    "is_training = tf.placeholder(tf.bool, name=\"is_training\") \n",
    "\n",
    "with slim.arg_scope(vgg.vgg_arg_scope()): \n",
    "    net, end_pts = vgg.vgg_16(X, is_training=is_training,\n",
    "                              num_classes=1000) \n",
    "    \n",
    "with tf.variable_scope(\"losses\"): \n",
    "    cls_loss = slim.losses.softmax_cross_entropy(net, y) \n",
    "    reg_loss = tf.add_n(slim.losses.get_regularization_losses()) \n",
    "    loss_op = type_loss + reg_loss \n",
    "        \n",
    "with tf.variable_scope(\"opt\"): \n",
    "    opt = tf.train.AdamOptimizer(0.001).minimize(loss_op) \n",
    "\n",
    "# Exclude fc8 layer to train last layer from scratch\n",
    "# To freeze other layers, trainable=False argument is needed in argscope\n",
    "self.load_from_path(ckpt_path=VGG_PATH, exclude=[\"vgg_16/fc8\"])\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "\n",
    "* Very useful visualizatio tool\n",
    "    * Graph visualization, loss/accuracy plot, weight histogram plot ...\n",
    "    \n",
    "* Simple usage\n",
    "    1. Make a tf.summary.FileWriter to write summaries\n",
    "    2. Create summaries using tf.summary.###\n",
    "    3. Run summaries with sess.run(...)\n",
    "    4. Add summaries to FileWriter with add_summary API\n",
    "    5. Run command  \n",
    "       tensorboard --logdir=## --host=## --port=##\n",
    "    6. Default port is 6006\n",
    "    \n",
    "https://www.tensorflow.org/api_docs/python/summary/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorboard & slim example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "max_steps = 10000\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "keep_prob = 0.5\n",
    "weight_decay = 0.0004\n",
    "logs_path = \"/tmp/tensorflow_logs/example\" \n",
    "\n",
    "def my_arg_scope(is_training, weight_decay):\n",
    "    with slim.arg_scope([slim.conv2d],\n",
    "        activation_fn=tf.nn.relu, \n",
    "        weights_regularizer=slim.l2_regularizer(weight_decay), \n",
    "        weights_initializer=slim.variance_scaling_initializer(), \n",
    "        biases_initializer=tf.zeros_initializer,\n",
    "        stride=1, padding=\"SAME\"): \n",
    "        \n",
    "        with slim.arg_scope([slim.dropout], \n",
    "                            is_training=is_training) as arg_sc: \n",
    "            return arg_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_net(x, keep_prob, outputs_collections=\"my_net\"): \n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1]) \n",
    "    \n",
    "    with slim.arg_scope([slim.conv2d, slim.max_pool2d], \n",
    "                        outputs_collections=outputs_collections): \n",
    "        net = slim.conv2d(x, 64, [3, 3], scope=\"conv1\") \n",
    "        net = slim.max_pool2d(net, [2, 2], scope=\"pool1\") \n",
    "        net = slim.conv2d(net, 128, [3, 3], scope=\"conv2\") \n",
    "        net = slim.max_pool2d(net, [2, 2], scope=\"pool2\") \n",
    "        net = slim.conv2d(net, 256, [3, 3], scope=\"conv3\") \n",
    "        # global average pooling \n",
    "        net = tf.reduce_mean(net, [1, 2], name=\"pool3\", keep_dims=True) \n",
    "        net = slim.dropout(net, keep_prob, scope=\"dropout3\") \n",
    "        net = slim.conv2d(net, 1024, [1, 1], scope=\"fc4\") \n",
    "        net = slim.dropout(net, keep_prob, scope=\"dropout4\") \n",
    "        net = slim.conv2d(net, 10, [1, 1], \n",
    "                          activation_fn=None, scope=\"fc5\") \n",
    "        \n",
    "        # Gather variables into end_points\n",
    "        end_points = \\\n",
    "            slim.utils.convert_collection_to_dict(outputs_collections) \n",
    "        \n",
    "        return tf.reshape(net, [-1, 10]), end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784]) \n",
    "y = tf.placeholder(tf.float32, [None, 10]) \n",
    "is_training = tf.placeholder(tf.bool) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-149-e616932b8bce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_arg_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_pts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"prediction\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-141-194f27ece28b>\u001b[0m in \u001b[0;36mmy_net\u001b[1;34m(x, keep_prob, outputs_collections)\u001b[0m\n\u001b[0;32m      4\u001b[0m     with slim.arg_scope([slim.conv2d, slim.max_pool2d], \n\u001b[0;32m      5\u001b[0m                         outputs_collections=outputs_collections): \n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"conv1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pool1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"conv2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_with_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_key_op'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_key_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\u001b[0m in \u001b[0;36mconvolution\u001b[1;34m(inputs, num_outputs, kernel_size, stride, padding, data_format, rate, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope)\u001b[0m\n\u001b[0;32m    905\u001b[0m                         \u001b[0m_scope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m                         _reuse=reuse)\n\u001b[1;32m--> 907\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m     \u001b[1;31m# Add variables to collections.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \"\"\"\n\u001b[1;32m--> 303\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m           \u001b[0minput_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    143\u001b[0m                                   \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_regularizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                                   \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                                   dtype=self.dtype)\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[0;32m    986\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 988\u001b[1;33m       custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m    989\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m    990\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[0;32m    888\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m           custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[0;32m    339\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m           validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m    342\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m       return _true_getter(\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36mvariable_getter\u001b[1;34m(getter, name, shape, dtype, initializer, regularizer, trainable, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m           \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m           variable_getter=functools.partial(getter, **kwargs))\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;31m# Build (if necessary) and call the layer, inside a variable scope.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m_add_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, variable_getter)\u001b[0m\n\u001b[0;32m    206\u001b[0m                                \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m                                \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m                                trainable=trainable and self.trainable)\n\u001b[0m\u001b[0;32m    209\u001b[0m     \u001b[1;31m# TODO(sguada) fix name = variable.op.name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvariable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexisting_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\u001b[0m in \u001b[0;36mlayer_variable_getter\u001b[1;34m(getter, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1308\u001b[0m       \u001b[0mgetter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_custom_getter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rename'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrename\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1310\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_model_variable_getter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1311\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mlayer_variable_getter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\u001b[0m in \u001b[0;36m_model_variable_getter\u001b[1;34m(getter, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, rename, **_)\u001b[0m\n\u001b[0;32m   1297\u001b[0m       \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m       \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1299\u001b[1;33m       custom_getter=getter)\n\u001b[0m\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_with_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_key_op'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_key_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\variables.py\u001b[0m in \u001b[0;36mmodel_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device, partitioner, custom_getter)\u001b[0m\n\u001b[0;32m    266\u001b[0m                  \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m                  \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m                  partitioner=partitioner, custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m    269\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_with_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_key_op'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_key_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\variables.py\u001b[0m in \u001b[0;36mvariable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device, partitioner, custom_getter)\u001b[0m\n\u001b[0;32m    223\u001b[0m                   \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                   \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m                   partitioner=partitioner)\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[0;32m    331\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[0;32m    682\u001b[0m         \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvariable_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 684\u001b[1;33m         validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m    685\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m     logging.vlog(1, \"Created variable %s with shape %s and init %s\", v.name,\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope)\u001b[0m\n\u001b[0;32m    195\u001b[0m           \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m           expected_shape=expected_shape)\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape)\u001b[0m\n\u001b[0;32m    272\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Initializer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m               self._initial_value = ops.convert_to_tensor(\n\u001b[1;32m--> 274\u001b[1;33m                   initial_value(), name=\"initial_value\", dtype=dtype)\n\u001b[0m\u001b[0;32m    275\u001b[0m               shape = (self._initial_value.get_shape()\n\u001b[0;32m    276\u001b[0m                        if validate_shape else tensor_shape.unknown_shape())\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    671\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m         init_val = lambda: initializer(\n\u001b[1;32m--> 673\u001b[1;33m             shape.as_list(), dtype=dtype, partition_info=partition_info)\n\u001b[0m\u001b[0;32m    674\u001b[0m         \u001b[0mvariable_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got multiple values for argument 'dtype'"
     ]
    }
   ],
   "source": [
    "with slim.arg_scope(my_arg_scope(is_training, weight_decay)): \n",
    "    net, end_pts = my_net(x, keep_prob) \n",
    "    pred = slim.softmax(net, scope=\"prediction\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-150-b1fbdc0f1876>:2: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead.\n",
      "WARNING:tensorflow:From C:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:394: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From C:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "WARNING:tensorflow:From <ipython-input-150-b1fbdc0f1876>:3: get_regularization_losses (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.get_regularization_losses instead.\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"losses\"): \n",
    "    cls_loss = slim.losses.softmax_cross_entropy(net, y) \n",
    "    reg_loss = tf.add_n(slim.losses.get_regularization_losses()) \n",
    "    loss_op = cls_loss + reg_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To gather grad to visualize\n",
    "with tf.variable_scope(\"Adam\"): \n",
    "    opt = tf.train.AdamOptimizer(lr) \n",
    "    # Op to calculate every variable gradient \n",
    "    grads = tf.gradients(loss_op, tf.trainable_variables()) \n",
    "    grads = list(zip(grads, tf.trainable_variables())) \n",
    "    # Op to update all variables according to their gradient \n",
    "    apply_grads = opt.apply_gradients(grads_and_vars=grads) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"accuracy\"): \n",
    "    correct_op = tf.equal(tf.argmax(net, 1), tf.argmax(y, 1)) \n",
    "    acc_op = tf.reduce_mean(tf.cast(correct_op, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name weight:0 is illegal; using weight_0 instead.\n",
      "INFO:tensorflow:Summary name weight_1:0 is illegal; using weight_1_0 instead.\n",
      "INFO:tensorflow:Summary name weight_2:0 is illegal; using weight_2_0 instead.\n",
      "INFO:tensorflow:Summary name weight_3:0 is illegal; using weight_3_0 instead.\n",
      "INFO:tensorflow:Summary name counter:0 is illegal; using counter_0 instead.\n",
      "INFO:tensorflow:Summary name counter_1:0 is illegal; using counter_1_0 instead.\n",
      "INFO:tensorflow:Summary name counter_2:0 is illegal; using counter_2_0 instead.\n",
      "INFO:tensorflow:Summary name Variable:0 is illegal; using Variable_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_1:0 is illegal; using Variable_1_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_2:0 is illegal; using Variable_2_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_3:0 is illegal; using Variable_3_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_4:0 is illegal; using Variable_4_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_5:0 is illegal; using Variable_5_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_6:0 is illegal; using Variable_6_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_7:0 is illegal; using Variable_7_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_8:0 is illegal; using Variable_8_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_9:0 is illegal; using Variable_9_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_10:0 is illegal; using Variable_10_0 instead.\n",
      "INFO:tensorflow:Summary name Variable_11:0 is illegal; using Variable_11_0 instead.\n",
      "INFO:tensorflow:Summary name var:0 is illegal; using var_0 instead.\n",
      "INFO:tensorflow:Summary name foo/bar/var:0 is illegal; using foo/bar/var_0 instead.\n",
      "INFO:tensorflow:Summary name foo/bar/var_1:0 is illegal; using foo/bar/var_1_0 instead.\n",
      "INFO:tensorflow:Summary name var_1:0 is illegal; using var_1_0 instead.\n",
      "INFO:tensorflow:Summary name foo_1/bar/var:0 is illegal; using foo_1/bar/var_0 instead.\n",
      "INFO:tensorflow:Summary name foo_1/bar/var_1:0 is illegal; using foo_1/bar/var_1_0 instead.\n",
      "INFO:tensorflow:Summary name var_2:0 is illegal; using var_2_0 instead.\n",
      "INFO:tensorflow:Summary name foo/bar/var_2:0 is illegal; using foo/bar/var_2_0 instead.\n",
      "INFO:tensorflow:Summary name foo2/bar2/var2:0 is illegal; using foo2/bar2/var2_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1/weights:0 is illegal; using conv1_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_1/weights:0 is illegal; using conv1_1_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_2/weights:0 is illegal; using conv1_1_2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_3/weights:0 is illegal; using conv1_1_3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_4/weights:0 is illegal; using conv1_1_4/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_5/weights:0 is illegal; using conv1_1_5/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_6/weights:0 is illegal; using conv1_1_6/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_7/weights:0 is illegal; using conv1_1_7/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_8/weights:0 is illegal; using conv1_1_8/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_9/weights:0 is illegal; using conv1_1_9/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_10/weights:0 is illegal; using conv1_1_10/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_11/weights:0 is illegal; using conv1_1_11/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_12/weights:0 is illegal; using conv1_1_12/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_13/weights:0 is illegal; using conv1_1_13/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_14/weights:0 is illegal; using conv1_1_14/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_15/weights:0 is illegal; using conv1_1_15/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_16/weights:0 is illegal; using conv1_1_16/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_17/weights:0 is illegal; using conv1_1_17/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_18/weights:0 is illegal; using conv1_1_18/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_19/weights:0 is illegal; using conv1_1_19/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_20/weights:0 is illegal; using conv1_1_20/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_21/weights:0 is illegal; using conv1_1_21/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_22/weights:0 is illegal; using conv1_1_22/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_23/weights:0 is illegal; using conv1_1_23/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_24/weights:0 is illegal; using conv1_1_24/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1_1_26/weights:0 is illegal; using conv1_1_26/weights_0 instead.\n",
      "INFO:tensorflow:Summary name conv1/weights:0 is illegal; using conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name weight:0/gradient is illegal; using weight_0/gradient instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[0;32m    492\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m           \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    109\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[0;32m     98\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m---> 99\u001b[1;33m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[0;32m    100\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"None values not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m     \u001b[1;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: None values not supported.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-153-69538cb8b6e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"my_summ\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/gradient\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"my_summ\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0msumm_wg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"my_summ\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\summary\\summary.py\u001b[0m in \u001b[0;36mhistogram\u001b[1;34m(name, values, collections)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     val = _gen_logging_ops._histogram_summary(\n\u001b[1;32m--> 203\u001b[1;33m         tag=scope.rstrip('/'), values=values, name=scope)\n\u001b[0m\u001b[0;32m    204\u001b[0m     \u001b[0m_collect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSUMMARIES\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_logging_ops.py\u001b[0m in \u001b[0;36m_histogram_summary\u001b[1;34m(tag, values, name)\u001b[0m\n\u001b[0;32m    137\u001b[0m   \"\"\"\n\u001b[0;32m    138\u001b[0m   result = _op_def_lib.apply_op(\"HistogramSummary\", tag=tag, values=values,\n\u001b[1;32m--> 139\u001b[1;33m                                 name=name)\n\u001b[0m\u001b[0;32m    140\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[1;31m# What type does convert_to_tensor think it has?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             observed = ops.internal_convert_to_tensor(\n\u001b[1;32m--> 504\u001b[1;33m                 values, as_ref=input_arg.is_ref).dtype.name\n\u001b[0m\u001b[0;32m    505\u001b[0m             prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\n\u001b[0;32m    506\u001b[0m                       (input_name, op_type_name, observed))\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m           \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    108\u001b[0m                                          as_ref=False):\n\u001b[0;32m    109\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[0;32m     97\u001b[0m   \u001b[0mtensor_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m---> 99\u001b[1;33m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[0;32m    100\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[1;32mC:\\Users\\JongSeop\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    358\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"None values not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m     \u001b[1;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;31m# provided if possible.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "# Create a summary to monitor loss and accuracy \n",
    "summ_loss = tf.summary.scalar(\"loss\", loss_op) \n",
    "summ_acc  = tf.summary.scalar(\"accuracy_test\", acc_op) \n",
    "\n",
    "# Create summaries to visualize weights and grads \n",
    "for var in tf.trainable_variables(): \n",
    "    tf.summary.histogram(var.name, var, collections=[\"my_summ\"]) \n",
    "for grad, var in grads: \n",
    "    tf.summary.histogram(var.name + \"/gradient\", grad, collections=[\"my_summ\"]) \n",
    "    \n",
    "    summ_wg = tf.summary.merge_all(key=\"my_summ\")\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summ_wg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-1d0f2e1ef34a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mbatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     _, loss, plot_loss, plot_wg = sess.run([apply_grads, loss_op, \n\u001b[1;32m----> 4\u001b[1;33m                                             summ_loss, summ_wg], \n\u001b[0m\u001b[0;32m      5\u001b[0m         feed_dict={x: batch_X, y: batch_y, is_training: True})\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'summ_wg' is not defined"
     ]
    }
   ],
   "source": [
    "for step in range(max_steps): \n",
    "    batch_X, batch_y = mnist.train.next_batch(batch_size)\n",
    "    _, loss, plot_loss, plot_wg = sess.run([apply_grads, loss_op, \n",
    "                                            summ_loss, summ_wg], \n",
    "        feed_dict={x: batch_X, y: batch_y, is_training: True})\n",
    "    \n",
    "    summary_writer.add_summary(plot_loss, step)\n",
    "    summary_writer.add_summary(plot_wg, step)\n",
    "    \n",
    "    if (step+1) % 100 == 0: \n",
    "        plot_acc = sess.run(summ_acc, feed_dict={x: mnist.test.images,\n",
    "                                                 y: mnist.test.labels,\n",
    "                                                 is_training: False})\n",
    "        \n",
    "        summary_writer.add_summary(plot_acc, step)\n",
    "        \n",
    "print(\"Optimization Finished!\") \n",
    "        \n",
    "test_acc = sess.run(acc_op, feed_dict={x: mnist.test.images,\n",
    "                                       y: mnist.test.labels,\n",
    "                                       is_training: False})\n",
    "        \n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss/Accuracy plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reading\n",
    "\n",
    "1. Pre-loading\n",
    "    * Pre-load data into  memory and feed to graph using feed_dict\n",
    "    * Fast if  your memory is sufficient :(\n",
    "    * Impossible to use this manner in practice\n",
    "    * Previous example uses this way\n",
    "    \n",
    "2. Feeding\n",
    "    * Load data, pre-process and feed to graph in every iteration\n",
    "    * Easy to implement, but slow\n",
    "    * GPU utilization is low (GPU is idel when data is loading)\n",
    "    \n",
    "3. Reading from files\n",
    "    * Using TFRecords, Queue in TensorFlow API\n",
    "    * Data loading is done by multi-threading\n",
    "    * Efficient and fast, but hard to implement\n",
    "    * Don't expect TensorFlow Docs, rather recomment these blogs:  \n",
    "        https://indico.io/blog/tensorflow-data-inputs-part1-placeholders-protobufs-queues/  \n",
    "        https://warmspringwinds.github.io/tensorflow/tf-slim/2016/12/21/tfrecords-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
