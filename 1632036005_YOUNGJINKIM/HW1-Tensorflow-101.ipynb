{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Tutorial\n",
    "- NamHyuk Ahn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "- TensorFlowKR-2017-talk by Jongwook Choi, Beomjun Shin\n",
    "    + https://wookayin.github.io/TensorFlowKR-2017-talk-bestpractice/ko/\n",
    "- Stanford CS224d Lecture 7\n",
    "    + http://cs244d.stanford.edu/lectures/CS244d-Lecture7.pdf(X)\n",
    "    + 현재 주소 : http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-tensorflow.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TensorFlow?\n",
    "- 구글에서 개발한 딥러닝 오픈소스 라이브러리 (Open-source deep learning library developed by Google)\n",
    "- why do people use TensorFlow?\n",
    "    + Easy numerical computation\n",
    "    + <font color='red'>Automatic derivatives</font> computation\n",
    "    + Deploy computation to multiple CPUs or GPUs\n",
    "    + 강력한 데이터 시각화 툴킷 (Powerful data visualization toolkit)\n",
    "- https://github.com/tensorflow/tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Tensor?\n",
    "- Terminology used in math and physics\n",
    "\n",
    "- Simply, tensor is a expansion of vector and matrix (not correct actually...)\n",
    "\n",
    "- Zero-order tensor: Scalar\n",
    "- First-order tensor: Vector\n",
    "- Second-order tensor: Matrix\n",
    "\n",
    "- So, tensor can be represented as a N-Dimension array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you know Numpy* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.zeros((2, 2)); b = np.ones((2, 2))\n",
    "np.sum(b, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.reshape(a, (1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python library for numerical computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.zeros((2, 2)); b = tf.ones((2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros:0\", shape=(2, 2), dtype=float32)\n",
      "Tensor(\"ones:0\", shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(a) #모든 원소의 값이 0인 Tensor\n",
    "print(b) #모든 원소의 값이 1인 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.reduce_sum(b, axis=1)) #모든 원소의 값이 1인 2x2텐서를 1x2로 합 : [2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.reduce_sum(a, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(2)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(2)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.reshape(a, (1, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.reshape(b, (1, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sess.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "a = np.zeros((2, 2)); ta = tf.zeros((2, 2))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros_1:0\", shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(ta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ta <- \n",
    "Tensorflow first defines a <font color='red'>computation graph</font>\n",
    "that has no numerical value until evaluated.\n",
    "(very similar concept to Theano)\n",
    "\n",
    "텐서플로우는 Computation Graph를 먼저 정의함. Theano와 매우 유사한 개념"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(ta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  2.]\n",
      " [ 2.  2.]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(ta+1*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In TensorFlow program, it first assembles a <font color='blue'>graph</font>, and uses a session to <font color='blue'>execute</font> ops in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.constant(5.0)\n",
    "b = tf.constant(6.0)\n",
    "c = a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"mul:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a <font color='red'>computation graph.</font> At this time, we cannot see real value.\n",
    "\n",
    "위 시점에서, Computation Graph르 정의함, 이 시점에서 실제 값을 볼 수 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate pre-defined computation graph. Graph evaluation must be followed by tf.<font color='blue'>Session</font> creation.\n",
    "\n",
    "- 미리 정의된 그래프, sess.run에서 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A <font color='blue'>Session</font> object encapsulates the environment in which operation and tensor objects are evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Both type of the session creation has same behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n",
      "30.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))\n",
    "    print(c.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c.eval() <- Syntactic sugar for sess.run(c) in the current active session. (Must locate inside of the tf.<font color='blue'>Session</font> scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All tensors used previously were constant tensors\n",
    "- To train a model, we need variable type tensor to hold and update values\n",
    "  \n",
    "  모델을 학습하기 위해, 값을 hold하고 업데이트하기 위한 variable 타입의 텐서가 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value weight\n\t [[Node: weight/_0 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_4_weight\", _device=\"/job:localhost/replica:0/task:0/gpu:0\"](weight)]]\n\t [[Node: weight/_1 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_4_weight\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value weight\n\t [[Node: weight/_0 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_4_weight\", _device=\"/job:localhost/replica:0/task:0/gpu:0\"](weight)]]\n\t [[Node: weight/_1 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_4_weight\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c90468e318cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"weight\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value weight\n\t [[Node: weight/_0 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_4_weight\", _device=\"/job:localhost/replica:0/task:0/gpu:0\"](weight)]]\n\t [[Node: weight/_1 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_4_weight\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]"
     ]
    }
   ],
   "source": [
    "#Attempting to use uninitialized value weight\n",
    "w = tf.Variable(tf.zeros((2, 2)), name=\"weight\")\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables need <font color='red'>initialization</font>!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01925569  0.10745185]\n",
      " [ 0.020722    0.22812574]\n",
      " [-0.14141308  0.14986865]\n",
      " [ 0.05808049  0.04597104]\n",
      " [ 0.02894816 -0.11611797]]\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(tf.random_normal([5, 2], stddev=0.1), name=\"weight\") #5x2 shape의 random value를 같는 variable 생성\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # 다른 op가 수행되기 전에 실행되어야 함.\n",
    "    print(sess.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.<font color='blue'>Variable</font> can be initialized from constants or random values\n",
    "\n",
    "variable 타입은 상수나 랜덤값으로 초기화될 수 있음\n",
    "\n",
    "tf.<font color='blue'>Variable</font> needs initialization step.\n",
    "\n",
    "variable 타입은 반드시 초기화 단계를 거쳐야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "state = tf.Variable(0, name=\"counter\") # 상수 0으로 variable 생성 : 변수\n",
    "new_value = tf.add(state, tf.constant(1)) # 1을 더하는 op : 함수?\n",
    "update = tf.assign(state, new_value) # 업데이트를 적용하는 op : 함수?\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(state))\n",
    "    for _ in range(3): # 0~3 반복\n",
    "        sess.run(update)\n",
    "        print(sess.run(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.constant(1)\n",
    "x2 = tf.constant(2)\n",
    "x3 = tf.constant(3)\n",
    "\n",
    "temp = tf.add(x2, x3)\n",
    "mul = tf.multiply(x1, temp)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result1, result2 = sess.run([mul, temp])\n",
    "    print(result1, result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling sess.run(var) outputs its value. \n",
    "\n",
    "Of course, we can output multiple tensors\n",
    "\n",
    "Simultaneously with sess.run([var1, ... ,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/you359/aiclass/blob/master/1632036005_YOUNGJINKIM/files/tensorflow-tutorial-22-638.jpg?raw=true\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TemsorFlow Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All previous examples have manually defined tensors\n",
    "    + How can we input external data into TensorFlow?\n",
    "    + Most simple way is using tf.placeholder and feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.int16)\n",
    "b = tf.placeholder(tf.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a <font color='red'>placeholder</font> to hold data.\n",
    "\n",
    "tf.placeholder is dummy node to provide entry points for data to computation graph.\n",
    "\n",
    "placeholder는 계산 시, 데이터를 위한 entry point를 제공하기 위한 더미 노드임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "add = tf.add(a, b)\n",
    "mul = tf.multiply(a, b)\n",
    "\n",
    "#runtime에서 데이터가 들어감?\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(add, feed_dict={a: 2, b: 3}))\n",
    "    print(sess.run(mul, feed_dict={a: 2, b: 3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feed_dict is a python dictionary from tf.placeholder to data value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n",
      "[[ 12.]]\n"
     ]
    }
   ],
   "source": [
    "# using tf.constant\n",
    "matrix1 = tf.constant([[3., 3.]])\n",
    "matrix2 = tf.constant([[2.], [2.]])\n",
    "product = tf.matmul(matrix1, matrix2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(product)\n",
    "    print(result)\n",
    "    \n",
    "# using placeholder\n",
    "import numpy as np\n",
    "\n",
    "matrix1 = tf.placeholder(tf.float32, [1, 2])\n",
    "matrix2 = tf.placeholder(tf.float32, [2, 1])\n",
    "product = tf.matmul(matrix1, matrix2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    mv1 = np.array([[3., 3.]])\n",
    "    mv2 = np.array([[2.], [2.]])\n",
    "    result = sess.run(product, feed_dict={matrix1: mv1, matrix2: mv2})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example = MNIST with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "\n",
    "learning_rate = 0.001\n",
    "max_steps = 15000\n",
    "batch_size = 128\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MLP(inputs): #3Layer MLP(784- 256 - 10)\n",
    "    w_1 = tf.Variable(tf.random_normal([784,256]))\n",
    "    b_1 = tf.Variable(tf.zeros([256]))\n",
    "    \n",
    "    w_2 = tf.Variable(tf.random_normal([256,256]))\n",
    "    b_2 = tf.Variable(tf.zeros([256]))\n",
    "    \n",
    "    w_out = tf.Variable(tf.random_normal([256, 10]))\n",
    "    b_out = tf.Variable(tf.zeros([10]))\n",
    "    \n",
    "    h_1 = tf.add(tf.matmul(inputs, w_1), b_1)\n",
    "    h_1 = tf.nn.relu(h_1)\n",
    "    \n",
    "    h_2 = tf.add(tf.matmul(h_1, w_2), b_2)\n",
    "    h_2 = tf.nn.relu(h_2)\n",
    "    \n",
    "    out = tf.add(tf.matmul(h_2, w_out), b_out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "net = MLP(x)\n",
    "\n",
    "# define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=net, labels=y))\n",
    "opt = tf.train.AdamOptimizer(learning_rate).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000/15000] loss:21.901\n",
      "[2000/15000] loss:18.465\n",
      "[3000/15000] loss:6.363\n",
      "[4000/15000] loss:8.929\n",
      "[5000/15000] loss:0.969\n",
      "[6000/15000] loss:0.254\n",
      "[7000/15000] loss:0.674\n",
      "[8000/15000] loss:0.587\n",
      "[9000/15000] loss:1.444\n",
      "[10000/15000] loss:0.000\n",
      "[11000/15000] loss:0.000\n",
      "[12000/15000] loss:0.000\n",
      "[13000/15000] loss:0.000\n",
      "[14000/15000] loss:0.077\n",
      "[15000/15000] loss:0.005\n",
      "Optimization Finished!\n",
      "Train accuracy: 0.997\n",
      "Test accuracy: 0.955\n"
     ]
    }
   ],
   "source": [
    "# initializing the variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "# train model\n",
    "for step in range(max_steps):\n",
    "    batch_X, batch_y = mnist.train.next_batch(batch_size)\n",
    "    _, loss = sess.run([opt, loss_op], feed_dict={x: batch_X, y: batch_y})\n",
    "    \n",
    "    if (step+1) % 1000 == 0:\n",
    "        print(\"[{}/{}] loss:{:.3f}\".format(step+1, max_steps, loss))\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "#test model\n",
    "correct_prediction = tf.equal(tf.argmax(net, 1), tf.argmax(y, 1))\n",
    "\n",
    "#calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Train accuracy: {:.3f}\".format(sess.run(accuracy, feed_dict={x: mnist.train.images, y: mnist.train.labels})))\n",
    "print(\"Test accuracy: {:.3f}\".format(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model\n",
    "- <font color='red'>tf.nn.softmax_cross_entropy_with_logits</font> is for calculate cross entropy loss between predictions and labels (instead, you can make own loss function)\n",
    "- <font color='red'>tf.train.####Optimizer</font> create an optimizer such as SGD, RMSProp, Adam etc..*\n",
    "- <font color='red'>tf.train.####Optimizer.minimize(loss_op)</font> adds optimization operation to computation graph\n",
    "- we don't have to be concern about computing gradients and updating variables\n",
    "    * https://www.tensorflow/org/api_docs/python/train/optimizers(X)\n",
    "    + https://www.tensorflow.org/api_docs/python/tf/train/Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Scope\n",
    "- Complicated TensorFlow models can have hundreds of variables - 모델은 수백개 이상의 variable를 갖음, 따라서 분리가 필요\n",
    "    - tf.variable_scope() provides simples name-spacing to avoid clashes\n",
    "    - tf.get_variable() creates/accesses variables from within a variable scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.variable_scope()\n",
    "- Variable scope is a simple type of name-spacing that adds prefixes to variable names within scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1 var:0\n",
      "var2 foo/bar/var:0\n",
      "var3 foo/bar/var_1:0\n"
     ]
    }
   ],
   "source": [
    "var1 = tf.Variable([1], name=\"var\")\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    with tf.variable_scope(\"bar\"):\n",
    "        var2 = tf.Variable([1], name=\"var\")\n",
    "        var3 = tf.Variable([1], name=\"var\")\n",
    "        \n",
    "print(\"var1 {}\".format(var1.name))\n",
    "print(\"var2 {}\".format(var2.name))\n",
    "print(\"var3 {}\".format(var3.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### tf.get_variable()\n",
    "- tf.Variable doesn't provide variable reuse => variable 재사용을 제공하지 않음\n",
    "    + Variable reuse is necessary to implement RNN or Recursive NN => RNN이나 Recursive NN을 구현하기 위해서는 재사용이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1 var_1:0\n",
      "var2 foo_1/bar/var:0\n",
      "var3 foo_1/bar/var_1:0\n"
     ]
    }
   ],
   "source": [
    "var1 = tf.Variable([1], name=\"var\")\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    with tf.variable_scope(\"bar\") as scp:\n",
    "        var2 = tf.Variable([1], name=\"var\")\n",
    "        scp.reuse_variables() # allow resue variables\n",
    "        var3 = tf.Variable([1], name=\"var\")\n",
    "        \n",
    "print(\"var1 {}\".format(var1.name))\n",
    "print(\"var2 {}\".format(var2.name))\n",
    "print(\"var3 {}\".format(var3.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want var3 refer to foo/bar/var:0 But impossible with tf.Variable\n",
    "- This works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1 var:0\n",
      "var2 foo/bar/var:0\n",
      "var3 foo/bar/var:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph() #그래프 리셋\n",
    "\n",
    "var1 = tf.get_variable(\"var\", [1])\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    with tf.variable_scope(\"bar\") as scp:\n",
    "        var2 = tf.get_variable(\"var\", [1])\n",
    "        scp.reuse_variables() # allow resue variables\n",
    "        var3 = tf.get_variable(\"var\", [1])\n",
    "        \n",
    "print(\"var1 {}\".format(var1.name))\n",
    "print(\"var2 {}\".format(var2.name))\n",
    "print(\"var3 {}\".format(var3.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Behavior depends on whether variable reuse enabled\n",
    "- <font color='blue'>Case 1</font>: reuse flag set to False\n",
    "    + Create new variable and return\n",
    "    + Raise <font color='green'>ValueError</font> if there exists variable with given name\n",
    "- <font color='blue'>Case 2</font>: reuse flag set to True\n",
    "    + Search for existing variable with given name\n",
    "    + Raise <font color='green'>ValueError</font> if not found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1 foo/bar/var:0\n",
      "var2 foo/bar/var:0\n",
      "var3 foo/bar/var:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph() #그래프 리셋\n",
    "\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    with tf.variable_scope(\"bar\") as scp:\n",
    "        var1 = tf.get_variable(\"var\", [1])\n",
    "        scp.reuse_variables() # allow resue variables\n",
    "        var2 = tf.get_variable(\"var\", [1])\n",
    "        \n",
    "    with tf.variable_scope(\"bar\", reuse=True):\n",
    "        var3 = tf.get_variable(\"var\", [1])\n",
    "        \n",
    "print(\"var1 {}\".format(var1.name))\n",
    "print(\"var2 {}\".format(var2.name))\n",
    "print(\"var3 {}\".format(var3.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips\n",
    "- <font color='red'>ALWAYS</font> use tf.get_variable instead of tf.Variable\n",
    "- 항상 tf.get_variable을 사용하자\n",
    "    + (tf.Variable doesn't provide parameter sharing)\n",
    "    + tf.Variable 은 parameter sharing을 제공하지 않음\n",
    "    + (Can't use initializer and regularizer in tf.Variable)\n",
    "    + tf.variable은 initializer나 regularizer를 사용할 수 없음\n",
    "- <font color='red'>GOOD scoping</font> is essential\n",
    "    + When visualize and recognize network\n",
    "    + 네트워크(모델)의 시각화나 이해를 위해\n",
    "    + Easy to get variables and ops by name or suffix\n",
    "    tf.###.get_variables_by_name(\"my_var\", \"my_scope\")\n",
    "    + this is useful when debug, load pre-trained model\n",
    "    + 디버깅이나 pre-trained 모델을 로드할 때 유용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - MNIST with CNN\n",
    "- Most of the code is same as previous MLP example except code for building network\n",
    "- CNN model needs conv, pool and FC layer\n",
    "    + tf.nn.conv2d(input, filter, strides, padding, name=None)\n",
    "    + tf.nn.##_pool(value, ksize, strides, padding, name=None)\n",
    "    + https://www.tensorflow.org/api_docs/python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import variance_scaling_initializer\n",
    "he_init = variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another init strategy called He init introduced in PReLuNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(bottom, num_filter, ksize=3, stride=1, padding=\"SAME\", scope=None):\n",
    "    bottom_shape = bottom.get_shape().as_list()[3]\n",
    "    # To calculate shape of previous(input) layer\n",
    "    \n",
    "    with tf.variable_scope(scope or \"conv\"):\n",
    "        W = tf.get_variable(\"W\", [ksize, ksize, bottom_shape, num_filter], initializer=he_init)\n",
    "        b = tf.get_variable(\"b\", [num_filter], initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        x = tf.nn.conv2d(botton, W, strides=[1, stride, stride, 1], padding=padding)\n",
    "        x = tf.nn.relu(tf.nn.bias_add(x, b))\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxpool(bottom, ksize=2, stride=2, padding=\"SAME\", scope=None):\n",
    "    with tf.variable_scope(scope or \"maxpool\"):\n",
    "        pool = tf.nn.max_pool(bottom, ksize=[1, ksize, ksize, 1], strides=[1, stride, stride, 1], padding=padding)\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc(bottom, num_dims, scope=None):\n",
    "    bottom_shape = bottom.get_shape().as_list()\n",
    "    if len(bottom_shape) > 2:\n",
    "        bottom = tf.reshape(bottom, [-1, reduce(lambda x, y: x*y, bottom_shape[1:])])\n",
    "        bottom_shape = bottom.get_shape().as_list()\n",
    "    \n",
    "    with tf.variable_scope(scope or \"fc\"):\n",
    "        W = tf.get_variable(\"W\", [bottom_shape[1], num_dims], initializer=he_init)\n",
    "        b = tf.get_variable(\"b\", [num_dims], initializer=tf.constant_initializer(0))\n",
    "        out = tf.nn.bias_add(tf.matmul(bottom, W), b)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if dim of bottom layer is more than 2, flatten\n",
    "   - output of conv has (N, H, W, C) shape\n",
    "   - output of FC has (N, C) shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_relu(bottom, num_dims, scope=None):\n",
    "    with tf.variable_scope(scope or \"fc\"):\n",
    "        out = fc(bottom, num_dims, scope=\"fc\")\n",
    "        relu = tf.nn.relu(out)\n",
    "        \n",
    "    return relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32, None)\n",
    "\n",
    "def conv_net(x, keep_prob):\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    conv1 = conv(x,32, 5, scope=\"conv_1\")\n",
    "    conv1 = maxpool(conv1, scope=\"maxpool_1\")\n",
    "    conv2 = conv(conv1, 64, 5, scope=\"conv_2\")\n",
    "    conv2 = maxpool(conv2, scope=\"maxpool_2\")\n",
    "    \n",
    "    fc1 = fc_relu(conv2, 1024, scope=\"fc_1\")\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    out = fc(fc1, 10, scope=\"out\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To hold keeping probability in dropout\n",
    "\n",
    "Feed value between(0, 1.0) in training, feed 1.0 in test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session config\n",
    "- by default, session consume all the GPU resource, no matter how a model capacity is\n",
    "    + With Killing all the other process!!! :(\n",
    "\n",
    "- Try this config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(\n",
    "    gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are a bunch of configuration, see\n",
    "\n",
    "  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.contrib\n",
    "- tf.contrib contains contributed code\n",
    "    + Contain contributions that eventually should get merged into core Tensorflow\n",
    "    + Code in tf.contrib isn't supported by the Tensorflow team\n",
    "      it is included in the hope that it is helful, but it might change or be removed at any time\n",
    "    + Many useful interfaces or utils exist\n",
    "    + RNN, Seq2Seq, layers(wrapper) and slim are quite useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.contrib.slim\n",
    "- Lightweight library in TensorFlow\n",
    "    + Components of slim can be freely mixed with native TensorFlow\n",
    "    + Gather useful tf.contrib component into tf.contrib.slim namespace\n",
    "\n",
    "- Why tf.slim?\n",
    "    + Makes building, training and evaluation neural networks simple\n",
    "    + Allows to define models compactly by eliminating boilerplate code\n",
    "    + Serveral widely used models have been developed in slim\n",
    "    + Other useful utilities are provided (initializer, regularizers, losses ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/you359/aiclass/blob/master/1632036005_YOUNGJINKIM/files/tensorflow-tutorial-55-638.jpg?raw=true\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/you359/aiclass/blob/master/1632036005_YOUNGJINKIM/files/tensorflow-tutorial-56-638.jpg?raw=true\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "input = tf.placeholder(shape=[3, 3, 3, 64], dtype='float32')\n",
    "with tf.name_scope('conv1_1') as scope:\n",
    "    kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32, stddev=1e-1), name=\"weights\")\n",
    "    conv = tf.nn.conv2d(input, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32), trainable=True, name='biases')\n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    "    conv1 = tf.nn.relu(bias, name=scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TF-Slim code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "import tensorflow.contrib.slim as slim\n",
    "input = tf.placeholder(shape=[3, 3, 3, 64], dtype='float32')\n",
    "net = slim.conv2d(input, 128,[3,3], padding = 'SAME', scope='conv1_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'fc1/fc1_3/Relu:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. simple network generation with slim\n",
    "net = tf.placeholder(shape=[3, 3, 3, 64], dtype='float32')\n",
    "net = slim.conv2d(net, 256, [3, 3], scope='conv3_1')\n",
    "net = slim.conv2d(net, 256, [3, 3], scope='conv3_2')\n",
    "net = slim.conv2d(net, 256, [3, 3], scope='conv3_3')\n",
    "net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "\n",
    "# 1. cleaner by repeat operation:\n",
    "net = tf.placeholder(shape=[3, 3, 3, 64], dtype='float32')\n",
    "net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3],\n",
    "scope='conv3')\n",
    "net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "\n",
    "# 2. Verbose way:\n",
    "x = slim.fully_connected(x, 32, scope='fc1/fc_1')\n",
    "x = slim.fully_connected(x, 64, scope='fc1/fc_2')\n",
    "x = slim.fully_connected(x, 128, scope='fc1/fc_3')\n",
    "\n",
    "# 2. Equivalent, TF-Slim way using slim.stack:\n",
    "slim.stack(x, slim.fully_connected, [32, 64, 128], scope='fc1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializer, Regularizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initializer:\n",
    "    + tf.truncated_normal_initializer (Included in tf)\n",
    "    + slim.xavier_initializer\n",
    "    + slim.variance_scaling_initializer\n",
    "\n",
    "- Regularizer:\n",
    "    + slim.l1_regularizer\n",
    "    + slim.l2_regularizer\n",
    "    + …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(shape=[3, 3, 3, 64], dtype='float32')\n",
    "net = slim.conv2d(inputs, 64, [11, 11], 4, padding='SAME', weights_initializer=slim.xavier_initializer(),weights_regularizer=slim.l2_regularizer(0.0005),scope='conv1-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### argscope\n",
    "\n",
    "\n",
    "- Eliminate boilerplate codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "he_init = slim.variance_scaling_initializer()\n",
    "xavier_init = slim.xavier_initializer()\n",
    "\n",
    "with slim.arg_scope([slim.conv2d, slim.fully_connected], activation_fn=tf.nn.relu, weights_initializer=he_init, weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "    with slim.arg_scope([slim.conv2d], stride=1, padding='SAME'):\n",
    "        net = slim.conv2d(inputs, 64, [11, 11], scope='conv1')\n",
    "        net = slim.conv2d(net, 256, [5, 5],weights_initializer=xavier_init,scope='conv2')\n",
    "        net = slim.fully_connected(net, 1000, activation_fn=None, scope='fc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Arguments are injected to [..]\n",
    "- Arguments are overwritten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses\n",
    "\n",
    "- Easy to handle multiple loss and regularize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-72120cab3924>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# Define the loss functions and get the total loss.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloss2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m# The following two lines have the same effect:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred1' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the loss functions and get the total loss.\n",
    "loss1 = slim.losses.softmax_cross_entropy(pred1, label1)\n",
    "loss2 = slim.losses.mean_squared_error(pred2, label2)\n",
    "# The following two lines have the same effect:\n",
    "total_loss = loss1 + loss2\n",
    "slim.losses.get_total_loss(add_regularization_losses=False)\n",
    "# If you want to add regularization loss\n",
    "reg_loss = tf.add_n(slim.losses.get_regularization_losses())\n",
    "total_loss = loss1 + loss2 + reg_loss\n",
    "# or\n",
    "total_loss = slim.losses.get_total_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection\n",
    "\n",
    "- Included in tf (not slim), anyway it's quite useful\n",
    "    - GLOBAL_VARIABLES\n",
    "    - TRAINABLE_VARIABLES\n",
    "    - SUMMARIES\n",
    "    - MOVING_AVERAGE_VARIABLES\n",
    "    - REGULARIZATION_LOSSES\n",
    "    - WEIGHTS\n",
    "    - BIASES\n",
    "    - ACTIVATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "\n",
    "def add_loss(loss, loss_collection=ops.GraphKeys.LOSSES):\n",
    "    if loss_collection:\n",
    "        ops.add_to_collection(loss_collection, loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add custom loss to LOSSES collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "\n",
    "def get_losses(scope=None, loss_collection=ops.GraphKeys.LOSSES):\n",
    "    return ops.get_collection(loss_collection, scope)\n",
    "\n",
    "def get_regularization_losses(scope=None):\n",
    "    return ops.get_collection(ops.GraphKeys.REGULARIZATION_LOSSES, scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy to get all losses with graph Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1/weights:0\n",
      "conv1/biases:0\n",
      "conv2/weights:0\n",
      "conv2/biases:0\n",
      "conv3/weights:0\n",
      "conv3/biases:0\n",
      "fc4/weights:0\n",
      "fc4/biases:0\n",
      "fc5/weights:0\n",
      "fc5/biases:0\n"
     ]
    }
   ],
   "source": [
    "for var in tf.trainable_variables():\n",
    "    print(var.name)\n",
    "    \n",
    "def trainable_variables():\n",
    "    return ops.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful to visualize or debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Restore\n",
    "\n",
    "- pages 71 ~ 74\n",
    "- My save wrapper function in my custom network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "def save(self, ckpt_dir, global_step=None):\n",
    "    if self.config.get(\"saver\") is None:\n",
    "        self.config[\"saver\"] = \\\n",
    "            tf.train.Saver(max_to_keep=30)     \n",
    "    \n",
    "    #Create saver to save a model\n",
    "    saver = self.config[\"saver\"]\n",
    "    sess = self.config[\"sess\"]\n",
    "    \n",
    "    dirname = os.path.join(ckpt_dir, self.name)\n",
    "\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    saver.save(sess, dirname, global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a model in dirname directory.\n",
    "global_step is for recording \"current step\" in filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- My restore wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_latest_checkpoint(self, ckpt_dir, exclude=None):\n",
    "    path = tf.train.latest_checkpoint(ckpt_dir)\n",
    "    if path is None:\n",
    "        raise AssertionError(\"No ckpt exists in {0}.\".format(ckpt_dir))\n",
    "    print(\"Load {} save file\".format(path))\n",
    "    self._load(path, exclude)\n",
    "def load_from_path(self, ckpt_path, exclude=None):\n",
    "    self._load(ckpt_path, exclude)\n",
    "def _load(self, ckpt_path, exclude):\n",
    "    init_fn = slim.assign_from_checkpoint_fn(ckpt_path,\n",
    "        slim.get_variables_to_restore(exclude=exclude),\n",
    "        ignore_missing_vars=True)\n",
    "    init_fn(self.config[\"sess\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude exclude variables when load a model\n",
    "If False, raise error when missing variables exist in checkpoint(save) file.\n",
    "When fine-tune model, have to set True to avoid error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_16(inputs,\n",
    "           num_classes=1000,\n",
    "           is_training=True,\n",
    "           dropout_keep_prob=0.5,\n",
    "           spatial_squeeze=True,\n",
    "           scope='vgg_16'):\n",
    "    with tf.variable_scope(scope, 'vgg_16', [inputs]) as sc:\n",
    "        end_points_collection = sc.name + '_end_points'\n",
    "        # collect outputs for conv2d, fully_connected, slim.max_pool2d],\n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n",
    "                           outputs_collections=end_points_collection):\n",
    "            net = slim.repeat(inputs, 2, slim.conv2d, 64, [3,3],scope='conv1')\n",
    "            net = slim.max_pool2d(net, [2,2], scope='pool1')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 128, [3,3], scope='conv2')\n",
    "            net = slim.max_pool2d(net, [2,2], scope='pool2')\n",
    "            net = slim.repeat(net, 3, slim.conv2d, 256, [3,3], scope='conv3')\n",
    "            net = slim.max_pool2d(net, [2,2], scope='pool3')\n",
    "            net = slim.repeat(net, 3, slim.conv2d, 512, [3,3], scope='conv4')            \n",
    "            net = slim.max_pool2d(net, [2,2], scope='pool4')\n",
    "            net = slim.repeat(net, 3, slim.conv2d, 512, [3,3], scope='conv5')            \n",
    "            net = slim.max_pool2d(net, [2,2], scope='pool5')\n",
    "            # Use conv2d instead of fully_connected layers.\n",
    "            net = slim.conv2d(net, 4096, [7,7], padding='VALID', scope='fc6')\n",
    "            net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                              scope='dropout6')\n",
    "            net = slim.conv2d(net, 4096, [1,1], scope='fc7')\n",
    "            net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                              scope='dropout7')\n",
    "            net = slim.conv2d(net, num_classes, [1,1],\n",
    "                             activation_fn=None,\n",
    "                             normalizer_fn=None,\n",
    "                             scope='fc8') \n",
    "            # Convert end_points_collection into a end_point dict.\n",
    "            end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "            if spatial_squeeze:\n",
    "                net = tf.squeeze(net, [1,2], name='fc8/squeezed')\n",
    "                end_points[sc.name + '/fc8'] = net\n",
    "            return net, end_points\n",
    "vgg_16.default_image_size = 224\n",
    "\n",
    "def vgg_arg_scope(weight_decay=0.0005):\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                       activation_fn=tf.nn.relu,\n",
    "                       weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "                       biases_initializer=tf.zeros_initializer):\n",
    "        with slim.arg_scope([slim.conv2d],padding='SAME') as arg_sc:\n",
    "            return arg_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tensorflow/models/blob/master/slim/nets/vgg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-59-2a7540ef17fa>:7: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "WARNING:tensorflow:From <ipython-input-59-2a7540ef17fa>:8: get_regularization_losses (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.get_regularization_losses instead.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'type_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-2a7540ef17fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcls_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mreg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_n\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_regularization_losses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreg_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"opt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'type_loss' is not defined"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 224, 224, 3], name=\"X\")\n",
    "y = tf.placeholder(tf.int32, [None, 1000], name=\"y\")\n",
    "is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "with slim.arg_scope(vgg_arg_scope()):\n",
    "    net, end_pts = vgg_16(X, is_training=is_training, num_classes=1000)\n",
    "with tf.variable_scope(\"losses\"):\n",
    "    cls_loss = slim.losses.softmax_cross_entropy(net, y)\n",
    "    reg_loss = tf.add_n(slim.losses.get_regularization_losses())\n",
    "    loss_op = type_loss + reg_loss\n",
    "with tf.variable_scope(\"opt\"):\n",
    "    opt = tf.train.AdamOptimizer(0.001).minimize(loss_op)\n",
    "self.load_from_path(ckpt_path=VGG_PATH, exclude=[\"vgg_16/fc8\"])\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "- Very useful visualization tool\n",
    "    + Graph visualization, loss/accuracy plot, weight histogram plot ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simple usage\n",
    "1. Make a tf.summary.FileWriter to write summaries\n",
    "2. Create summaries using tf.summary.###\n",
    "3. Run summaries with sess.run(...)\n",
    "4. Add summaries to FileWriter with add_summary API\n",
    "5. Run command\n",
    "   tensorboard -- logdir=## --host=## --port=##\n",
    "6. Default port is 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude fc8 layer to train last layer from scratch\n",
    "To freeze other layers, trainable=False argument is needed in argscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.summary.writer.writer.FileWriter"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.FileWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.ops.summary_ops.tensor_summary>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.tensor_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard & slim example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "\n",
    "max_steps = 10000\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "keep_prob = 0.5\n",
    "weight_decay = 0.0004\n",
    "logs_path = \"MNIST_data/tensorflow_logs/example\"\n",
    "\n",
    "def my_arg_scope(is_training, weight_decay):\n",
    "    with slim.arg_scope([slim.conv2d],\n",
    "        activation_fn=tf.nn.relu,\n",
    "        weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "        weights_initializer=slim.variance_scaling_initializer(),\n",
    "        biases_initializer=tf.zeros_initializer,\n",
    "        stride=1, padding=\"SAME\"):\n",
    "        with slim.arg_scope([slim.dropout],\n",
    "                            is_training=is_training) as arg_sc:\n",
    "            return  arg_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_net(x, keep_prob, outputs_collections=\"my_net\"):\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    with slim.arg_scope([slim.conv2d, slim.max_pool2d], outputs_collections=outputs_collections):\n",
    "        net = slim.conv2d(x, 64, [3, 3], scope=\"conv1\")\n",
    "        net = slim.max_pool2d(net, [2, 2], scope=\"pool1\")\n",
    "        net = slim.conv2d(net, 128, [3, 3], scope=\"conv2\")\n",
    "        net = slim.max_pool2d(net, [2, 2], scope=\"pool2\")\n",
    "        net = slim.conv2d(net, 256, [3, 3], scope=\"conv3\")\n",
    "        # global average pooling\n",
    "        net = tf.reduce_mean(net, [1, 2], name=\"pool3\", keep_dims=True)\n",
    "        net = slim.dropout(net, keep_prob, scope=\"dropout3\")\n",
    "        net = slim.conv2d(net, 1024, [1, 1], scope=\"fc4\")\n",
    "        net = slim.dropout(net, keep_prob, scope=\"dropout4\")\n",
    "        net = slim.conv2d(net, 10, [1, 1],\n",
    "                          activation_fn=None, scope=\"fc5\")\n",
    "    end_points = slim.utils.convert_collection_to_dict(outputs_collections)\n",
    "    return tf.reshape(net, [-1, 10]), end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather variables into end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-64-7a14222404b4>:10: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "WARNING:tensorflow:From <ipython-input-64-7a14222404b4>:11: get_regularization_losses (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.get_regularization_losses instead.\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "with slim.arg_scope(my_arg_scope(is_training, weight_decay)):\n",
    "    net, end_pts = my_net(x, keep_prob)\n",
    "    pred = slim.softmax(net, scope=\"prediction\")\n",
    "\n",
    "with tf.variable_scope(\"losses\"):\n",
    "    cls_loss = slim.losses.softmax_cross_entropy(net, y)\n",
    "    reg_loss = tf.add_n(slim.losses.get_regularization_losses())\n",
    "    loss_op = cls_loss + reg_loss\n",
    "    \n",
    "with tf.variable_scope(\"Adam\"):\n",
    "    opt = tf.train.AdamOptimizer(lr)\n",
    "    # Op to calculate every variable gradient\n",
    "    grads = tf.gradients(loss_op, tf.trainable_variables())\n",
    "    grads = list(zip(grads, tf.trainable_variables()))\n",
    "    # Op to update all variables according to their gradient\n",
    "    apply_grads = opt.apply_gradients(grads_and_vars=grads)\n",
    "    \n",
    "with tf.variable_scope(\"accuracy\"):\n",
    "    correct_op = tf.equal(tf.argmax(net, 1), tf.argmax(y, 1))\n",
    "    acc_op = tf.reduce_mean(tf.cast(correct_op, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gather grad to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a summary to monitor loss and accuracy\n",
    "summ_loss = tf.summary.scalar(\"loss\", loss_op)\n",
    "summ_acc = tf.summary.scalar(\"accuracy_test\", acc_op)\n",
    "\n",
    "# Create summaries to visualize weights and grads\n",
    "for var in tf.trainable_variables():\n",
    "    tf.summary.histogram(var.name, var, collections=[\"my_summ\"])\n",
    "for grad, var in grads:\n",
    "    tf.summary.histogram(var.name + \"/gradient\", grad,\n",
    "collections=[\"my_summ\"])\n",
    "\n",
    "summ_wg = tf.summary.merge_all(key=\"my_summ\")\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "summary_writer = tf.summary.FileWriter(logs_path,\n",
    "                                       graph=sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for step in range(max_steps):\n",
    "    batch_X, batch_y = mnist.train.next_batch(batch_size)\n",
    "    _, loss, plot_loss, plot_wg = sess.run([apply_grads, loss_op,\n",
    "                                           summ_loss, summ_wg],\n",
    "                                           feed_dict={x: batch_X, y: batch_y, is_training: True})\n",
    "\n",
    "    summary_writer.add_summary(plot_loss, step)\n",
    "    summary_writer.add_summary(plot_wg, step)\n",
    "\n",
    "    if (step+1) % 100 == 0:\n",
    "        plot_acc = sess.run(summ_acc, feed_dict={x: mnist.test.images,\n",
    "                                                 y: mnist.test.labels,\n",
    "                                                 is_training: False})\n",
    "        summary_writer.add_summary(plot_acc, step)\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "test_acc = sess.run(acc_op, feed_dict={x: mnist.test.images,\n",
    "                                       y: mnist.test.labels,\n",
    "                                       is_training: False})\n",
    "\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/you359/aiclass/master/1632036005_YOUNGJINKIM/files/graph%20visualization.JPG\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
