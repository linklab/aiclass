{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment01 - TensorFlow Tutorial\n",
    "__전기전자통신공학과 정보통신공학전공 1632061005 신예진__\n",
    "\n",
    "- TensorFlow 101에서 제공되는 슬라이드를 보면서 슬라이드 내에 소개되는 모든 코드를 본인 스스로 작성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy (p.5)\n",
    "- NumPy is the fundamental package for scientific computing with Python. (출처 : www.numpy.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # numpy의 모든 객체를 np.obj 형식으로 불러 사용할 수 있음\n",
    "\n",
    "a = np.zeros((2, 2)); # 0으로 채워진 2x2 크기의 배열 생성\n",
    "b = np.ones((2, 2)); # 1로 채워진 2x2 크기의 배열 생성\n",
    "\n",
    "np.sum(b, axis=1) # axis=1은 각 행에 대한 합계를 계산함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape # 각 차원의 크기를 알려줌. (2, 2)일 경우 2행 2열이라는 의미임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(a, (1, 4)) # 배열의 행수와 열수를 1행 4열으로 조절함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow - sess.run() (pp.6-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.Session() # 세션 생성\n",
    "\n",
    "a = tf.zeros((2, 2)) # 모든 원소의 값이 0인 텐서를 생성함\n",
    "b = tf.ones((2, 2)) # 모든 원소의 값이 1인 텐서를 생성함\n",
    "\n",
    "sess.run(tf.reduce_sum(b, axis=1)) # reduce_sum 함수를 통해 텐서의 모든 원소를 더함\n",
    "                                   # sess.run() 함수를 통해 파라미터로 전달된 내용을 실행함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(2)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_shape() # 텐서의 크기를 알려줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.reshape(a, (1, 4))) # 텐서의 원소는 유지하면서 텐서의 구조를 1행 4열으로 바꿈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session() # 다시 세션을 생성함\n",
    "a = np.zeros((2, 2)); # 0으로 채워진 2x2 크기의 배열 생성\n",
    "ta = tf.zeros((2, 2)); # 0으로 채워진 텐서 생성\n",
    "\n",
    "print(a) # 배열 a 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros_1:0\", shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(ta) # 텐서 ta 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(ta)) # 파라미터로 전달된 ta를 실행함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation Graph (pp.9-11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(5.0) # 파라미터로 전달된 값을 이용하여 상수 텐서 생성\n",
    "b = tf.constant(6.0)\n",
    "c = a * b # 텐서 a와 b를 곱함 --> computation graph 정의\n",
    "\n",
    "sess = tf.Session() # 그래프를 계산하기 위한 세션 생성\n",
    "print(sess.run(c)) # 파라미터로 전달된 c를 실행함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Session (pp.12-13)\n",
    "\n",
    "- 그래프를 실행하기 위해서는 세션 객체가 필요하며, 세션은 오퍼레이션의 실행 환경을 캡슐화한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n",
      "30.0\n",
      "30.0\n"
     ]
    }
   ],
   "source": [
    "# 세션을 생성하고 그래프를 실행하는 방법은 아래 두 가지가 있음\n",
    "sess = tf.Session() # 세션 생성 방법 1\n",
    "print(sess.run(c))\n",
    "\n",
    "with tf.Session() as sess: # 세션 생성 방법 2\n",
    "    print(sess.run(c))\n",
    "    print(c.eval()) # tf.Session() 범위 내에서만 동작함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Variables (pp.14-22)\n",
    "- TensorFlow에서 변수형은 그래프를 실행하기 전에 초기화를 해줘야 그 값이 변수에 지정이 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1827545   0.06516678]\n",
      " [-0.06095904 -0.05283963]\n",
      " [-0.06533372  0.04602138]\n",
      " [ 0.15323842  0.02323494]\n",
      " [ 0.16797489 -0.07754844]]\n"
     ]
    }
   ],
   "source": [
    "# Variable 함수를 사용하여 변수 생성\n",
    "w = tf.Variable(tf.random_normal([5, 2], stddev=0.1), name=\"weight\") # 정규 분포 형태의 난수 텐서를 생성함 (5행 2열, 표준편차 0.1)\n",
    "\n",
    "with tf.Session() as sess: # 세션 생성\n",
    "    sess.run(tf.global_variables_initializer()) # 변수들을 초기화한 후, 초기화된 결과를 세션에 전달함\n",
    "    print(sess.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Updating Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "state = tf.Variable(0, name=\"counter\") # 변수를 생성하고, 변수에 0을 저장함\n",
    "new_value = tf.add(state, tf.constant(1)) # state 변수에 상수 1을 더한 값을 new_value에 저장함\n",
    "update = tf.assign(state, new_value) # state 변수에 new_value의 값을 대입함\n",
    "\n",
    "with tf.Session() as sess: # 세션 생성\n",
    "    sess.run(tf.global_variables_initializer()) # 초기화\n",
    "    print(sess.run(state))\n",
    "    for _ in range(3) : # 3회 반복\n",
    "        sess.run(update) # 대입 연산\n",
    "        print(sess.run(state)) # 대입 연산 후의 state 변수의 값 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fetching Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.constant(1)\n",
    "x2 = tf.constant(2)\n",
    "x3 = tf.constant(3) # 상수 텐서 x1, x2, x3 생성\n",
    "temp = tf.add(x2, x3) # 텐서 x2, x3를 더한 결과 값을 temp에 저장\n",
    "mul = tf.multiply(x1, temp) # 텐서 x1, temp를 곱한 값을 mul에 저장\n",
    "\n",
    "\n",
    "with tf.Session() as sess: # 세션 생성\n",
    "    result1, result2 = sess.run([mul, temp]) # mul, temp 값을 각각 result1, result2에 저장\n",
    "    print(result1, result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__자료 내의 mul() 함수는 multiply()로 변경!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Placeholder (pp.24-26)\n",
    "- placeholder : 학습용 데이터를 담기 위한 심볼릭 변수. placeholder()를 이용해서 데이터형을 지정하고, 실제 데이터는 실행 단계에서 입력받는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.int16) # 2차원 텐서를 위한 정수형 변수 정의\n",
    "b = tf.placeholder(tf.int16)\n",
    "\n",
    "add = tf.add(a, b) # a와 b의 합\n",
    "mul = tf.multiply(a, b) # a와 b의 곱\n",
    "\n",
    "with tf.Session() as sess: # 세션 생성\n",
    "    print(sess.run(add, feed_dict={a: 2, b: 3})) # run 메소드에 feed_dict 파라미터 값으로 변수의 값을 전달\n",
    "    print(sess.run(mul, feed_dict={a: 2, b: 3})) # feed_dict에 의해서 전달된 값을 읽어서 실행함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n",
      "[[ 12.]]\n"
     ]
    }
   ],
   "source": [
    "# using tf.constant\n",
    "matrix1 = tf.constant([[3., 3.]]) # 상수 행렬 생성\n",
    "matrix2 = tf.constant([[2.], [2.]])\n",
    "product = tf.matmul(matrix1, matrix2) # 행렬 곱 계산 (그래프 정의)\n",
    "\n",
    "with tf.Session() as sess: # 세션 생성\n",
    "    result = sess.run(product) # 행렬 곱 실행\n",
    "    print(result)\n",
    "    \n",
    "# using placeholder\n",
    "import numpy as np\n",
    "\n",
    "matrix1 = tf.placeholder(tf.float32, [1, 2]) # 1행 2열의 실수형 매트릭스 정의\n",
    "matrix2 = tf.placeholder(tf.float32, [2, 1])\n",
    "product = tf.matmul(matrix1, matrix2) # 행렬 곱 계산 (그래프 정의)\n",
    "\n",
    "with tf.Session() as sess: # 세션 생성\n",
    "    mv1 = np.array([[3., 3.]]) # 행렬 정의\n",
    "    mv2 = np.array([[2.], [2.]])\n",
    "    result = sess.run(product, feed_dict={matrix1: mv1, matrix2: mv2}) # 변수 값 설정 및 행렬 곱 실행\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - MNIST with MLP (pp.27-31)\n",
    "- MNIST (Mixed National Institute of Standard and Technology) : 필기체 이미지 등이 포함된 데이터 셋\n",
    "- MLP (Multi-Layer Perceptron) : 여러 개의 perceptron layer들을 중첩시켜 만든 인공신경망 기계학습 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "[1000/15000] loss:20.085\n",
      "[2000/15000] loss:6.390\n",
      "[3000/15000] loss:5.294\n",
      "[4000/15000] loss:0.856\n",
      "[5000/15000] loss:0.861\n",
      "[6000/15000] loss:1.017\n",
      "[7000/15000] loss:0.933\n",
      "[8000/15000] loss:0.094\n",
      "[9000/15000] loss:0.315\n",
      "[10000/15000] loss:0.024\n",
      "[11000/15000] loss:0.000\n",
      "[12000/15000] loss:0.000\n",
      "[13000/15000] loss:0.000\n",
      "[14000/15000] loss:0.000\n",
      "[15000/15000] loss:0.000\n",
      "Optimaization Finished!\n",
      "Train accuracy: 0.998\n",
      "Test accuracy: 0.953\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True) # MNIST 데이터셋 불러오기\n",
    "\n",
    "learning_rate = 0.001\n",
    "max_steps = 15000\n",
    "batch_size = 128\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # 이미지 데이터 플레이스홀더\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "def MLP(inputs):\n",
    "    W_1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "    b_1 = tf.Variable(tf.zeros([256]))\n",
    "    \n",
    "    W_2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "    b_2 = tf.Variable(tf.zeros([256]))\n",
    "    \n",
    "    W_out = tf.Variable(tf.random_normal([256, 10]))\n",
    "    b_out = tf.Variable(tf.zeros([10]))\n",
    "    \n",
    "    h_1 = tf.add(tf.matmul(inputs, W_1), b_1)\n",
    "    h_1 = tf.nn.relu(h_1)\n",
    "    \n",
    "    h_2 = tf.add(tf.matmul(h_1, W_2), b_2)\n",
    "    h_2 = tf.nn.relu(h_2)\n",
    "    \n",
    "    out = tf.add(tf.matmul(h_2, W_out), b_out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "net = MLP(x)\n",
    "\n",
    "# define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, y)) # 정규화되지 않은 logit에 대한 cross-entropy 구현\n",
    "opt = tf.train.AdamOptimizer(learning_rate).minimize(loss_op)\n",
    "\n",
    "# initializing the variables\n",
    "init_op = tf.global_variables_initializer() # 위에서 작성한 변수들을 초기화함\n",
    "\n",
    "sess = tf.Session() # 세션에서 모델을 실행시킴\n",
    "sess.run(init_op)\n",
    "\n",
    "# train model\n",
    "for step in range(max_steps):\n",
    "    batch_X, batch_y = mnist.train.next_batch(batch_size)\n",
    "    _, loss = sess.run([opt, loss_op], feed_dict={x: batch_X, y: batch_y})\n",
    "    \n",
    "    if(step+1) % 1000 == 0:\n",
    "        print(\"[{}/{}] loss:{:.3f}\".format(step+1, max_steps, loss))\n",
    "print(\"Optimaization Finished!\")\n",
    "\n",
    "# test model\n",
    "correct_prediction = tf.equal(tf.argmax(net, 1), tf.argmax(y, 1)) # argmax는 텐서 안에서 특정 축을 따라 가장 큰 값의 인덱스를 찾음\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # 리스트를 부동소수점 값으로 변환한 후 평균을 계산 (정확도 계산을 위함)\n",
    "print(\"Train accuracy: {:.3f}\".format(sess.run(accuracy, feed_dict={x: mnist.train.images, y: mnist.train.labels})))\n",
    "print(\"Test accuracy: {:.3f}\".format(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Scope (pp.34-42)\n",
    "- TensorFlow의 Variable Scope 매커니즘은 두 개의 메인 함수로 반환함\n",
    " 1. tf.get_variable() : 입력된 이름의 변수를 생성하거나 반환함\n",
    " 2. tf.variable_scope() : tf.get_variable()에 전달된 이름의 네임스페이스를 관리함\n",
    "- (출처 : https://tensorflowkorea.gitbooks.io/tensorflow-kr/g3doc/how_tos/variable_scope/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1: var:0\n",
      "var2: foo/bar/var:0\n",
      "var3: foo/bar/var_1:0\n"
     ]
    }
   ],
   "source": [
    "# tf.variable_scope()\n",
    "\n",
    "var1 = tf.Variable([1], name=\"var\") # 변수 생성\n",
    "with tf.variable_scope(\"foo\"): # 변수 이름에 대한 접두사로 사용되는 이름을 들고 있음\n",
    "    with tf.variable_scope(\"bar\"):\n",
    "        var2 = tf.Variable([1], name=\"var\") # 제공된 shape와 데이터 타입을 가지는 새로운 텐서 변수\n",
    "        var3 = tf.Variable([1], name=\"var\") # 생성된 변수의 이름은 현재 변수 범위 이름 + 제공된 name\n",
    "        \n",
    "print (\"var1: {}\".format(var1.name))\n",
    "print (\"var2: {}\".format(var2.name))\n",
    "print (\"var3: {}\".format(var3.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1: var_1:0\n",
      "var2: foo_1/bar/var:0\n",
      "var3: foo_1/bar/var_1:0\n"
     ]
    }
   ],
   "source": [
    "# tf.get_variable()\n",
    "\n",
    "var1 = tf.Variable([1], name=\"var\") # 변수 생성\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    with tf.variable_scope(\"bar\") as scp:\n",
    "        var2 = tf.Variable([1], name=\"var\")\n",
    "        scp.reuse_variables() # allow reuse variables\n",
    "        var3 = tf.Variable([1], name=\"var\")\n",
    "        \n",
    "print (\"var1: {}\".format(var1.name))\n",
    "print (\"var2: {}\".format(var2.name))\n",
    "print (\"var3: {}\".format(var3.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1: foo/bar/var_2:0\n",
      "var2: foo/bar/var_2:0\n",
      "var3: foo/bar/var_2:0\n"
     ]
    }
   ],
   "source": [
    "# Parameter sharing\n",
    "\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    with tf.variable_scope(\"bar\") as scp:\n",
    "        var1 = tf.get_variable(\"var\", [1])\n",
    "        scp.reuse_variables() # allow reuse variables\n",
    "        var2 = tf.get_variable(\"var\", [1])\n",
    "        \n",
    "    with tf.variable_scope(\"bar\", reuse=True): \n",
    "        var3 = tf.get_variable(\"var\", [1])\n",
    "        \n",
    "print (\"var1: {}\".format(var1.name))\n",
    "print (\"var2: {}\".format(var2.name))\n",
    "print (\"var3: {}\".format(var3.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - MNIST with CNN (pp.43-51)\n",
    "- Convolution Neural Network (CNN) : 딥러닝의 유형 중 하나. 입력 데이터로 이미지를 받는 것으로 neural network를 효율적으로 구현할 수 있고, 필요한 parameter의 수를 줄일 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wrappers\n",
    "\n",
    "# Another init strategy called He init introduced in PReLUNet\n",
    "from tensorflow.contrib.layers import variance_scaling_initializer\n",
    "he_init = variance_scaling_initializer()\n",
    "\n",
    "def conv(bottom, num_filter, ksize=3, stride=1, padding=\"SAME\", scope=None):\n",
    "    bottom_shape = bottom.get_shape().as_list()[3] # To calculate shape of previous (input) layer\n",
    "    \n",
    "    with tf.variable_scope(scope or \"conv\"):\n",
    "        W = tf.get_variable(\"W\", [ksize, ksize, bottom_shape, num_filter], initializer=he_init)\n",
    "        b = tf.get_variable(\"b\", [num_filter], initializer=tf.constant_initializer(0))\n",
    "        x = tf.nn.conv2d(bottom, W, strides=[1, stride, stride, 1], padding=padding)\n",
    "        x = tf.nn.relu(tf.nn.bias_add(x, b)) # convolution layer를 적용한 결과를 ReLU에 전달\n",
    "        \n",
    "    return x\n",
    "\n",
    "# pooling (sampling) : 여러 개 중에서 하나를 선택\n",
    "def maxpool(bottom, ksize=2, stride=2, padding=\"SAME\", scope=None):\n",
    "    with tf.variable_scope(scope or \"maxpool\"):\n",
    "        pool = tf.nn.max_pool(bottom, ksize=[1, ksize, ksize, 1], strides=[1, stride, stride, 1], padding=padding)\n",
    "    \n",
    "    return pool\n",
    "\n",
    "def fc(bottom, num_dims, scope=None):\n",
    "    bottom_shape = bottom.get_shape().as_list()\n",
    "    if len(bottom_shape) > 2:\n",
    "        bottom = tf.reshape(bottom, [-1, reduce(lambda x, y: x*y, bottom_shape[1:])])\n",
    "        bottom_shape = bottom.get_shape().as_list()\n",
    "        \n",
    "    with tf.variable_scope(scope or \"fc\"):\n",
    "        W = tf.get_variable(\"W\", [bottom_shape[1], num_dims], initializer=he_init)\n",
    "        b = tf.get_variable(\"b\", [num_dims], initializer=tf.constant_initializer(0))\n",
    "        out = tf.nn.bias_add(tf.matmul(bottom, W), b)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def fc_relu(bottom, num_dims, scope=None):\n",
    "    with tf.variable_scope(scope or \"fc\"):\n",
    "        out = fc(bottom, num_dims, scope=\"fc\")\n",
    "        relu = tf.nn.relu(out)\n",
    "        \n",
    "    return relu\n",
    "\n",
    "# All Together\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, None)\n",
    "\n",
    "def conv_net(x, keep_prob):\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1]) # 변수의 형태를 4차원으로 변환. -1은 갯수가 정해지지 않아서 모를 경우에 사용함\n",
    "    \n",
    "    conv1 = conv(x, 32, 5, scope=\"conv_1\")\n",
    "    conv1 = maxpool(conv1, scope=\"maxpool_1\") \n",
    "    conv2 = conv(conv1, 64, 5, scope=\"conv_2\")\n",
    "    conv2 = maxpool(conv2, scope=\"maxpool_2\")\n",
    "    \n",
    "    fc1 = fc_relu(conv2, 1024, scope=\"fc_1\")\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    out = fc(fc1, 10, scope=\"out\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers (pp.56-59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\")\n",
    "input = tf.reshape(x, [3, 3, 3, 64]) # 적절한 4D 텐서 정의\n",
    "\n",
    "with tf.name_scope('conv1_1') as scope:\n",
    "    kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32, stddev=1e-1), name='weights')\n",
    "    conv = tf.nn.conv2d(x, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32), trainable=True, name='biases')\n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    "    conv1 = tf.nn.relu(bias, name=scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In TF-Slim code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "net = slim.conv2d(input, 128, [3, 3], padding='SAME', scope='conv1_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\")\n",
    "\n",
    "# 1. simple network generation with slim\n",
    "net = tf.reshape(x, [3, 3, 3, 64]) # 적절한 4D 텐서 정의\n",
    "\n",
    "net = slim.conv2d(net, 256, [3, 3], scope='conv3_1')\n",
    "net = slim.conv2d(net, 256, [3, 3], scope='conv3_2')\n",
    "net = slim.conv2d(net, 256, [3, 3], scope='conv3_3')\n",
    "net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "\n",
    "# 1. cleaner by repeat operation:\n",
    "net = tf.reshape(x, [3, 3, 3, 64])\n",
    "net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "net = slim.max_pool(net, [2, 2], scope='pool3')\n",
    "\n",
    "# 2. Verbose way:\n",
    "x = slim.fully_connected(x, 32, scope='fc/fc_1')\n",
    "x = slim.fully_connected(x, 64, scope='fc/fc_2')\n",
    "x = slim.fully_connected(x, 128, scope='fc/fc_3')\n",
    "\n",
    "# 2. Equivalent, TF-Slim way using slim.stack:\n",
    "slim.stack(x, slim.fully_connected, [32, 64, 128], scope='fc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### argscope (pp.61-63)\n",
    "- provides a new scope named 'arg_scope' that allows a user to define default arguments for specific operations within that scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "he_init = slim.variance_scaling_initializer()\n",
    "xavier_init = slim.xavier_initializer()\n",
    "\n",
    "with slim.arg_scope([slim.conv2d, slim.fully_connected], activation_fn=tf.nn.relu, \n",
    "                     weights_initializer=he_init, weights_regularizer=slim.l2_regularizer(0.0005)): # Arguments are injected to [..]\n",
    "    with slim.arg_scope([slim.conv2d], stride=1, padding='SAME'):\n",
    "        net = slim.conv2d(inputs, 64, [11, 11], 4, scope='conv1')\n",
    "        net = slim.conv2d(net, 256, [5, 5], weights_initializer=xavier_init, scope='conv2')\n",
    "        net = slim.fully_connected(net, 1000, activation_fn=None, scope='fc') # arguments are overwritten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses (p.64)\n",
    "- Easy to handle multiple loss and regularize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the loss functions and get the total loss\n",
    "loss1 = slim.losses.softmax_cross_entropy(pred1, lable1)\n",
    "loss2 = slim.losses.mean_squared_errer(perd2, label2)\n",
    "\n",
    "# The following two lines have the same effect:\n",
    "total_loss = loss1 + loss2\n",
    "slim.losses.get_total_loss(add_regularization_losses=False)\n",
    "\n",
    "# If you want to add regularization loss\n",
    "reg_loss = tf.add_n(slim.losses.get_regularization_losses())\n",
    "total_loss = loss1 + loss2 + reg_loss\n",
    "# or\n",
    "total_loss = slim.losses.get_total_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection (pp.65-69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:0\n",
      "Variable_1:0\n",
      "Variable_2:0\n",
      "Variable_3:0\n",
      "Variable_4:0\n",
      "Variable_5:0\n",
      "Variable_6:0\n",
      "Variable_7:0\n",
      "Variable_8:0\n",
      "Variable_9:0\n",
      "Variable_10:0\n",
      "Variable_11:0\n",
      "Variable_12:0\n",
      "Variable_13:0\n",
      "Variable_14:0\n",
      "Variable_15:0\n",
      "Variable_16:0\n",
      "Variable_17:0\n",
      "Variable_18:0\n",
      "Variable_19:0\n",
      "Variable_20:0\n",
      "Variable_21:0\n",
      "Variable_22:0\n",
      "Variable_23:0\n",
      "Variable_24:0\n",
      "Variable_25:0\n",
      "Variable_26:0\n",
      "Variable_27:0\n",
      "Variable_28:0\n",
      "Variable_29:0\n",
      "var:0\n",
      "foo/bar/var:0\n",
      "foo/bar/var_1:0\n",
      "var_1:0\n",
      "foo_1/bar/var:0\n",
      "foo_1/bar/var_1:0\n",
      "foo_2/bar/var:0\n",
      "foo_2/bar/var_1:0\n",
      "foo_3/bar/var:0\n",
      "foo_3/bar/var_1:0\n",
      "foo/bar/var_2:0\n",
      "conv1_1/weights:0\n",
      "conv1_1_1/weights:0\n",
      "conv1_1_2/weights:0\n",
      "conv1_1_3/weights:0\n",
      "conv1_1_6/weights:0\n",
      "conv1_1_7/weights:0\n",
      "conv1_1_8/weights:0\n",
      "conv1_1_9/weights:0\n",
      "conv1_1_9/biases:0\n",
      "conv1_1/weights_1:0\n",
      "conv1_1/biases:0\n",
      "conv3_1/weights:0\n",
      "conv3_1/biases:0\n",
      "conv2/weights:0\n",
      "conv2/biases:0\n",
      "fc/weights:0\n",
      "fc/biases:0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "\n",
    "# Add custom loss to LOSSES collection\n",
    "def add_loss(loss, loss_collection=ops.GraphKeys.LOSSES):\n",
    "    if loss_collection:\n",
    "        ops.add_to_collection(loss_collection, loss)\n",
    "\n",
    "# Easy to get all losses with graph Collection\n",
    "def get_losses(scope=None, loss_collection=ops.GraphKeys.LOSSES):\n",
    "    return ops.get_collection(loss_collection, scope)\n",
    "\n",
    "def get_regularization_loss(scope=None):\n",
    "    return ops.get_collection(ops.GraphKeys.REGULARIZATION_LOSSES, scope)\n",
    "\n",
    "# Useful to visualize or debugging\n",
    "for var in tf.trainable_variables():\n",
    "    print(var.name)\n",
    "    \n",
    "def trainable_variables():\n",
    "    return ops.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Restore (pp.70-75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# My save wrapper function in my custom network class\n",
    "def save(self, ckpt_dir, global_step=None): # create saver to save a model\n",
    "    if self.config.get(\"saver\") is None:\n",
    "        self.config[\"saver\"] = \\\n",
    "            tf.train.Saver(max_to_keep=30)\n",
    "            \n",
    "    saver = self.config[\"saver\"]\n",
    "    sess = self.config[\"sess\"]\n",
    "    \n",
    "    dirname = os.path.join(ckpt_dir, self.name)\n",
    "    \n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    saver.save(sess, dirname, global_step)\n",
    "    # save a model in dirname directory. global_step is for recording \"current step\" in filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# My restore wrapper function\n",
    "def load_latest_checkpoint(self, ckpt_dir, exclude=None):\n",
    "    path = tf.train.latest_checkpoint(ckpt_dir)\n",
    "    if path is None:\n",
    "        raise AssertionError(\"No ckpt exists in {0}.\".format(ckpt_dir))\n",
    "        \n",
    "    print (\"Load {} save file\".format(path))\n",
    "    self._load(path, exclude)\n",
    "\n",
    "def load_from_path(self, ckpt_path, exclude=None):\n",
    "    self._load(ckpt_path, exclude)\n",
    "    \n",
    "# Exclude 'exclude' variables when load a model\n",
    "def _load(self, ckpt_path, exclude):\n",
    "    init_fn = slim.assign_from_checkpoint_fn(ckpt_path, slim.get_variables_to_restore(exclude=exclude), ignore_missing_vars=True)\n",
    "    # if 'False', raise error when missing variables exist in checkpoint (save) file. When 'fine-tune' model, have to set 'True' to avoid error\n",
    "    init_fn(self.config[\"sess\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16 Build network (pp.76-79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_16(inputs, num_classes=1000, is_training=True, dropout_keep_prob=0.5, spatial_squeeze=True, scope='vgg_16'):\n",
    "    with tf.variable_scope(scope, 'vgg_16', [inputs]) as sc:\n",
    "        end_points_collection = sc.name + '_end_points'\n",
    "        # Collect outputs for conv2d, fully_connected and max_pool2d\n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d], outputs_collections=end_points_collection):\n",
    "            net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "            net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "            net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "            net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "            # Use conv2d instead of fully_connected layers.\n",
    "            net = slim.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')\n",
    "            net = slim.dropout(net, dropout_keep_prob, is_training=is_training, scope='dropout6')\n",
    "            net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "            net = slim.dropout(net, dropout_keep_prob, is_training=is_training, scope='dropout7')\n",
    "            net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, normalizer_fn=None, scope='fc8')\n",
    "            \n",
    "            # Convert end_points_collection into a end_point dict.\n",
    "            end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "            if spatial_squeeze:\n",
    "                net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\n",
    "                end_points[sc.name + '/fc8'] = net\n",
    "            return net, end_points\n",
    "vgg_16.default_image_size = 224\n",
    "\n",
    "# VGGNET argscope\n",
    "def vgg_arg_scope(weight_decay=0.0005):\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected], activation_fn=tf.nn.relu, \n",
    "                         weights_regularizer=slim.l2_regularizer(weight_decay), biases_initializer=tf.zeros_initializer):\n",
    "        with slim.arg_scope([slim.conv2d], padding='SAME') as arg_sc:\n",
    "            return arg_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# VGG-16\n",
    "X = tf.placeholder(tf.float32, [None, 224, 224, 3], name=\"X\")\n",
    "y = tf.placeholder(tf.int32, [None, 8], name=\"y\")\n",
    "is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "\n",
    "with slim.arg_scope(vgg_arg_scope()):\n",
    "    net, end_pts = vgg_16(X, is_training=is_training, num_classes=1000)\n",
    "    \n",
    "with tf.variable_scope(\"losses\"):\n",
    "    cls_loss = slim.losses.softmax_cross_entropy(net, y)\n",
    "    reg_loss = tf.add_n(slim.losses.get_regularization_losses())\n",
    "    loss_op = type_loss + reg_loss\n",
    "    \n",
    "with tf.variable_scope(\"opt\"):\n",
    "    opt = tf.train.AdamOptimizer(0.001).minimize(loss_ip)\n",
    "    \n",
    "self.load_from_path(ckpt_path=VGG_PATH, exclude=[\"vgg_16/fc8\"])\n",
    "# Exclude 'fc8' layer to train last layer from scratch\n",
    "# To freeze other layers, 'trainable=False' argument is needed in argscope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard & slim example (pp.80-88)\n",
    "- 매우 유용한 시각화 툴 (graph visualization, loss/accuracy plot, weight histogram plot ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "# import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "max_steps = 10000\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "keep_prob = 0.5\n",
    "weight_decay = 0.0004\n",
    "logs_path = \"/tmp/tensorflow_logs/example\"\n",
    "\n",
    "def my_arg_scope(is_training, weight_decay):\n",
    "    with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu, weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "                        weights_initializer=slim.variance_scaling_initializer(), biases_initializer=tf.zeros_initializer,\n",
    "                        stride=1, padding=\"SAME\"):\n",
    "        with slim.arg_scope([slim.dropout], is_training=is_training) as arg_sc:\n",
    "            return arg_sc\n",
    "        \n",
    "def my_net(x, keep_prob, outputs_collections=\"my_net\"):\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    with slim.arg_scope([slim.conv2d, slim.max_pool2d], outputs_collections=outputs_collections):\n",
    "        net = slim.conv2d(x, 64, [3, 3], scope=\"conv1\")\n",
    "        net = slim.max_pool2d(net, [2, 2], scope=\"pool1\")\n",
    "        net = slim.conv2d(net, 128, [3, 3], scope=\"conv2\")\n",
    "        net = slim.max_pool2d(net, [2, 2], scope=\"pool2\")\n",
    "        net = slim.conv2d(net, 256, [3, 3], scope=\"conv3\")\n",
    "        # global average pooling\n",
    "        net = tf.reduce_mean(net, [1, 2], name=\"pool3\", keep_dims=True)\n",
    "        net = slim.dropout(net, keep_prob, scope=\"dropout3\")\n",
    "        net = slim.conv2d(net, 1024, [1, 1], scope=\"fc4\")\n",
    "        net = slim.dropout(net, keep_prob, scope=\"dropout4\")\n",
    "        net = slim.conv2d(net, 10, [1, 1], activation_fn=None, scope=\"fc5\")\n",
    "        \n",
    "     # Gather variables into end_points\n",
    "    end_points = \\\n",
    "        slim.utils.convert_collection_to_dict(outputs_collections)\n",
    "    return tf.reshape(net, [-1, 10], end_points)\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "with slim.arg_scope(my_arg_scope(is_training, weight_decay)):\n",
    "    net, end_pts = my_net(x, keep_prob)\n",
    "    pred = slim.softmax(net, scope=\"prediction\")\n",
    "    \n",
    "with tf.variable_scope(\"losses\"):\n",
    "    cls_loss = slim.losses.softmax_cross_entropy(net, y)\n",
    "    reg_loss = tf.add_n(slim.losses.get_regularization_losses())\n",
    "    loss_op = cls_loss + reg_loss\n",
    "    \n",
    "with tf.variable_scope(\"Adam\"): # To gather grad to visualize\n",
    "    opt = tf.train.AdamOptimizer(lr)\n",
    "    # Op to calculate every variable gradient\n",
    "    grads = tf.gradients(loss_op, tf.trainable_variables())\n",
    "    grads = list(zip(grads, tf.trainable_variables()))\n",
    "    # Op to update all variables according to their gradient\n",
    "    apply_grads = opt.apply_gradients(grads_and_vars=grads)\n",
    "    \n",
    "with tf.variable_scope(\"accuracy\"):\n",
    "    correct_op = tf.equal(tf.argmax(net, 1), tf.argmax(y, 1))\n",
    "    acc_op = tf.reduce_mean(tf.cast(correct_op, tf.float32))\n",
    "    \n",
    "# Create a summary to monitor loss and accuracy\n",
    "summ_loss = tf.summary.scalar(\"loss\", loss_op)\n",
    "summ_acc = tf.summary.scalar(\"accuracy_test\", acc_op)\n",
    "\n",
    "# Create summaries to visualize weights and grads\n",
    "for var in tf.trainable_variables():\n",
    "    tf.summary.histogram(var.name, var, collections=[\"my_summ\"])\n",
    "for grad, var in grads:\n",
    "    tf.summary.histogram(var.name + \"/gradient\", grad, collections=[\"my_summ\"])\n",
    "    \n",
    "summ_wg = tf.summary.merge_all(key=\"my_summ\")\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=sess.graph)\n",
    "\n",
    "for step in range(max_steps):\n",
    "    batch_X, batch_y = mnist.train.next_batch(batch_size)\n",
    "    _, loss, plot_loss, plot_wg = sess.run([apply_grads, loss_op, summ_loss, summ_wg], \n",
    "                                           feed_dict={x: batch_X, y: batch_y, is_training: True})\n",
    "    summary_writer.add_summary(plot_loss, step)\n",
    "    summary_writer.add_summary(plot_wg, step)\n",
    "    \n",
    "    if (step+1) % 100 == 0:\n",
    "        plot_acc = sess.run(summ_acc, feed_dict{x: mnist.test.images, y: mnist.test.labels, is_training: False})\n",
    "        summary_writer.add_summary(plot_acc, step)\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "test_acc = sess.run(acc_op, feed_dict={x: mnist.test.images, y: mnist.test.labels, is_training: False})\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
