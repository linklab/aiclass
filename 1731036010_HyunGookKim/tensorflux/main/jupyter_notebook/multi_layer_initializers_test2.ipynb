{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflux'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ef8638a332a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/Users/yhhan/git/aiclass/0.Professor/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeep_learning_networks\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menums\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflux'"
     ]
    }
   ],
   "source": [
    "# pip install numba --upgrade\n",
    "# pip install pygraphviz\n",
    "import sys\n",
    "sys.path.insert(0, \"/Users/yhhan/git/aiclass/0.Professor/\")\n",
    "\n",
    "import tensorflux.graph as tfg\n",
    "import tensorflux.deep_learning_networks as tfn\n",
    "import tensorflux.enums as tfe\n",
    "import datasource.mnist as mnist\n",
    "import tensorflux.functions as tff\n",
    "import math\n",
    "import numba\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(numba.__version__)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "max_epoch = 10\n",
    "input_size = 784\n",
    "hidden_layer1_size = 128\n",
    "hidden_layer2_size = 128\n",
    "output_size = 10\n",
    "\n",
    "x = tfg.Placeholder(name=\"x\")\n",
    "target = tfg.Placeholder(name=\"target\")\n",
    "\n",
    "data = mnist.MNIST_Data(validation_size=5000, n_splits=12, is_onehot_target=True)\n",
    "#data = mnist.Fashion_MNIST_Data(validation_size=5000, n_splits=12, is_onehot_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_xavier_normal = tfn.Multi_Layer_Network(\n",
    "    input_size=input_size,\n",
    "    hidden_size_list=[hidden_layer1_size, hidden_layer2_size],\n",
    "    output_size=output_size,\n",
    "    input_node=x,\n",
    "    target_node=target,\n",
    "    initializer=tfe.Initializer.Xavier_Normal.value,\n",
    "    activator=tfe.Activator.ReLU.value,\n",
    "    optimizer=tfe.Optimizer.Adam.value,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "n_xavier_normal.learning(\n",
    "    max_epoch=max_epoch, \n",
    "    data=data, \n",
    "    batch_size=batch_size, \n",
    "    print_period=5, \n",
    "    is_numba=True, \n",
    "    verbose=False\n",
    ")\n",
    "data.reset_kfold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_xavier_uniform = tfn.Multi_Layer_Network(\n",
    "    input_size=input_size,\n",
    "    hidden_size_list=[hidden_layer1_size, hidden_layer2_size],\n",
    "    output_size=output_size,\n",
    "    input_node=x,\n",
    "    target_node=target,\n",
    "    initializer=tfe.Initializer.Xavier_Uniform.value,\n",
    "    activator=tfe.Activator.ReLU.value,\n",
    "    optimizer=tfe.Optimizer.Adam.value,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "n_xavier_uniform.learning(\n",
    "    max_epoch=max_epoch, \n",
    "    data=data, \n",
    "    batch_size=batch_size, \n",
    "    print_period=5, \n",
    "    is_numba=True, \n",
    "    verbose=False\n",
    ")\n",
    "data.reset_kfold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_he_normal = tfn.Multi_Layer_Network(\n",
    "    input_size=input_size,\n",
    "    hidden_size_list=[hidden_layer1_size, hidden_layer2_size],\n",
    "    output_size=output_size,\n",
    "    input_node=x,\n",
    "    target_node=target,\n",
    "    initializer=tfe.Initializer.He_Normal.value,\n",
    "    activator=tfe.Activator.ReLU.value,\n",
    "    optimizer=tfe.Optimizer.Adam.value,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "n_he_normal.learning(\n",
    "    max_epoch=max_epoch, \n",
    "    data=data, \n",
    "    batch_size=batch_size, \n",
    "    print_period=5, \n",
    "    is_numba=True, \n",
    "    verbose=False\n",
    ")\n",
    "data.reset_kfold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_he_uniform = tfn.Multi_Layer_Network(\n",
    "    input_size=input_size,\n",
    "    hidden_size_list=[hidden_layer1_size, hidden_layer2_size],\n",
    "    output_size=output_size,\n",
    "    input_node=x,\n",
    "    target_node=target,\n",
    "    initializer=tfe.Initializer.He_Uniform.value,\n",
    "    activator=tfe.Activator.ReLU.value,\n",
    "    optimizer=tfe.Optimizer.Adam.value,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "n_he_uniform.learning(\n",
    "    max_epoch=max_epoch, \n",
    "    data=data, \n",
    "    batch_size=batch_size, \n",
    "    print_period=5, \n",
    "    is_numba=True, \n",
    "    verbose=False\n",
    ")\n",
    "data.reset_kfold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_networks = {\n",
    "    \"xavier_normal\": n_xavier_normal, \n",
    "    \"xavier_uniform\": n_xavier_uniform,     \n",
    "    \"he_normal\": n_he_normal,         \n",
    "    \"he_uniform\": n_he_uniform\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, neural_network in neural_networks.items():\n",
    "    print(\"{:10s} - Epoch:{:3d}, Min Train Error:{:7.5f}, Min Validation Error:{:7.5f}, Test Accuracy:{:7.5f}\".format(\n",
    "        key, \n",
    "        neural_network.min_validation_error_epoch,\n",
    "        neural_network.min_train_error,\n",
    "        neural_network.min_validation_error,\n",
    "        neural_network.test_accuracy_at_min_validation_error_epoch\n",
    "    ))\n",
    "\n",
    "print()\n",
    "    \n",
    "for key, neural_network in neural_networks.items():    \n",
    "    print(\"{:10s} - Mean of Min Va. Error For All Folds:{:7.5f}\".format(\n",
    "        key,         \n",
    "        neural_network.mean_min_validation_error_for_all_folds\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = {\n",
    "    \"xavier_normal\": \"+\", \n",
    "    \"xavier_uniform\": \"d\",     \n",
    "    \"he_normal\": \"*\",         \n",
    "    \"he_uniform\": \"o\"\n",
    "}\n",
    "\n",
    "epoch_list = np.arange(len(neural_networks[\"xavier_normal\"].train_error_list))\n",
    "\n",
    "f, axarr = plt.subplots(2, 2, figsize=(15,10))\n",
    "\n",
    "for key, neural_network in neural_networks.items():\n",
    "    axarr[0, 0].plot(epoch_list, neural_network.train_error_list, marker=markers[key], markevery=2, label=key)\n",
    "axarr[0, 0].set_ylabel('Train Error')\n",
    "axarr[0, 0].set_xlabel('Epochs')\n",
    "axarr[0, 0].grid(True)\n",
    "axarr[0, 0].set_title('Train Error')\n",
    "axarr[0, 0].legend(loc='upper left')\n",
    "\n",
    "for key, neural_network in neural_networks.items():\n",
    "    axarr[0, 1].plot(epoch_list, neural_network.validation_error_list, marker=markers[key], markevery=2, label=key)\n",
    "axarr[0, 1].set_ylabel('Validation Error')\n",
    "axarr[0, 1].set_xlabel('Epochs')\n",
    "axarr[0, 1].grid(True)\n",
    "axarr[0, 1].set_title('Validation Error')\n",
    "axarr[0, 1].legend(loc='upper left')\n",
    "\n",
    "for key, neural_network in neural_networks.items():\n",
    "    axarr[1, 0].plot(epoch_list, neural_network.train_error_list, marker=markers[key], markevery=2, label=key)\n",
    "axarr[1, 0].set_ylabel('Train Error')\n",
    "axarr[1, 0].set_xlabel('Epochs')\n",
    "axarr[1, 0].grid(True)\n",
    "axarr[1, 0].set_ylim(0, 0.15)\n",
    "axarr[1, 0].set_title('Train Error (0.00 ~ 0.15)')\n",
    "axarr[1, 0].legend(loc='upper left')\n",
    "\n",
    "for key, neural_network in neural_networks.items():\n",
    "    axarr[1, 1].plot(epoch_list, neural_network.validation_error_list, marker=markers[key], markevery=2, label=key)\n",
    "axarr[1, 1].set_ylabel('Validation Error')\n",
    "axarr[1, 1].set_xlabel('Epochs')\n",
    "axarr[1, 1].grid(True)\n",
    "axarr[1, 1].set_ylim(0, 0.15)\n",
    "axarr[1, 1].set_title('Validation Error (0.00 ~ 0.15)')\n",
    "axarr[1, 1].legend(loc='upper left')\n",
    "\n",
    "f.subplots_adjust(hspace=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(2, 1, figsize=(15,10))\n",
    "\n",
    "for key, neural_network in neural_networks.items():\n",
    "    axarr[0].plot(epoch_list, neural_network.test_accuracy_list, marker=markers[key], markevery=1, label=key)\n",
    "axarr[0].set_ylabel('Test Accuracy')\n",
    "axarr[0].set_xlabel('Epochs')\n",
    "axarr[0].grid(True)\n",
    "axarr[0].set_title('Test Accuracy')\n",
    "axarr[0].legend(loc='upper left')\n",
    "\n",
    "for key, neural_network in neural_networks.items():\n",
    "    axarr[1].plot(epoch_list, neural_network.test_accuracy_list, marker=markers[key], markevery=1, label=key)\n",
    "axarr[1].set_ylabel('Test Accuracy')\n",
    "axarr[1].set_xlabel('Epochs')\n",
    "axarr[1].grid(True)\n",
    "axarr[1].set_ylim(0.95, 0.99)\n",
    "axarr[1].set_title('Test Accuracy (0.95 ~ 0.99)')\n",
    "axarr[1].legend(loc='upper left')\n",
    "\n",
    "f.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_networks[\"xavier_normal\"].draw_param_description(figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_networks[\"xavier_normal\"].draw_output_description(figsize=(20, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
