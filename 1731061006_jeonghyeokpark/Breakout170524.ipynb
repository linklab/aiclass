{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-28 22:15:42,116] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, Epsilon: 0.495, Reward: 2.0\n",
      "episode: 1, Epsilon: 0.49005, Reward: 2.0\n",
      "episode: 2, Epsilon: 0.48514949999999996, Reward: 2.0\n",
      "episode: 3, Epsilon: 0.480298005, Reward: 2.0\n",
      "episode: 4, Epsilon: 0.47549502494999996, Reward: 3.0\n",
      "episode: 5, Epsilon: 0.47074007470049994, Reward: 2.0\n",
      "episode: 6, Epsilon: 0.46603267395349496, Reward: 2.0\n",
      "episode: 7, Epsilon: 0.46137234721396, Reward: 3.0\n",
      "episode: 8, Epsilon: 0.45675862374182036, Reward: 2.0\n",
      "episode: 9, Epsilon: 0.45219103750440215, Reward: 3.0\n",
      "episode: 10, Epsilon: 0.44766912712935814, Reward: 3.0\n",
      "episode: 11, Epsilon: 0.44319243585806456, Reward: 2.0\n",
      "episode: 12, Epsilon: 0.4387605114994839, Reward: 0.0\n",
      "episode: 13, Epsilon: 0.43437290638448905, Reward: 2.0\n",
      "episode: 14, Epsilon: 0.43002917732064416, Reward: 2.0\n",
      "episode: 15, Epsilon: 0.4257288855474377, Reward: 2.0\n",
      "episode: 16, Epsilon: 0.4214715966919633, Reward: 0.0\n",
      "episode: 17, Epsilon: 0.4172568807250437, Reward: 0.0\n",
      "episode: 18, Epsilon: 0.41308431191779327, Reward: 2.0\n",
      "episode: 19, Epsilon: 0.4089534687986153, Reward: 3.0\n",
      "episode: 20, Epsilon: 0.40486393411062915, Reward: 2.0\n",
      "episode: 21, Epsilon: 0.4008152947695229, Reward: 0.0\n",
      "episode: 22, Epsilon: 0.39680714182182764, Reward: 2.0\n",
      "episode: 23, Epsilon: 0.3928390704036094, Reward: 2.0\n",
      "episode: 24, Epsilon: 0.38891067969957327, Reward: 5.0\n",
      "episode: 25, Epsilon: 0.38502157290257755, Reward: 2.0\n",
      "episode: 26, Epsilon: 0.3811713571735518, Reward: 0.0\n",
      "episode: 27, Epsilon: 0.37735964360181623, Reward: 2.0\n",
      "episode: 28, Epsilon: 0.3735860471657981, Reward: 0.0\n",
      "episode: 29, Epsilon: 0.3698501866941401, Reward: 2.0\n",
      "episode: 30, Epsilon: 0.3661516848271987, Reward: 2.0\n",
      "episode: 31, Epsilon: 0.3624901679789267, Reward: 3.0\n",
      "episode: 32, Epsilon: 0.3588652662991374, Reward: 3.0\n",
      "episode: 33, Epsilon: 0.35527661363614604, Reward: 3.0\n",
      "episode: 34, Epsilon: 0.3517238474997846, Reward: 2.0\n",
      "episode: 35, Epsilon: 0.34820660902478673, Reward: 4.0\n",
      "episode: 36, Epsilon: 0.34472454293453886, Reward: 3.0\n",
      "episode: 37, Epsilon: 0.3412772975051935, Reward: 2.0\n",
      "episode: 38, Epsilon: 0.33786452453014154, Reward: 0.0\n",
      "episode: 39, Epsilon: 0.33448587928484014, Reward: 3.0\n",
      "episode: 40, Epsilon: 0.33114102049199173, Reward: 2.0\n",
      "episode: 41, Epsilon: 0.3278296102870718, Reward: 0.0\n",
      "episode: 42, Epsilon: 0.3245513141842011, Reward: 0.0\n",
      "episode: 43, Epsilon: 0.32130580104235906, Reward: 3.0\n",
      "episode: 44, Epsilon: 0.31809274303193547, Reward: 2.0\n",
      "episode: 45, Epsilon: 0.31491181560161613, Reward: 3.0\n",
      "episode: 46, Epsilon: 0.3117626974456, Reward: 2.0\n",
      "episode: 47, Epsilon: 0.308645070471144, Reward: 3.0\n",
      "episode: 48, Epsilon: 0.30555861976643256, Reward: 0.0\n",
      "episode: 49, Epsilon: 0.30250303356876823, Reward: 2.0\n",
      "episode: 50, Epsilon: 0.29947800323308055, Reward: 3.0\n",
      "episode: 51, Epsilon: 0.2964832232007497, Reward: 2.0\n",
      "episode: 52, Epsilon: 0.2935183909687422, Reward: 3.0\n",
      "episode: 53, Epsilon: 0.29058320705905477, Reward: 2.0\n",
      "episode: 54, Epsilon: 0.28767737498846424, Reward: 3.0\n",
      "episode: 55, Epsilon: 0.2848006012385796, Reward: 2.0\n",
      "episode: 56, Epsilon: 0.2819525952261938, Reward: 5.0\n",
      "episode: 57, Epsilon: 0.2791330692739319, Reward: 3.0\n",
      "episode: 58, Epsilon: 0.27634173858119254, Reward: 4.0\n",
      "episode: 59, Epsilon: 0.2735783211953806, Reward: 0.0\n",
      "episode: 60, Epsilon: 0.2708425379834268, Reward: 3.0\n",
      "episode: 61, Epsilon: 0.2681341126035925, Reward: 3.0\n",
      "episode: 62, Epsilon: 0.2654527714775566, Reward: 3.0\n",
      "episode: 63, Epsilon: 0.262798243762781, Reward: 0.0\n",
      "episode: 64, Epsilon: 0.2601702613251532, Reward: 2.0\n",
      "episode: 65, Epsilon: 0.25756855871190165, Reward: 3.0\n",
      "episode: 66, Epsilon: 0.25499287312478264, Reward: 2.0\n",
      "episode: 67, Epsilon: 0.2524429443935348, Reward: 5.0\n",
      "episode: 68, Epsilon: 0.24991851494959946, Reward: 3.0\n",
      "episode: 69, Epsilon: 0.24741932980010348, Reward: 2.0\n",
      "episode: 70, Epsilon: 0.24494513650210245, Reward: 0.0\n",
      "episode: 71, Epsilon: 0.24249568513708142, Reward: 3.0\n",
      "episode: 72, Epsilon: 0.2400707282857106, Reward: 2.0\n",
      "episode: 73, Epsilon: 0.2376700210028535, Reward: 3.0\n",
      "episode: 74, Epsilon: 0.23529332079282497, Reward: 2.0\n",
      "episode: 75, Epsilon: 0.2329403875848967, Reward: 0.0\n",
      "episode: 76, Epsilon: 0.23061098370904776, Reward: 4.0\n",
      "episode: 77, Epsilon: 0.22830487387195728, Reward: 3.0\n",
      "episode: 78, Epsilon: 0.2260218251332377, Reward: 0.0\n",
      "episode: 79, Epsilon: 0.22376160688190533, Reward: 2.0\n",
      "episode: 80, Epsilon: 0.22152399081308627, Reward: 0.0\n",
      "episode: 81, Epsilon: 0.2193087509049554, Reward: 4.0\n",
      "episode: 82, Epsilon: 0.21711566339590585, Reward: 3.0\n",
      "episode: 83, Epsilon: 0.2149445067619468, Reward: 0.0\n",
      "episode: 84, Epsilon: 0.21279506169432733, Reward: 3.0\n",
      "episode: 85, Epsilon: 0.21066711107738406, Reward: 2.0\n",
      "episode: 86, Epsilon: 0.20856043996661022, Reward: 0.0\n",
      "episode: 87, Epsilon: 0.20647483556694413, Reward: 3.0\n",
      "episode: 88, Epsilon: 0.20441008721127468, Reward: 2.0\n",
      "episode: 89, Epsilon: 0.20236598633916195, Reward: 3.0\n",
      "episode: 90, Epsilon: 0.20034232647577033, Reward: 2.0\n",
      "episode: 91, Epsilon: 0.19833890321101263, Reward: 3.0\n",
      "episode: 92, Epsilon: 0.1963555141789025, Reward: 3.0\n",
      "episode: 93, Epsilon: 0.19439195903711348, Reward: 4.0\n",
      "episode: 94, Epsilon: 0.19244803944674235, Reward: 2.0\n",
      "episode: 95, Epsilon: 0.19052355905227492, Reward: 2.0\n",
      "episode: 96, Epsilon: 0.18861832346175217, Reward: 0.0\n",
      "episode: 97, Epsilon: 0.18673214022713464, Reward: 0.0\n",
      "episode: 98, Epsilon: 0.18486481882486328, Reward: 3.0\n",
      "episode: 99, Epsilon: 0.18301617063661463, Reward: 3.0\n",
      "episode: 100, Epsilon: 0.18118600893024847, Reward: 2.0\n",
      "episode: 101, Epsilon: 0.179374148840946, Reward: 0.0\n",
      "episode: 102, Epsilon: 0.17758040735253652, Reward: 2.0\n",
      "episode: 103, Epsilon: 0.17580460327901115, Reward: 0.0\n",
      "episode: 104, Epsilon: 0.17404655724622103, Reward: 3.0\n",
      "episode: 105, Epsilon: 0.17230609167375882, Reward: 0.0\n",
      "episode: 106, Epsilon: 0.17058303075702122, Reward: 0.0\n",
      "episode: 107, Epsilon: 0.168877200449451, Reward: 2.0\n",
      "episode: 108, Epsilon: 0.1671884284449565, Reward: 4.0\n",
      "episode: 109, Epsilon: 0.16551654416050693, Reward: 2.0\n",
      "episode: 110, Epsilon: 0.16386137871890186, Reward: 4.0\n",
      "episode: 111, Epsilon: 0.16222276493171284, Reward: 2.0\n",
      "episode: 112, Epsilon: 0.1606005372823957, Reward: 2.0\n",
      "episode: 113, Epsilon: 0.15899453190957175, Reward: 3.0\n",
      "episode: 114, Epsilon: 0.15740458659047601, Reward: 2.0\n",
      "episode: 115, Epsilon: 0.15583054072457125, Reward: 3.0\n",
      "episode: 116, Epsilon: 0.15427223531732553, Reward: 5.0\n",
      "episode: 117, Epsilon: 0.15272951296415227, Reward: 2.0\n",
      "episode: 118, Epsilon: 0.15120221783451074, Reward: 2.0\n",
      "episode: 119, Epsilon: 0.14969019565616562, Reward: 3.0\n",
      "episode: 120, Epsilon: 0.14819329369960396, Reward: 0.0\n",
      "episode: 121, Epsilon: 0.14671136076260793, Reward: 2.0\n",
      "episode: 122, Epsilon: 0.14524424715498185, Reward: 3.0\n",
      "episode: 123, Epsilon: 0.14379180468343203, Reward: 2.0\n",
      "episode: 124, Epsilon: 0.1423538866365977, Reward: 2.0\n",
      "episode: 125, Epsilon: 0.14093034777023172, Reward: 0.0\n",
      "episode: 126, Epsilon: 0.1395210442925294, Reward: 2.0\n",
      "episode: 127, Epsilon: 0.1381258338496041, Reward: 2.0\n",
      "episode: 128, Epsilon: 0.13674457551110808, Reward: 2.0\n",
      "episode: 129, Epsilon: 0.135377129755997, Reward: 2.0\n",
      "episode: 130, Epsilon: 0.13402335845843702, Reward: 2.0\n",
      "episode: 131, Epsilon: 0.13268312487385264, Reward: 3.0\n",
      "episode: 132, Epsilon: 0.1313562936251141, Reward: 2.0\n",
      "episode: 133, Epsilon: 0.13004273068886296, Reward: 3.0\n",
      "episode: 134, Epsilon: 0.12874230338197434, Reward: 0.0\n",
      "episode: 135, Epsilon: 0.1274548803481546, Reward: 2.0\n",
      "episode: 136, Epsilon: 0.12618033154467306, Reward: 3.0\n",
      "episode: 137, Epsilon: 0.12491852822922633, Reward: 4.0\n",
      "episode: 138, Epsilon: 0.12366934294693407, Reward: 2.0\n",
      "episode: 139, Epsilon: 0.12243264951746473, Reward: 4.0\n",
      "episode: 140, Epsilon: 0.12120832302229008, Reward: 2.0\n",
      "episode: 141, Epsilon: 0.11999623979206718, Reward: 2.0\n",
      "episode: 142, Epsilon: 0.11879627739414651, Reward: 0.0\n",
      "episode: 143, Epsilon: 0.11760831462020505, Reward: 0.0\n",
      "episode: 144, Epsilon: 0.116432231474003, Reward: 3.0\n",
      "episode: 145, Epsilon: 0.11526790915926297, Reward: 3.0\n",
      "episode: 146, Epsilon: 0.11411523006767034, Reward: 3.0\n",
      "episode: 147, Epsilon: 0.11297407776699364, Reward: 0.0\n",
      "episode: 148, Epsilon: 0.11184433698932371, Reward: 0.0\n",
      "episode: 149, Epsilon: 0.11072589361943047, Reward: 2.0\n",
      "episode: 150, Epsilon: 0.10961863468323617, Reward: 2.0\n",
      "episode: 151, Epsilon: 0.1085224483364038, Reward: 3.0\n",
      "episode: 152, Epsilon: 0.10743722385303976, Reward: 2.0\n",
      "episode: 153, Epsilon: 0.10636285161450937, Reward: 5.0\n",
      "episode: 154, Epsilon: 0.10529922309836427, Reward: 3.0\n",
      "episode: 155, Epsilon: 0.10424623086738063, Reward: 0.0\n",
      "episode: 156, Epsilon: 0.10320376855870683, Reward: 4.0\n",
      "episode: 157, Epsilon: 0.10217173087311976, Reward: 4.0\n",
      "episode: 158, Epsilon: 0.10115001356438856, Reward: 2.0\n",
      "episode: 159, Epsilon: 0.10013851342874468, Reward: 0.0\n",
      "episode: 160, Epsilon: 0.09913712829445723, Reward: 2.0\n",
      "episode: 161, Epsilon: 0.09814575701151265, Reward: 2.0\n",
      "episode: 162, Epsilon: 0.09716429944139753, Reward: 0.0\n",
      "episode: 163, Epsilon: 0.09619265644698355, Reward: 2.0\n",
      "episode: 164, Epsilon: 0.09523072988251371, Reward: 2.0\n",
      "episode: 165, Epsilon: 0.09427842258368857, Reward: 3.0\n",
      "episode: 166, Epsilon: 0.09333563835785168, Reward: 0.0\n",
      "episode: 167, Epsilon: 0.09240228197427316, Reward: 2.0\n",
      "episode: 168, Epsilon: 0.09147825915453044, Reward: 0.0\n",
      "episode: 169, Epsilon: 0.09056347656298513, Reward: 0.0\n",
      "episode: 170, Epsilon: 0.08965784179735528, Reward: 3.0\n",
      "episode: 171, Epsilon: 0.08876126337938173, Reward: 2.0\n",
      "episode: 172, Epsilon: 0.08787365074558791, Reward: 3.0\n",
      "episode: 173, Epsilon: 0.08699491423813203, Reward: 0.0\n",
      "episode: 174, Epsilon: 0.08612496509575071, Reward: 3.0\n",
      "episode: 175, Epsilon: 0.0852637154447932, Reward: 0.0\n",
      "episode: 176, Epsilon: 0.08441107829034528, Reward: 2.0\n",
      "episode: 177, Epsilon: 0.08356696750744182, Reward: 0.0\n",
      "episode: 178, Epsilon: 0.08273129783236739, Reward: 0.0\n",
      "episode: 179, Epsilon: 0.08190398485404372, Reward: 2.0\n",
      "episode: 180, Epsilon: 0.08108494500550328, Reward: 2.0\n",
      "episode: 181, Epsilon: 0.08027409555544825, Reward: 0.0\n",
      "episode: 182, Epsilon: 0.07947135459989377, Reward: 3.0\n",
      "episode: 183, Epsilon: 0.07867664105389482, Reward: 0.0\n",
      "episode: 184, Epsilon: 0.07788987464335588, Reward: 2.0\n",
      "episode: 185, Epsilon: 0.07711097589692233, Reward: 0.0\n",
      "episode: 186, Epsilon: 0.0763398661379531, Reward: 2.0\n",
      "episode: 187, Epsilon: 0.07557646747657357, Reward: 0.0\n",
      "episode: 188, Epsilon: 0.07482070280180783, Reward: 0.0\n",
      "episode: 189, Epsilon: 0.07407249577378976, Reward: 0.0\n",
      "episode: 190, Epsilon: 0.07333177081605186, Reward: 2.0\n",
      "episode: 191, Epsilon: 0.07259845310789134, Reward: 0.0\n",
      "episode: 192, Epsilon: 0.07187246857681243, Reward: 3.0\n",
      "episode: 193, Epsilon: 0.0711537438910443, Reward: 2.0\n",
      "episode: 194, Epsilon: 0.07044220645213385, Reward: 3.0\n",
      "episode: 195, Epsilon: 0.06973778438761251, Reward: 3.0\n",
      "episode: 196, Epsilon: 0.06904040654373639, Reward: 2.0\n",
      "episode: 197, Epsilon: 0.06835000247829902, Reward: 2.0\n",
      "episode: 198, Epsilon: 0.06766650245351603, Reward: 0.0\n",
      "episode: 199, Epsilon: 0.06698983742898088, Reward: 3.0\n",
      "episode: 200, Epsilon: 0.06631993905469107, Reward: 0.0\n",
      "episode: 201, Epsilon: 0.06565673966414415, Reward: 0.0\n",
      "episode: 202, Epsilon: 0.06500017226750271, Reward: 4.0\n",
      "episode: 203, Epsilon: 0.06435017054482768, Reward: 2.0\n",
      "episode: 204, Epsilon: 0.06370666883937941, Reward: 0.0\n",
      "episode: 205, Epsilon: 0.06306960215098562, Reward: 3.0\n",
      "episode: 206, Epsilon: 0.06243890612947576, Reward: 3.0\n",
      "episode: 207, Epsilon: 0.061814517068181, Reward: 0.0\n",
      "episode: 208, Epsilon: 0.06119637189749919, Reward: 0.0\n",
      "episode: 209, Epsilon: 0.0605844081785242, Reward: 2.0\n",
      "episode: 210, Epsilon: 0.05997856409673896, Reward: 2.0\n",
      "episode: 211, Epsilon: 0.05937877845577157, Reward: 0.0\n",
      "episode: 212, Epsilon: 0.05878499067121386, Reward: 4.0\n",
      "episode: 213, Epsilon: 0.05819714076450172, Reward: 0.0\n",
      "episode: 214, Epsilon: 0.057615169356856705, Reward: 2.0\n",
      "episode: 215, Epsilon: 0.05703901766328814, Reward: 2.0\n",
      "episode: 216, Epsilon: 0.05646862748665526, Reward: 2.0\n",
      "episode: 217, Epsilon: 0.0559039412117887, Reward: 5.0\n",
      "episode: 218, Epsilon: 0.05534490179967082, Reward: 2.0\n",
      "episode: 219, Epsilon: 0.05479145278167411, Reward: 3.0\n",
      "episode: 220, Epsilon: 0.054243538253857373, Reward: 0.0\n",
      "episode: 221, Epsilon: 0.0537011028713188, Reward: 3.0\n",
      "episode: 222, Epsilon: 0.053164091842605614, Reward: 2.0\n",
      "episode: 223, Epsilon: 0.05263245092417956, Reward: 0.0\n",
      "episode: 224, Epsilon: 0.05210612641493776, Reward: 3.0\n",
      "episode: 225, Epsilon: 0.05158506515078838, Reward: 0.0\n",
      "episode: 226, Epsilon: 0.051069214499280494, Reward: 2.0\n",
      "episode: 227, Epsilon: 0.05055852235428769, Reward: 3.0\n",
      "episode: 228, Epsilon: 0.050052937130744816, Reward: 2.0\n",
      "episode: 229, Epsilon: 0.04955240775943737, Reward: 4.0\n",
      "episode: 230, Epsilon: 0.049056883681842994, Reward: 0.0\n",
      "episode: 231, Epsilon: 0.048566314845024564, Reward: 3.0\n",
      "episode: 232, Epsilon: 0.048080651696574314, Reward: 0.0\n",
      "episode: 233, Epsilon: 0.04759984517960857, Reward: 2.0\n",
      "episode: 234, Epsilon: 0.04712384672781249, Reward: 2.0\n",
      "episode: 235, Epsilon: 0.04665260826053436, Reward: 0.0\n",
      "episode: 236, Epsilon: 0.04618608217792902, Reward: 4.0\n",
      "episode: 237, Epsilon: 0.04572422135614973, Reward: 3.0\n",
      "episode: 238, Epsilon: 0.04526697914258823, Reward: 3.0\n",
      "episode: 239, Epsilon: 0.044814309351162346, Reward: 2.0\n",
      "episode: 240, Epsilon: 0.04436616625765072, Reward: 2.0\n",
      "episode: 241, Epsilon: 0.04392250459507421, Reward: 2.0\n",
      "episode: 242, Epsilon: 0.04348327954912347, Reward: 2.0\n",
      "episode: 243, Epsilon: 0.04304844675363223, Reward: 2.0\n",
      "episode: 244, Epsilon: 0.042617962286095906, Reward: 2.0\n",
      "episode: 245, Epsilon: 0.04219178266323495, Reward: 4.0\n",
      "episode: 246, Epsilon: 0.0417698648366026, Reward: 2.0\n",
      "episode: 247, Epsilon: 0.041352166188236575, Reward: 3.0\n",
      "episode: 248, Epsilon: 0.040938644526354206, Reward: 2.0\n",
      "episode: 249, Epsilon: 0.04052925808109067, Reward: 2.0\n",
      "episode: 250, Epsilon: 0.04012396550027976, Reward: 2.0\n",
      "episode: 251, Epsilon: 0.03972272584527696, Reward: 2.0\n",
      "episode: 252, Epsilon: 0.03932549858682419, Reward: 2.0\n",
      "episode: 253, Epsilon: 0.03893224360095594, Reward: 3.0\n",
      "episode: 254, Epsilon: 0.038542921164946384, Reward: 2.0\n",
      "episode: 255, Epsilon: 0.03815749195329692, Reward: 3.0\n",
      "episode: 256, Epsilon: 0.037775917033763956, Reward: 0.0\n",
      "episode: 257, Epsilon: 0.03739815786342632, Reward: 0.0\n",
      "episode: 258, Epsilon: 0.03702417628479206, Reward: 0.0\n",
      "episode: 259, Epsilon: 0.03665393452194413, Reward: 3.0\n",
      "episode: 260, Epsilon: 0.03628739517672469, Reward: 2.0\n",
      "episode: 261, Epsilon: 0.03592452122495744, Reward: 3.0\n",
      "episode: 262, Epsilon: 0.03556527601270787, Reward: 0.0\n",
      "episode: 263, Epsilon: 0.03520962325258079, Reward: 2.0\n",
      "episode: 264, Epsilon: 0.034857527020054985, Reward: 3.0\n",
      "episode: 265, Epsilon: 0.03450895174985443, Reward: 3.0\n",
      "episode: 266, Epsilon: 0.03416386223235589, Reward: 2.0\n",
      "episode: 267, Epsilon: 0.03382222361003233, Reward: 2.0\n",
      "episode: 268, Epsilon: 0.033484001373932, Reward: 4.0\n",
      "episode: 269, Epsilon: 0.033149161360192685, Reward: 3.0\n",
      "episode: 270, Epsilon: 0.032817669746590755, Reward: 0.0\n",
      "episode: 271, Epsilon: 0.032489493049124844, Reward: 4.0\n",
      "episode: 272, Epsilon: 0.0321645981186336, Reward: 3.0\n",
      "episode: 273, Epsilon: 0.03184295213744726, Reward: 3.0\n",
      "episode: 274, Epsilon: 0.03152452261607279, Reward: 2.0\n",
      "episode: 275, Epsilon: 0.03120927738991206, Reward: 3.0\n",
      "episode: 276, Epsilon: 0.03089718461601294, Reward: 2.0\n",
      "episode: 277, Epsilon: 0.03058821276985281, Reward: 0.0\n",
      "episode: 278, Epsilon: 0.03028233064215428, Reward: 0.0\n",
      "episode: 279, Epsilon: 0.02997950733573274, Reward: 2.0\n",
      "episode: 280, Epsilon: 0.02967971226237541, Reward: 2.0\n",
      "episode: 281, Epsilon: 0.029382915139751657, Reward: 3.0\n",
      "episode: 282, Epsilon: 0.02908908598835414, Reward: 2.0\n",
      "episode: 283, Epsilon: 0.028798195128470597, Reward: 5.0\n",
      "episode: 284, Epsilon: 0.02851021317718589, Reward: 2.0\n",
      "episode: 285, Epsilon: 0.02822511104541403, Reward: 0.0\n",
      "episode: 286, Epsilon: 0.02794285993495989, Reward: 0.0\n",
      "episode: 287, Epsilon: 0.027663431335610292, Reward: 3.0\n",
      "episode: 288, Epsilon: 0.02738679702225419, Reward: 3.0\n",
      "episode: 289, Epsilon: 0.027112929052031647, Reward: 2.0\n",
      "episode: 290, Epsilon: 0.02684179976151133, Reward: 2.0\n",
      "episode: 291, Epsilon: 0.026573381763896217, Reward: 2.0\n",
      "episode: 292, Epsilon: 0.026307647946257253, Reward: 2.0\n",
      "episode: 293, Epsilon: 0.026044571466794682, Reward: 2.0\n",
      "episode: 294, Epsilon: 0.025784125752126734, Reward: 2.0\n",
      "episode: 295, Epsilon: 0.025526284494605467, Reward: 2.0\n",
      "episode: 296, Epsilon: 0.025271021649659414, Reward: 4.0\n",
      "episode: 297, Epsilon: 0.02501831143316282, Reward: 2.0\n",
      "episode: 298, Epsilon: 0.02476812831883119, Reward: 2.0\n",
      "episode: 299, Epsilon: 0.02452044703564288, Reward: 3.0\n",
      "episode: 300, Epsilon: 0.02427524256528645, Reward: 2.0\n",
      "episode: 301, Epsilon: 0.024032490139633583, Reward: 0.0\n",
      "episode: 302, Epsilon: 0.023792165238237246, Reward: 3.0\n",
      "episode: 303, Epsilon: 0.023554243585854874, Reward: 2.0\n",
      "episode: 304, Epsilon: 0.023318701149996325, Reward: 3.0\n",
      "episode: 305, Epsilon: 0.02308551413849636, Reward: 0.0\n",
      "episode: 306, Epsilon: 0.022854658997111397, Reward: 5.0\n",
      "episode: 307, Epsilon: 0.022626112407140284, Reward: 3.0\n",
      "episode: 308, Epsilon: 0.02239985128306888, Reward: 2.0\n",
      "episode: 309, Epsilon: 0.02217585277023819, Reward: 2.0\n",
      "episode: 310, Epsilon: 0.021954094242535808, Reward: 3.0\n",
      "episode: 311, Epsilon: 0.02173455330011045, Reward: 0.0\n",
      "episode: 312, Epsilon: 0.021517207767109345, Reward: 2.0\n",
      "episode: 313, Epsilon: 0.02130203568943825, Reward: 3.0\n",
      "episode: 314, Epsilon: 0.021089015332543867, Reward: 2.0\n",
      "episode: 315, Epsilon: 0.02087812517921843, Reward: 2.0\n",
      "episode: 316, Epsilon: 0.020669343927426243, Reward: 2.0\n",
      "episode: 317, Epsilon: 0.02046265048815198, Reward: 3.0\n",
      "episode: 318, Epsilon: 0.020258023983270458, Reward: 0.0\n",
      "episode: 319, Epsilon: 0.020055443743437755, Reward: 4.0\n",
      "episode: 320, Epsilon: 0.019854889306003376, Reward: 0.0\n",
      "episode: 321, Epsilon: 0.01965634041294334, Reward: 2.0\n",
      "episode: 322, Epsilon: 0.019459777008813905, Reward: 3.0\n",
      "episode: 323, Epsilon: 0.019265179238725765, Reward: 2.0\n",
      "episode: 324, Epsilon: 0.019072527446338507, Reward: 3.0\n",
      "episode: 325, Epsilon: 0.01888180217187512, Reward: 2.0\n",
      "episode: 326, Epsilon: 0.01869298415015637, Reward: 2.0\n",
      "episode: 327, Epsilon: 0.018506054308654804, Reward: 2.0\n",
      "episode: 328, Epsilon: 0.018320993765568255, Reward: 0.0\n",
      "episode: 329, Epsilon: 0.018137783827912573, Reward: 3.0\n",
      "episode: 330, Epsilon: 0.017956405989633446, Reward: 3.0\n",
      "episode: 331, Epsilon: 0.017776841929737112, Reward: 3.0\n",
      "episode: 332, Epsilon: 0.01759907351043974, Reward: 2.0\n",
      "episode: 333, Epsilon: 0.01742308277533534, Reward: 3.0\n",
      "episode: 334, Epsilon: 0.017248851947581988, Reward: 3.0\n",
      "episode: 335, Epsilon: 0.01707636342810617, Reward: 3.0\n",
      "episode: 336, Epsilon: 0.016905599793825107, Reward: 2.0\n",
      "episode: 337, Epsilon: 0.016736543795886856, Reward: 3.0\n",
      "episode: 338, Epsilon: 0.016569178357927986, Reward: 0.0\n",
      "episode: 339, Epsilon: 0.016403486574348706, Reward: 2.0\n",
      "episode: 340, Epsilon: 0.016239451708605218, Reward: 2.0\n",
      "episode: 341, Epsilon: 0.016077057191519167, Reward: 2.0\n",
      "episode: 342, Epsilon: 0.015916286619603974, Reward: 3.0\n",
      "episode: 343, Epsilon: 0.015757123753407935, Reward: 3.0\n",
      "episode: 344, Epsilon: 0.015599552515873855, Reward: 0.0\n",
      "episode: 345, Epsilon: 0.015443556990715116, Reward: 0.0\n",
      "episode: 346, Epsilon: 0.015289121420807966, Reward: 4.0\n",
      "episode: 347, Epsilon: 0.015136230206599885, Reward: 3.0\n",
      "episode: 348, Epsilon: 0.014984867904533886, Reward: 3.0\n",
      "episode: 349, Epsilon: 0.014835019225488548, Reward: 5.0\n",
      "episode: 350, Epsilon: 0.014686669033233662, Reward: 0.0\n",
      "episode: 351, Epsilon: 0.014539802342901325, Reward: 0.0\n",
      "episode: 352, Epsilon: 0.014394404319472311, Reward: 3.0\n",
      "episode: 353, Epsilon: 0.014250460276277587, Reward: 2.0\n",
      "episode: 354, Epsilon: 0.014107955673514812, Reward: 4.0\n",
      "episode: 355, Epsilon: 0.013966876116779664, Reward: 2.0\n",
      "episode: 356, Epsilon: 0.013827207355611867, Reward: 2.0\n",
      "episode: 357, Epsilon: 0.013688935282055748, Reward: 0.0\n",
      "episode: 358, Epsilon: 0.013552045929235191, Reward: 2.0\n",
      "episode: 359, Epsilon: 0.013416525469942838, Reward: 2.0\n",
      "episode: 360, Epsilon: 0.01328236021524341, Reward: 0.0\n",
      "episode: 361, Epsilon: 0.013149536613090975, Reward: 0.0\n",
      "episode: 362, Epsilon: 0.013018041246960066, Reward: 3.0\n",
      "episode: 363, Epsilon: 0.012887860834490466, Reward: 3.0\n",
      "episode: 364, Epsilon: 0.012758982226145561, Reward: 3.0\n",
      "episode: 365, Epsilon: 0.012631392403884105, Reward: 5.0\n",
      "episode: 366, Epsilon: 0.012505078479845264, Reward: 2.0\n",
      "episode: 367, Epsilon: 0.01238002769504681, Reward: 3.0\n",
      "episode: 368, Epsilon: 0.012256227418096342, Reward: 0.0\n",
      "episode: 369, Epsilon: 0.012133665143915378, Reward: 2.0\n",
      "episode: 370, Epsilon: 0.012012328492476224, Reward: 2.0\n",
      "episode: 371, Epsilon: 0.011892205207551462, Reward: 3.0\n",
      "episode: 372, Epsilon: 0.011773283155475947, Reward: 2.0\n",
      "episode: 373, Epsilon: 0.011655550323921187, Reward: 0.0\n",
      "episode: 374, Epsilon: 0.011538994820681976, Reward: 2.0\n",
      "episode: 375, Epsilon: 0.011423604872475157, Reward: 3.0\n",
      "episode: 376, Epsilon: 0.011309368823750405, Reward: 2.0\n",
      "episode: 377, Epsilon: 0.011196275135512902, Reward: 3.0\n",
      "episode: 378, Epsilon: 0.011084312384157772, Reward: 0.0\n",
      "episode: 379, Epsilon: 0.010973469260316195, Reward: 3.0\n",
      "episode: 380, Epsilon: 0.010863734567713033, Reward: 2.0\n",
      "episode: 381, Epsilon: 0.010755097222035902, Reward: 0.0\n",
      "episode: 382, Epsilon: 0.010647546249815542, Reward: 3.0\n",
      "episode: 383, Epsilon: 0.010541070787317386, Reward: 2.0\n",
      "episode: 384, Epsilon: 0.010435660079444213, Reward: 2.0\n",
      "episode: 385, Epsilon: 0.010331303478649771, Reward: 0.0\n",
      "episode: 386, Epsilon: 0.010227990443863274, Reward: 0.0\n",
      "episode: 387, Epsilon: 0.01012571053942464, Reward: 2.0\n",
      "episode: 388, Epsilon: 0.010024453434030394, Reward: 0.0\n",
      "episode: 389, Epsilon: 0.00992420889969009, Reward: 0.0\n",
      "episode: 390, Epsilon: 0.009824966810693189, Reward: 4.0\n",
      "episode: 391, Epsilon: 0.009726717142586256, Reward: 2.0\n",
      "episode: 392, Epsilon: 0.009629449971160393, Reward: 2.0\n",
      "episode: 393, Epsilon: 0.00953315547144879, Reward: 3.0\n",
      "episode: 394, Epsilon: 0.009437823916734301, Reward: 2.0\n",
      "episode: 395, Epsilon: 0.009343445677566958, Reward: 3.0\n",
      "episode: 396, Epsilon: 0.009250011220791289, Reward: 2.0\n",
      "episode: 397, Epsilon: 0.009157511108583375, Reward: 0.0\n",
      "episode: 398, Epsilon: 0.009065935997497542, Reward: 0.0\n",
      "episode: 399, Epsilon: 0.008975276637522567, Reward: 2.0\n",
      "episode: 400, Epsilon: 0.008885523871147341, Reward: 2.0\n",
      "episode: 401, Epsilon: 0.008796668632435868, Reward: 4.0\n",
      "episode: 402, Epsilon: 0.00870870194611151, Reward: 0.0\n",
      "episode: 403, Epsilon: 0.008621614926650395, Reward: 0.0\n",
      "episode: 404, Epsilon: 0.008535398777383891, Reward: 2.0\n",
      "episode: 405, Epsilon: 0.008450044789610053, Reward: 3.0\n",
      "episode: 406, Epsilon: 0.008365544341713953, Reward: 4.0\n",
      "episode: 407, Epsilon: 0.008281888898296813, Reward: 3.0\n",
      "episode: 408, Epsilon: 0.008199070009313844, Reward: 3.0\n",
      "episode: 409, Epsilon: 0.008117079309220706, Reward: 0.0\n",
      "episode: 410, Epsilon: 0.008035908516128499, Reward: 0.0\n",
      "episode: 411, Epsilon: 0.007955549430967214, Reward: 2.0\n",
      "episode: 412, Epsilon: 0.007875993936657543, Reward: 3.0\n",
      "episode: 413, Epsilon: 0.0077972339972909675, Reward: 0.0\n",
      "episode: 414, Epsilon: 0.0077192616573180575, Reward: 2.0\n",
      "episode: 415, Epsilon: 0.007642069040744877, Reward: 2.0\n",
      "episode: 416, Epsilon: 0.007565648350337428, Reward: 2.0\n",
      "episode: 417, Epsilon: 0.007489991866834054, Reward: 2.0\n",
      "episode: 418, Epsilon: 0.007415091948165713, Reward: 0.0\n",
      "episode: 419, Epsilon: 0.007340941028684056, Reward: 0.0\n",
      "episode: 420, Epsilon: 0.007267531618397215, Reward: 3.0\n",
      "episode: 421, Epsilon: 0.007194856302213243, Reward: 2.0\n",
      "episode: 422, Epsilon: 0.00712290773919111, Reward: 2.0\n",
      "episode: 423, Epsilon: 0.007051678661799198, Reward: 0.0\n",
      "episode: 424, Epsilon: 0.006981161875181207, Reward: 2.0\n",
      "episode: 425, Epsilon: 0.006911350256429394, Reward: 0.0\n",
      "episode: 426, Epsilon: 0.0068422367538651, Reward: 2.0\n",
      "episode: 427, Epsilon: 0.0067738143863264495, Reward: 2.0\n",
      "episode: 428, Epsilon: 0.006706076242463185, Reward: 2.0\n",
      "episode: 429, Epsilon: 0.006639015480038553, Reward: 0.0\n",
      "episode: 430, Epsilon: 0.0065726253252381675, Reward: 2.0\n",
      "episode: 431, Epsilon: 0.006506899071985785, Reward: 3.0\n",
      "episode: 432, Epsilon: 0.006441830081265927, Reward: 3.0\n",
      "episode: 433, Epsilon: 0.0063774117804532675, Reward: 0.0\n",
      "episode: 434, Epsilon: 0.006313637662648735, Reward: 0.0\n",
      "episode: 435, Epsilon: 0.006250501286022248, Reward: 3.0\n",
      "episode: 436, Epsilon: 0.006187996273162025, Reward: 2.0\n",
      "episode: 437, Epsilon: 0.006126116310430405, Reward: 2.0\n",
      "episode: 438, Epsilon: 0.006064855147326101, Reward: 2.0\n",
      "episode: 439, Epsilon: 0.00600420659585284, Reward: 2.0\n",
      "episode: 440, Epsilon: 0.005944164529894312, Reward: 2.0\n",
      "episode: 441, Epsilon: 0.0058847228845953685, Reward: 3.0\n",
      "episode: 442, Epsilon: 0.005825875655749415, Reward: 0.0\n",
      "episode: 443, Epsilon: 0.005767616899191921, Reward: 2.0\n",
      "episode: 444, Epsilon: 0.005709940730200002, Reward: 0.0\n",
      "episode: 445, Epsilon: 0.005652841322898002, Reward: 0.0\n",
      "episode: 446, Epsilon: 0.005596312909669022, Reward: 0.0\n",
      "episode: 447, Epsilon: 0.005540349780572332, Reward: 2.0\n",
      "episode: 448, Epsilon: 0.005484946282766609, Reward: 0.0\n",
      "episode: 449, Epsilon: 0.005430096819938943, Reward: 2.0\n",
      "episode: 450, Epsilon: 0.005375795851739553, Reward: 0.0\n",
      "episode: 451, Epsilon: 0.005322037893222158, Reward: 0.0\n",
      "episode: 452, Epsilon: 0.0052688175142899365, Reward: 2.0\n",
      "episode: 453, Epsilon: 0.005216129339147037, Reward: 2.0\n",
      "episode: 454, Epsilon: 0.0051639680457555666, Reward: 0.0\n",
      "episode: 455, Epsilon: 0.005112328365298011, Reward: 0.0\n",
      "episode: 456, Epsilon: 0.00506120508164503, Reward: 3.0\n",
      "episode: 457, Epsilon: 0.00501059303082858, Reward: 4.0\n",
      "episode: 458, Epsilon: 0.004960487100520294, Reward: 2.0\n",
      "episode: 459, Epsilon: 0.004910882229515091, Reward: 0.0\n",
      "episode: 460, Epsilon: 0.00486177340721994, Reward: 2.0\n",
      "episode: 461, Epsilon: 0.00481315567314774, Reward: 2.0\n",
      "episode: 462, Epsilon: 0.0047650241164162626, Reward: 0.0\n",
      "episode: 463, Epsilon: 0.0047173738752520995, Reward: 2.0\n",
      "episode: 464, Epsilon: 0.004670200136499578, Reward: 2.0\n",
      "episode: 465, Epsilon: 0.0046234981351345825, Reward: 3.0\n",
      "episode: 466, Epsilon: 0.004577263153783237, Reward: 2.0\n",
      "episode: 467, Epsilon: 0.004531490522245404, Reward: 0.0\n",
      "episode: 468, Epsilon: 0.00448617561702295, Reward: 0.0\n",
      "episode: 469, Epsilon: 0.00444131386085272, Reward: 2.0\n",
      "episode: 470, Epsilon: 0.004396900722244193, Reward: 3.0\n",
      "episode: 471, Epsilon: 0.004352931715021751, Reward: 0.0\n",
      "episode: 472, Epsilon: 0.004309402397871534, Reward: 3.0\n",
      "episode: 473, Epsilon: 0.0042663083738928185, Reward: 2.0\n",
      "episode: 474, Epsilon: 0.00422364529015389, Reward: 4.0\n",
      "episode: 475, Epsilon: 0.004181408837252351, Reward: 2.0\n",
      "episode: 476, Epsilon: 0.004139594748879827, Reward: 2.0\n",
      "episode: 477, Epsilon: 0.004098198801391029, Reward: 2.0\n",
      "episode: 478, Epsilon: 0.004057216813377119, Reward: 2.0\n",
      "episode: 479, Epsilon: 0.004016644645243348, Reward: 0.0\n",
      "episode: 480, Epsilon: 0.003976478198790915, Reward: 2.0\n",
      "episode: 481, Epsilon: 0.003936713416803005, Reward: 0.0\n",
      "episode: 482, Epsilon: 0.0038973462826349748, Reward: 2.0\n",
      "episode: 483, Epsilon: 0.003858372819808625, Reward: 3.0\n",
      "episode: 484, Epsilon: 0.0038197890916105387, Reward: 2.0\n",
      "episode: 485, Epsilon: 0.0037815912006944332, Reward: 3.0\n",
      "episode: 486, Epsilon: 0.003743775288687489, Reward: 3.0\n",
      "episode: 487, Epsilon: 0.003706337535800614, Reward: 4.0\n",
      "episode: 488, Epsilon: 0.0036692741604426077, Reward: 2.0\n",
      "episode: 489, Epsilon: 0.0036325814188381818, Reward: 2.0\n",
      "episode: 490, Epsilon: 0.0035962556046498, Reward: 4.0\n",
      "episode: 491, Epsilon: 0.003560293048603302, Reward: 2.0\n",
      "episode: 492, Epsilon: 0.003524690118117269, Reward: 0.0\n",
      "episode: 493, Epsilon: 0.0034894432169360963, Reward: 2.0\n",
      "episode: 494, Epsilon: 0.0034545487847667355, Reward: 2.0\n",
      "episode: 495, Epsilon: 0.0034200032969190683, Reward: 0.0\n",
      "episode: 496, Epsilon: 0.0033858032639498777, Reward: 0.0\n",
      "episode: 497, Epsilon: 0.003351945231310379, Reward: 2.0\n",
      "episode: 498, Epsilon: 0.0033184257789972754, Reward: 4.0\n",
      "episode: 499, Epsilon: 0.0032852415212073025, Reward: 0.0\n",
      "episode: 500, Epsilon: 0.0032523891059952296, Reward: 4.0\n",
      "episode: 501, Epsilon: 0.003219865214935277, Reward: 0.0\n",
      "episode: 502, Epsilon: 0.0031876665627859242, Reward: 0.0\n",
      "episode: 503, Epsilon: 0.003155789897158065, Reward: 3.0\n",
      "episode: 504, Epsilon: 0.0031242319981864843, Reward: 2.0\n",
      "episode: 505, Epsilon: 0.0030929896782046196, Reward: 0.0\n",
      "episode: 506, Epsilon: 0.0030620597814225736, Reward: 0.0\n",
      "episode: 507, Epsilon: 0.0030314391836083476, Reward: 2.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f4b30fc5a9f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[1;31m# action을 수행함 --> Get new state and reward from environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmainDQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0megreedy_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f4b30fc5a9f1>\u001b[0m in \u001b[0;36megreedy_action\u001b[0;34m(self, epsilon, env, state)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[1;31m#print(\"Episode: {0}, Action: {1}\".format(episode, action))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mQ_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[1;31m#print(\"Episode: {0}, State: {1}, Q_h: {2}, Action: {3}\".format(episode, state, Q_h, action))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f4b30fc5a9f1>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m210\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m160\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[1;31m# e-greedy 를 사용하여 action값 구함\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\JEONGHYEOK\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\JEONGHYEOK\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\JEONGHYEOK\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\JEONGHYEOK\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\JEONGHYEOK\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# https://github.com/openai/gym/wiki/CartPole-v0\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "\n",
    "\n",
    "# 하이퍼파라미터\n",
    "INITIAL_EPSILON = 0.5\n",
    "learning_rate = 0.0001\n",
    "max_episodes = 10000\n",
    "discount_factor = 0.9\n",
    "episode_list = []\n",
    "train_error_list = []\n",
    "actions_list = []\n",
    "HEIGHT = 210\n",
    "WIDTH = 160\n",
    "\n",
    "# 테스트 에피소드 주기\n",
    "TEST_PERIOD = 100\n",
    "\n",
    "# src model에서 target model로 trainable variable copy 주기\n",
    "COPY_PERIOD = 10\n",
    "\n",
    "\n",
    "\n",
    "#네트워크 클래스 구성\n",
    "class DQN:\n",
    "    def __init__(self, session, height, width, output_size, name=\"main\"):\n",
    "        # 네트워크 정보 입력\n",
    "        self.session = session\n",
    "        self.height = HEIGHT\n",
    "        self.width = WIDTH\n",
    "        self.output_size = output_size\n",
    "        self.net_name = name\n",
    "        \n",
    "        # 네트워크 생성\n",
    "        self.build_network()\n",
    "\n",
    "    def build_network(self):\n",
    "        with tf.variable_scope(self.net_name):\n",
    "            # Convolutional Neural Network (3 filter 2 Fc layer)\n",
    "            self.X = tf.placeholder(shape=[None, self.height, self.width, 1], dtype=tf.float32)\n",
    "            self.Y = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "\n",
    "            W_conv1 = tf.Variable(tf.truncated_normal([6, 4, 1, 32], stddev =0.1))\n",
    "            W_conv2 = tf.Variable(tf.truncated_normal([4, 4, 32, 64], stddev =0.1))\n",
    "            W_conv3 = tf.Variable(tf.truncated_normal([5, 3, 64, 64], stddev =0.1))\n",
    "            b_conv1 = tf.Variable(tf.constant(0.1, shape = [32]))\n",
    "            b_conv2 = tf.Variable(tf.constant(0.1, shape = [64]))\n",
    "            b_conv3 = tf.Variable(tf.constant(0.1, shape = [64]))\n",
    "            \n",
    "            W_fc1 = tf.Variable(tf.truncated_normal([11*9*64, 512], stddev= 0.1))\n",
    "            b_fc1 = tf.Variable(tf.constant(0.1, shape = [512]))\n",
    "            W_fc2 = tf.Variable(tf.truncated_normal([512, output_size], stddev =0.1))\n",
    "            b_fc2 = tf.Variable(tf.constant(0.1, shape = [output_size]))\n",
    "            \n",
    "            h_conv1 = tf.nn.relu(tf.nn.conv2d(self.X, W_conv1, strides= [1,4,4,1], padding='VALID') + b_conv1)\n",
    "            h_conv2 = tf.nn.relu(tf.nn.conv2d(h_conv1, W_conv2, strides = [1,2,2,1], padding ='VALID') + b_conv2)\n",
    "            h_conv3 = tf.nn.relu(tf.nn.conv2d(h_conv2, W_conv3, strides = [1,2,2,1], padding ='VALID') + b_conv3)\n",
    "            \n",
    "            L1 = tf.reshape(h_conv3, [-1, W_fc1.get_shape().as_list()[0]])\n",
    "            L2 = tf.nn.relu(tf.matmul(L1,W_fc1)+b_fc1)\n",
    "            self.Qpred = tf.matmul(L2, W_fc2)+b_fc2\n",
    "            \n",
    "        # 손실 함수 및 최적화 함수\n",
    "        self.action = tf.placeholder(shape=[None, self.output_size], dtype=tf.float32)\n",
    "        Q_action = tf.reduce_sum(tf.multiply(self.Qpred, self.action), reduction_indices=1)\n",
    "        self.loss = tf.reduce_mean(tf.square(self.Y - Q_action))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "\n",
    "    # 예측한 Q값 구하기\n",
    "    def predict(self, state):\n",
    "#         x = np.reshape(state, newshape=[-1, 84, 84, 1])\n",
    "\n",
    "        x = np.reshape(state, newshape=[-1, 210,160, 1])\n",
    "        return self.session.run(self.Qpred, feed_dict={self.X: x})\n",
    "\n",
    "    # e-greedy 를 사용하여 action값 구함\n",
    "    def egreedy_action(self, epsilon, env, state):\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "            #print(\"Episode: {0}, Action: {1}\".format(episode, action))\n",
    "        else:\n",
    "            Q_h = self.predict(state)\n",
    "            action = np.argmax(Q_h)\n",
    "            #print(\"Episode: {0}, State: {1}, Q_h: {2}, Action: {3}\".format(episode, state, Q_h, action))\n",
    "        return action\n",
    "    \n",
    "def update_from_memory(mainDQN, targetDQN, batch_size):\n",
    "    state_batch = np.ndarray(shape=[batch_size, mainDQN.height, mainDQN.width, 1])\n",
    "    action_batch = np.ndarray(shape=[batch_size, mainDQN.output_size])\n",
    "\n",
    "    minibatch = random.sample(REPLAY_MEMORY, batch_size)\n",
    "    i = 0\n",
    "    y_batch = []\n",
    "    for sample in minibatch:\n",
    "        state, action, reward,new_state, ter = sample         # unpacking\n",
    "\n",
    "        if done:\n",
    "            y_batch.append(reward)\n",
    "        else:\n",
    "            y_batch.append(reward + (1 - ter ) * discount_factor * np.max(targetDQN.predict(new_state)))\n",
    "\n",
    "        one_hot_action = np.zeros(mainDQN.output_size) # [0.0, 0.0]\n",
    "        one_hot_action[action] = 1\n",
    "        \n",
    "#         one_hot_action = tf.one_hot(action, mainDQN.output_size, 1.0, 0.0)\n",
    "\n",
    "        state_batch[i] = np.reshape(new_state, newshape=[210,160,1])\n",
    "        action_batch[i] = one_hot_action\n",
    "        i += 1\n",
    "\n",
    "    # DQN 알고리즘으로 학습\n",
    "    loss_value, _ = mainDQN.session.run([mainDQN.loss, mainDQN.optimizer],\n",
    "                                     feed_dict={mainDQN.X: state_batch, mainDQN.Y: y_batch, mainDQN.action: action_batch})\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_terminal(start_live, l, reward, no_life_game, ter): \n",
    "    '''목숨이 줄어들거나, negative reward를 받았을 때, terminal 처리 \n",
    "    Args: start_live(int): 라이프가 있는 게임일 경우, 현재 라이프 수 \n",
    "    l(dict): 다음 상태에서 라이프가 줄었는지 확인하기 위한 다음 frame의 라이프 \n",
    "    info no_life_game(bool): 라이프가 없는 게임일 경우, negative reward를 받으면 \n",
    "    terminal 처리를 해주기 위한 게임 타입 ter(bool): terminal 처리를 저장할 \n",
    "    arg Returns: \n",
    "    list: \n",
    "    ter(bool): terminal 상태 \n",
    "    start_live(int): 줄어든 라이프로 업데이트된 값 ''' \n",
    "    if no_life_game: # 목숨이 없는 게임일 경우 Terminal 처리 \n",
    "        if reward < 0: \n",
    "            ter = True \n",
    "    else: # 목숨 있는 게임일 경우 Terminal 처리 \n",
    "        if start_live > l['ale.lives']: \n",
    "            ter = True \n",
    "            start_live = l['ale.lives'] \n",
    "    return [ter, start_live]\n",
    "\n",
    "def bot_play(DQN, env):\n",
    "    \"\"\"\n",
    "    See our trained network in action\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    state = np.sum(state, axis=2)\n",
    "    reward_sum = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = np.argmax(DQN.predict(state))\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "        state = np.sum(new_state, axis=2)\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "def get_copy_var_ops(*, src_scope_name='main', target_scope_name='target'):\n",
    "    op_holder = []\n",
    "\n",
    "    src_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=src_scope_name)\n",
    "    target_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=target_scope_name)\n",
    "\n",
    "    for src_var, target_var in zip(src_vars, target_vars):\n",
    "        op_holder.append(target_var.assign(src_var.value()))\n",
    "\n",
    "    return op_holder\n",
    "\n",
    "def saveModel(session, src_scope_name='main', path='./breakout.ckpt'):\n",
    "    src_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=src_scope_name)\n",
    "    tf.train.Saver(src_vars).save(session, path)\n",
    "    print(\"Model saved successfully!\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    env = gym.make('Breakout-v0')\n",
    "    env.reset()\n",
    "    episode = 0\n",
    "    reward_sum = 0\n",
    "    num_actions = 0\n",
    "    action_desc = ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
    "    #The three FIRE actions, 'FIRE', 'RIGHTFIRE', 'LEFTFIRE', make the game start for human player, but this is unnecessary in learning procedure.\n",
    "    output_size = env.action_space.n                # 6 'NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE'\n",
    "    height = HEIGHT\n",
    "    width = WIDTH\n",
    "    # 리플레이를 저장할 리스트\n",
    "    REPLAY_MEMORY = deque()\n",
    "\n",
    "    # 미니배치 - 꺼내서 사용할 리플레이 갯수\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # DQN 클래스의 mainDQN 인스턴스 생성\n",
    "        mainDQN = DQN(sess, height, width, output_size, name='main')\n",
    "        targetDQN = DQN(sess, height, width, output_size, name='target')\n",
    "\n",
    "        # 변수 초기화\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        epsilon = INITIAL_EPSILON\n",
    "\n",
    "        copy_ops = get_copy_var_ops(src_scope_name='main', target_scope_name='target')\n",
    "        sess.run(copy_ops)\n",
    "\n",
    "        for episode in range(max_episodes):\n",
    "            state = env.reset()\n",
    "            state = np.sum(state, axis=2)\n",
    "            rAll = 0\n",
    "            done = False\n",
    "            epsilon *= 0.99\n",
    "            start_lives = 5\n",
    "        \n",
    "            \n",
    "            while not done:\n",
    "                # action을 수행함 --> Get new state and reward from environment\n",
    "                action = mainDQN.egreedy_action(epsilon, env, state)\n",
    "                new_state, reward, done, info = env.step(action)\n",
    "\n",
    "                ter = done\n",
    "                \n",
    "                ter, start_lives = get_terminal(start_lives,info, reward, False, ter)\n",
    "                \n",
    "                new_state = np.sum(new_state, axis=2)\n",
    "                \n",
    "                # state, action, reward, next_state, done 을 메모리에 저장\n",
    "                REPLAY_MEMORY.append((state, action, reward, new_state,ter))\n",
    "\n",
    "                # 메모리에 10000개 이상의 값이 들어가면 가장 먼저 들어간 것부터 삭제\n",
    "                if len(REPLAY_MEMORY) > 10000:\n",
    "                    REPLAY_MEMORY.popleft()\n",
    "\n",
    "                # REPLAY_MEMORY 크기가 BATCH_SIZE 보다 크면 학습 - 이런식으로 되면 너무 코드가 오래걸림 학습을 너무 자주시키는 느낌\n",
    "                # 그래서 50000만이상으로 바꿈\n",
    "                if len(REPLAY_MEMORY) > 50000:\n",
    "                    mean_loss_value = update_from_memory(mainDQN, targetDQN, BATCH_SIZE)\n",
    "\n",
    "                rAll += reward\n",
    "                state = new_state\n",
    "\n",
    "#             if len(REPLAY_MEMORY) > BATCH_SIZE:\n",
    "#                 episode_list.append(episode)\n",
    "#                 train_error_list.append(mean_loss_value)\n",
    "#                 actions_list.append(rAll)\n",
    "\n",
    "            if episode % COPY_PERIOD == 1:\n",
    "                sess.run(copy_ops)\n",
    "\n",
    "#             if episode % TEST_PERIOD == 0:\n",
    "#                 total_reward = 0\n",
    "#                 for i in range(1):\n",
    "#                     total_reward += bot_play(mainDQN, env)\n",
    "\n",
    "#                 ave_reward = total_reward / 1\n",
    "#                 print(\"episode: {0}, Epsilon: {1}, Evaluation Average Reward: {2}\".format(episode,epsilon, ave_reward))\n",
    "#                 if ave_reward >= 200:\n",
    "#                     break\n",
    "                    \n",
    "            print(\"episode: {0}, Epsilon: {1}, Reward: {2}\".format(episode,epsilon, rAll))\n",
    "\n",
    "        saveModel(sess, src_scope_name='main', path='./breakout.ckpt')\n",
    "\n",
    "        env.reset()\n",
    "        env.close()\n",
    "#         draw_error_values()\n",
    "\n",
    "        input(\"Press Enter to make the trained bot play...\")\n",
    "        bot_play(mainDQN, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-27 20:27:45,059] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "output = env.observation_space\n",
    "output1 = env.action_space\n",
    "print(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}