{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# https://github.com/openai/gym/wiki/CartPole-v0\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "\n",
    "\n",
    "# 하이퍼파라미터\n",
    "\n",
    "max_episodes = 10000\n",
    "discount_factor = 0.9\n",
    "episode_list = []\n",
    "train_error_list = []\n",
    "actions_list = []\n",
    "HEIGHT = 210\n",
    "WIDTH = 160\n",
    "\n",
    "# 테스트 에피소드 주기\n",
    "TEST_PERIOD = 100\n",
    "\n",
    "# src model에서 target model로 trainable variable copy 주기\n",
    "COPY_PERIOD = 10\n",
    "\n",
    "\n",
    "\n",
    "#네트워크 클래스 구성\n",
    "class DQN:\n",
    "    def __init__(self, session, height, width, output_size, name=\"main\"):\n",
    "        # 네트워크 정보 입력\n",
    "        self.session = session\n",
    "        self.height = HEIGHT\n",
    "        self.width = WIDTH\n",
    "        self.output_size = output_size\n",
    "        self.net_name = name\n",
    "        \n",
    "        # 네트워크 생성\n",
    "        self.build_network()\n",
    "\n",
    "    def build_network(self):\n",
    "        with tf.variable_scope(self.net_name):\n",
    "            # Convolutional Neural Network (3 filter 2 Fc layer)\n",
    "            self.X = tf.placeholder(shape=[None, self.height, self.width, 1], dtype=tf.float32)\n",
    "            self.Y = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "\n",
    "            W_conv1 = tf.Variable(tf.truncated_normal([6, 4, 1, 32], stddev =0.1))\n",
    "            W_conv2 = tf.Variable(tf.truncated_normal([4, 4, 32, 64], stddev =0.1))\n",
    "            W_conv3 = tf.Variable(tf.truncated_normal([5, 3, 64, 64], stddev =0.1))\n",
    "            b_conv1 = tf.Variable(tf.constant(0.1, shape = [32]))\n",
    "            b_conv2 = tf.Variable(tf.constant(0.1, shape = [64]))\n",
    "            b_conv3 = tf.Variable(tf.constant(0.1, shape = [64]))\n",
    "            \n",
    "            W_fc1 = tf.Variable(tf.truncated_normal([11*9*64, 512], stddev= 0.1))\n",
    "            b_fc1 = tf.Variable(tf.constant(0.1, shape = [512]))\n",
    "            W_fc2 = tf.Variable(tf.truncated_normal([512, output_size], stddev =0.1))\n",
    "            b_fc2 = tf.Variable(tf.constant(0.1, shape = [output_size]))\n",
    "            \n",
    "            h_conv1 = tf.nn.relu(tf.nn.conv2d(self.X, W_conv1, strides= [1,4,4,1], padding='VALID') + b_conv1)\n",
    "            h_conv2 = tf.nn.relu(tf.nn.conv2d(h_conv1, W_conv2, strides = [1,2,2,1], padding ='VALID') + b_conv2)\n",
    "            h_conv3 = tf.nn.relu(tf.nn.conv2d(h_conv2, W_conv3, strides = [1,2,2,1], padding ='VALID') + b_conv3)\n",
    "            \n",
    "            L1 = tf.reshape(h_conv3, [-1, W_fc1.get_shape().as_list()[0]])\n",
    "            L2 = tf.nn.relu(tf.matmul(L1,W_fc1)+b_fc1)\n",
    "            self.Qpred = tf.matmul(L2, W_fc2)+b_fc2\n",
    "            \n",
    "        # 손실 함수 및 최적화 함수\n",
    "        self.action = tf.placeholder(shape=[None, self.output_size], dtype=tf.float32)\n",
    "        Q_action = tf.reduce_sum(tf.multiply(self.Qpred, self.action), reduction_indices=1)\n",
    "        self.loss = tf.reduce_mean(tf.square(self.Y - Q_action))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "\n",
    "    # 예측한 Q값 구하기\n",
    "    def predict(self, state):\n",
    "#         x = np.reshape(state, newshape=[-1, 84, 84, 1])\n",
    "\n",
    "        x = np.reshape(state, newshape=[-1, 210,160, 1])\n",
    "        return self.session.run(self.Qpred, feed_dict={self.X: x})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def bot_play(DQN, env):\n",
    "    \"\"\"\n",
    "    See our trained network in action\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    state = np.sum(state, axis=2)\n",
    "    reward_sum = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = np.argmax(DQN.predict(state))\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "        state = np.sum(new_state, axis=2)\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def restoreModel(session, path='./breakout.ckpt'):\n",
    "    tf.train.Saver().restore(sess=session, save_path=path)\n",
    "    print(\"Model restored successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('Breakout-v0')\n",
    "    height = HEIGHT \n",
    "    width = WIDTH\n",
    "    output_size = env.action_space.n                # 6 'NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE'\n",
    "\n",
    "    # 미니배치 - 꺼내서 사용할 리플레이 갯수\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # DQN 클래스의 mainDQN 인스턴스 생성\n",
    "        mainDQN = DQN(sess, height, width, output_size, name='main')\n",
    "        restoreModel(sess, \"./cartpole.ckpt\")\n",
    "\n",
    "        for episode in range(max_episodes):\n",
    "            bot_play(mainDQN, env)\n",
    "\n",
    "        env.reset()\n",
    "        env.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
