{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - If you know Numpy*,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.zeros((2,2)); b = np.ones((2,2))\n",
    "np.sum(b, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(a,(1,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Repeat in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "a = tf.zeros((2, 2)); b = tf.ones((2, 2))\n",
    "sess.run(tf.reduce_sum(b,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(2)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.reshape(a, (1, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sess.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "a = np.zeros((2, 2)); ta = tf.zeros((2, 2))\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros_8:0\", shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow first defines a computation graph that has no numerical value until evaluated.(very similar concept to Theano)\n",
    "print(ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(ta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Computation Graph\n",
    " - In TensorFlow program, it first assembles a graph, and uses a session to execute ops in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(5.0)\n",
    "b = tf.constant(6.0)\n",
    "c = a * b\n",
    "\n",
    "sess = tf.Session()\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Session\n",
    "\n",
    " - A Session object encapsulates the environment in which operation and tensor objects are evaluated\n",
    " - Both type of the session creation has same behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n",
      "30.0\n",
      "30.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(c))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))\n",
    "    print(c.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Variables\n",
    " - All tensors used previously were constant tensors\n",
    " - To train a model, we need variable type to hold and update values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(tf.zeros((2,2)),name =\"weight\")\n",
    "with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables need initialization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01294442  0.01419749]\n",
      " [ 0.06661883 -0.00926303]\n",
      " [ 0.00124434  0.07218869]\n",
      " [ 0.13402466 -0.05318997]\n",
      " [ 0.04164162 -0.09472268]]\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(tf.random_normal([5, 2], stddev=0.1), name=\"weight\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "state = tf.Variable(0, name=\"counter\")\n",
    "new_value = tf.add(state, tf.constant(1))\n",
    "update = tf.assign(state, new_value)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(state))\n",
    "    for _ in range(3):\n",
    "        sess.run(update)\n",
    "        print(sess.run(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.constant(1)\n",
    "x2 = tf.constant(2)\n",
    "x3 = tf.constant(3)\n",
    "temp = tf.add(x2, x3)\n",
    "mul  = tf.multiply(x1, temp)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result1, result2 = sess.run([mul, temp])\n",
    "    print(result1, result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Placeholder\n",
    " - All previous examples have manually defined tensors\n",
    "  - How can we input externlal data into TensorFlow?\n",
    "  - Most simple way is using tf.placeholder and feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.int16)\n",
    "b = tf.placeholder(tf.int16)\n",
    "\n",
    "add = tf.add(a, b)\n",
    "mul = tf.multiply(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(add, feed_dict={a:2, b:3}))\n",
    "    print(sess.run(mul, feed_dict={a:2, b:3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n",
      "[[ 12.]]\n"
     ]
    }
   ],
   "source": [
    "# using tf.constant\n",
    "matrix1 = tf.constant([[3., 3.]])\n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "product = tf.matmul(matrix1, matrix2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(product)\n",
    "    print(result)\n",
    "    \n",
    "# using placeholder\n",
    "import numpy as np\n",
    "matrix1 = tf.placeholder(tf.float32, [1, 2])\n",
    "matrix2 = tf.placeholder(tf.float32, [2, 1])\n",
    "product = tf.matmul(matrix1, matrix2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    mv1 = np.array([[3., 3.]])\n",
    "    mv2 = np.array([[2.],[2.]])\n",
    "    result = sess.run(product, feed_dict = {matrix1: mv1, matrix2: mv2})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - MNIST with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      " [1000/15000] loss:35.415 \n",
      " [2000/15000] loss:12.680 \n",
      " [3000/15000] loss:14.525 \n",
      " [4000/15000] loss:3.504 \n",
      " [5000/15000] loss:2.866 \n",
      " [6000/15000] loss:3.122 \n",
      " [7000/15000] loss:2.622 \n",
      " [8000/15000] loss:0.818 \n",
      " [9000/15000] loss:1.386 \n",
      " [10000/15000] loss:0.000 \n",
      " [11000/15000] loss:0.000 \n",
      " [12000/15000] loss:0.373 \n",
      " [13000/15000] loss:0.000 \n",
      " [14000/15000] loss:0.000 \n",
      " [15000/15000] loss:0.000 \n",
      " Optimization Finished! \n",
      "Train accuracy: 0.998 \n",
      "Test accuracy: 0.956 \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MINST data\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data \n",
    "mnist = input_data. read_data_sets ( \"/tmp/data/\" , one_hot= True )\n",
    "\n",
    "learning_rate = 0.001 \n",
    "max_steps = 15000 \n",
    "batch_size = 128\n",
    "\n",
    "x = tf. placeholder (tf.float32, [ None , 784 ]) \n",
    "y = tf. placeholder (tf.float32, [ None , 10 ])\n",
    "\n",
    "def MLP (inputs):\n",
    "    W_1 = tf. Variable (tf. random_normal ([ 784 , 256 ])) \n",
    "    b_1 = tf. Variable (tf. zeros ([ 256 ]))\n",
    "\n",
    "    W_2 = tf. Variable (tf. random_normal ([ 256 , 256 ]))\n",
    "    b_2 = tf. Variable (tf. zeros ([ 256 ]))\n",
    "\n",
    "    W_out = tf.Variable (tf. random_normal ([ 256 , 10 ]))\n",
    "    b_out = tf. Variable (tf. zeros ([ 10 ]))\n",
    "\n",
    "    h_1 = tf.add (tf.matmul (inputs, W_1), b_1)\n",
    "    h_1 = tf.nn.relu (h_1)\n",
    "\n",
    "    h_2 = tf.add (tf.matmul (h_1, W_2), b_2) \n",
    "    h_2 = tf.nn.relu (h_2)\n",
    "\n",
    "    out = tf.add (tf.matmul (h_2, W_out), b_out)\n",
    "    return out\n",
    "\n",
    "net = MLP (x)\n",
    "\n",
    "\n",
    "# define loss and optimizer \n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=net, labels=y))\n",
    "opt = tf.train.AdamOptimizer(learning_rate).minimize (loss_op)\n",
    "\n",
    "# initializing the variables\n",
    "init_op = tf.global_variables_initializer() \n",
    "sess = tf.Session () \n",
    "sess.run(init_op) \n",
    "\n",
    "# train model\n",
    "\n",
    "for step in range (max_steps):\n",
    "\n",
    "    batch_X, batch_y = mnist.train.next_batch(batch_size) \n",
    "    _, loss = sess.run([opt, loss_op], feed_dict={x: batch_X, y: batch_y})  # _ 이거는 빈 상자 같은 뜻인 듯 데이터 처리 후 버릴때 쓰는 것 ? \n",
    "\n",
    "    if (step+ 1) % 1000 == 0 :                                         #1000마다 한번씩 출력되도록\n",
    "        print (\" [{}/{}] loss:{:.3f} \". format (step+ 1 , max_steps, loss))\n",
    "        \n",
    "print (\" Optimization Finished! \") \n",
    "\n",
    "# test modelb\n",
    "correct_prediction = tf.equal (tf.argmax(net, 1), tf.argmax (y, 1)) \n",
    "\n",
    "# calculate accuracy\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast (correct_prediction, tf.float32)) \n",
    "print (\"Train accuracy: {:.3f} \".format (sess. run (accuracy, feed_dict={x: mnist.train.images, y: mnist.train.labels}))) \n",
    "print (\"Test accuracy: {:.3f} \".format (sess. run (accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model\n",
    " - tf.nn.softmax_cross_entropy_with_logits\n",
    " is for calculate cross entropy loss between predictions and labels (instead, you can make own loss function)\n",
    " \n",
    " - tf.train.####Optimizer create an optimizer such as SGD, RMSProp, Adam etc..*\n",
    " \n",
    " - tf.train.####Optimizer.minimize(loss_op) adds optimization operation to computation graph\n",
    " \n",
    " - we don't have to be concern about computing gradients and updating variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Scope\n",
    " - Complicated TensorFlow models can have hendreds of variables\n",
    "  - tf.variable_scope() provides simple name-spacing to avoid clashes\n",
    "  - tf.get_variable() creates/accesses variables from within a variable scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.variable_scope()\n",
    " - Variable scope is a simple type of name-spacing that adds prefixes to variable names within scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1: var_4:0\n",
      "var2: foo_4/bar/var:0\n",
      "var3: foo_4/bar/var_1:0\n"
     ]
    }
   ],
   "source": [
    "var1 = tf.Variable([1], name=\"var\")\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    with tf.variable_scope(\"bar\"):\n",
    "        var2 = tf.Variable([1], name=\"var\")\n",
    "        var3 = tf.Variable([1], name=\"var\")\n",
    "        \n",
    "print(\"var1: {}\".format(var1.name))\n",
    "print(\"var2: {}\".format(var2.name))\n",
    "print(\"var3: {}\".format(var3.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.get_variable()\n",
    " - tf.Variable doesn't provide variable reuse\n",
    "  - Variable reuse is necessary to implement RNN of Recursive NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1: var1:0\n",
      "var2: foo1/bar/var:0\n",
      "var3: foo1/bar/var:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph() #주피터 노트북과 같은 환경에서 코드를 실행하게되면, 주피터 커널을 리스타트하지않는 이상 변수들의 컨택스트(상황정보?)\n",
    "                         #컨택스트가 그대로 유지되기 때문에, 아래의 코드를 tf.reset_default_graph()없이 두번 이상 실행하게되면 에러가 난다.\n",
    "                         #위치는 상관없는 것 같다 아래에 있어도 괜찮음. \n",
    "\n",
    "var1 = tf.get_variable(\"var1\", [1])\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"foo1\"):\n",
    "    with tf.variable_scope(\"bar\") as scp:\n",
    "        var2 = tf.get_variable(\"var\", [1]) \n",
    "        scp.reuse_variables() # allow reuse variables\n",
    "        var3 = tf.get_variable(\"var\", [1])\n",
    "        \n",
    "print(\"var1: {}\".format(var1.name)) \n",
    "print(\"var2: {}\".format(var2.name))\n",
    "print(\"var3: {}\".format(var3.name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.get_variable()\n",
    " - Behavior depends on whether variable reuse enabled\n",
    " - Case 1:reuse flag set to False\n",
    "  - Create new variable and return\n",
    "  - Raise ValueError if there exists variable with given name\n",
    " - Case 2:reuse flag set to True\n",
    "  - Search for existinf variable with given name\n",
    "  - Raise ValueError if not found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1: foo/bar/var_2:0\n",
      "var2: foo/bar/var_2:0\n",
      "var3: foo/bar/var_2:0\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"foo\"):\n",
    "    with tf.variable_scope(\"bar\") as scp:\n",
    "        scp.reuse_variables()\n",
    "        var1 = tf.get_variable(\"var\", [1])  \n",
    "        var2 = tf.get_variable(\"var\", [1])\n",
    "        \n",
    "    with tf.variable_scope(\"bar\", reuse=True):\n",
    "        var3 = tf.get_variable(\"var\", [1])\n",
    "\n",
    "print(\"var1: {}\".format(var1.name)) \n",
    "print(\"var2: {}\".format(var2.name))\n",
    "print(\"var3: {}\".format(var3.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import variance_scaling_initializer\n",
    "he_init = variance_scaling_initializer()\n",
    "\n",
    "def conv(bottom, num_filter, ksize=3, stride=1, padding=\"SAME\", scope=None):\n",
    "    bottom_shape = bottom.get_shape().as_list()[3]\n",
    "    \n",
    "    with tf.variable_scope(scope or \"conv\"):\n",
    "        W = tf.get_variable(\"W\", [ksize, ksize, bottom_shape, numfilter], initializer=tf.constant_initializer(0))\n",
    "        b = tf.get_variable(\"b\", [num_filter], initializer = tf.constant_initializer(0))\n",
    "        x = tf.nn.conv2d(bottom, W, strides = [1, stride, stride, 1], padding = padding)\n",
    "        x = tf.nn.relu(tf.nn.bias_add(x,b))\n",
    "        \n",
    "    return x\n",
    "\n",
    "def maxpool(bottom, ksize=2, stride=2, padding=\"SAME\", scope= None):\n",
    "    with tf.variable_scope(scope or \"maxpool\"):\n",
    "        pool = tf.nn.max_pool(bottom, ksize=[1,ksize,ksize,1],stride=[1,stride,stride,1], padding=padding)\n",
    "        return pool\n",
    "    \n",
    "    \n",
    "def fc(bottom, num_dims, scope=None):\n",
    "    bottom_shape = bottom.get_shape().as_list()\n",
    "    if len(bottom_shape) > 2:\n",
    "        bottom = tf.reshape(bottom, [-1, reduce(lambda x, y: x*y, bottom_shape[1:])])\n",
    "        bottom_shape = bottom.get_shape().as_list()\n",
    "        \n",
    "    with tf.variable_scope(scope or \"fc\"):\n",
    "        W = tf.get_variable(\"W\", [bottom_shape[1], num_dims], initializer = he_init)\n",
    "        b = tf.get_variable(\"b\", [num_dims], initializer = tf.constant_initializer(0))\n",
    "        \n",
    "        out = tf.nn.bias_add(tf.matmul(bottom, W), b)\n",
    "    return out\n",
    "\n",
    "def fc_relu(bottom, num_dims, scope=None):\n",
    "    with tf.variable_scope(scope or \"fc\"):\n",
    "        out = fc(bottom, num_dims, scope = \"fc\")\n",
    "        relu = tf.nn.relu(out)\n",
    "        \n",
    "    return relu\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, None)\n",
    "\n",
    "def conv_net(x, keep_prob):\n",
    "    x = tf.reshape(x, shape = [-1,28,28,1])\n",
    "    \n",
    "    conv1 = conv(x, 32, 5, scope = \"conv_1\")\n",
    "    conv1 = ma8\n",
    "    \n",
    "    xpool(conv1, scope =\"maxpool_1\")\n",
    "    conv2 = conv(conv1, 64, 5, scope = \"conv_2\")\n",
    "    conv2 = maxpool(conv2, scope =\"maxpool_2\")\n",
    "    \n",
    "    fc1 = fc_relu(conv2, 1024, scope= \"fc_1\")\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    out = fc(fc1, 10, scope= \"out\" )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/Restore\n",
    " - My save wrapper function in my custom network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save(self, ckpt_dir, global_step=None):\n",
    "    if self.config.get(\"saver\") is None:\n",
    "        self.config[\"saver\"] = \\\n",
    "            tf.train.Saver(max_to_keep = 30)\n",
    "    \n",
    "    saver = self.config[\"saver\"]\n",
    "    sess  = self.config[\"sess\"]\n",
    "    \n",
    "    dirname = os.path.join(ckpt_dir, self.name)\n",
    "    \n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    saver.save(sess, dirname, global_step)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/Restore\n",
    " - My restore wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_latest_checkpoint(self, ckpt_dir, exclude=None):\n",
    "    path = tf.train.latest_checkpoint(ckpt_dir)\n",
    "    if path is None:\n",
    "        raise AssertionError(\"No ckpt exists in {0}.\".format(ckpt_dir))\n",
    "        \n",
    "        print(\"Load {} save file\".format(path))\n",
    "        self._load(path, exclude)\n",
    "\n",
    "\n",
    "def load_from_path(self, ckpt_path, exclude = None):\n",
    "    self._load(ckpt_path, exclude)\n",
    "    \n",
    "def _load(self, ckpt_path, exclude):\n",
    "    init_fn = slim.assign_from_checkpoint_fn(ckpt_path, slim.get_variables_to_restore(exclude=exclude), ignore_missing_vars=True)\n",
    "    init_fn(self.config[\"sess\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Contains model definitions for versions of the Oxford VGG network.\n",
    "These model definitions were introduced in the following technical report:\n",
    "  Very Deep Convolutional Networks For Large-Scale Image Recognition\n",
    "  Karen Simonyan and Andrew Zisserman\n",
    "  arXiv technical report, 2015\n",
    "  PDF: http://arxiv.org/pdf/1409.1556.pdf\n",
    "  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n",
    "  CC-BY-4.0\n",
    "More information can be obtained from the VGG website:\n",
    "www.robots.ox.ac.uk/~vgg/research/very_deep/\n",
    "Usage:\n",
    "  with slim.arg_scope(vgg.vgg_arg_scope()):\n",
    "    outputs, end_points = vgg.vgg_a(inputs)\n",
    "  with slim.arg_scope(vgg.vgg_arg_scope()):\n",
    "    outputs, end_points = vgg.vgg_16(inputs)\n",
    "@@vgg_a\n",
    "@@vgg_16\n",
    "@@vgg_19\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "\n",
    "def vgg_arg_scope(weight_decay=0.0005):\n",
    "  \"\"\"Defines the VGG arg scope.\n",
    "  Args:\n",
    "    weight_decay: The l2 regularization coefficient.\n",
    "  Returns:\n",
    "    An arg_scope.\n",
    "  \"\"\"\n",
    "  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                      activation_fn=tf.nn.relu,\n",
    "                      weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "                      biases_initializer=tf.zeros_initializer()):\n",
    "    with slim.arg_scope([slim.conv2d], padding='SAME') as arg_sc:\n",
    "      return arg_sc\n",
    "\n",
    "\n",
    "def vgg_a(inputs,\n",
    "          num_classes=1000,\n",
    "          is_training=True,\n",
    "          dropout_keep_prob=0.5,\n",
    "          spatial_squeeze=True,\n",
    "          scope='vgg_a'):\n",
    "  \"\"\"Oxford Net VGG 11-Layers version A Example.\n",
    "  Note: All the fully_connected layers have been transformed to conv2d layers.\n",
    "        To use in classification mode, resize input to 224x224.\n",
    "  Args:\n",
    "    inputs: a tensor of size [batch_size, height, width, channels].\n",
    "    num_classes: number of predicted classes.\n",
    "    is_training: whether or not the model is being trained.\n",
    "    dropout_keep_prob: the probability that activations are kept in the dropout\n",
    "      layers during training.\n",
    "    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n",
    "      outputs. Useful to remove unnecessary dimensions for classification.\n",
    "    scope: Optional scope for the variables.\n",
    "  Returns:\n",
    "    the last op containing the log predictions and end_points dict.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope, 'vgg_a', [inputs]) as sc:\n",
    "    end_points_collection = sc.name + '_end_points'\n",
    "    # Collect outputs for conv2d, fully_connected and max_pool2d.\n",
    "    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n",
    "                        outputs_collections=end_points_collection):\n",
    "      net = slim.repeat(inputs, 1, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "      net = slim.repeat(net, 1, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "      net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "      # Use conv2d instead of fully_connected layers.\n",
    "      net = slim.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout6')\n",
    "      net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout7')\n",
    "      net = slim.conv2d(net, num_classes, [1, 1],\n",
    "                        activation_fn=None,\n",
    "                        normalizer_fn=None,\n",
    "                        scope='fc8')\n",
    "      # Convert end_points_collection into a end_point dict.\n",
    "      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "      if spatial_squeeze:\n",
    "        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\n",
    "        end_points[sc.name + '/fc8'] = net\n",
    "      return net, end_points\n",
    "vgg_a.default_image_size = 224\n",
    "\n",
    "\n",
    "def vgg_16(inputs,\n",
    "           num_classes=1000,\n",
    "           is_training=True,\n",
    "           dropout_keep_prob=0.5,\n",
    "           spatial_squeeze=True,\n",
    "           scope='vgg_16'):\n",
    "  \"\"\"Oxford Net VGG 16-Layers version D Example.\n",
    "  Note: All the fully_connected layers have been transformed to conv2d layers.\n",
    "        To use in classification mode, resize input to 224x224.\n",
    "  Args:\n",
    "    inputs: a tensor of size [batch_size, height, width, channels].\n",
    "    num_classes: number of predicted classes.\n",
    "    is_training: whether or not the model is being trained.\n",
    "    dropout_keep_prob: the probability that activations are kept in the dropout\n",
    "      layers during training.\n",
    "    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n",
    "      outputs. Useful to remove unnecessary dimensions for classification.\n",
    "    scope: Optional scope for the variables.\n",
    "  Returns:\n",
    "    the last op containing the log predictions and end_points dict.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope, 'vgg_16', [inputs]) as sc:\n",
    "    end_points_collection = sc.name + '_end_points'\n",
    "    # Collect outputs for conv2d, fully_connected and max_pool2d.\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n",
    "                        outputs_collections=end_points_collection):\n",
    "      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "      # Use conv2d instead of fully_connected layers.\n",
    "      net = slim.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout6')\n",
    "      net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout7')\n",
    "      net = slim.conv2d(net, num_classes, [1, 1],\n",
    "                        activation_fn=None,\n",
    "                        normalizer_fn=None,\n",
    "                        scope='fc8')\n",
    "      # Convert end_points_collection into a end_point dict.\n",
    "      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "      if spatial_squeeze:\n",
    "        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\n",
    "        end_points[sc.name + '/fc8'] = net\n",
    "      return net, end_points\n",
    "vgg_16.default_image_size = 224\n",
    "\n",
    "\n",
    "def vgg_19(inputs,\n",
    "           num_classes=1000,\n",
    "           is_training=True,\n",
    "           dropout_keep_prob=0.5,\n",
    "           spatial_squeeze=True,\n",
    "           scope='vgg_19'):\n",
    "  \"\"\"Oxford Net VGG 19-Layers version E Example.\n",
    "  Note: All the fully_connected layers have been transformed to conv2d layers.\n",
    "        To use in classification mode, resize input to 224x224.\n",
    "  Args:\n",
    "    inputs: a tensor of size [batch_size, height, width, channels].\n",
    "    num_classes: number of predicted classes.\n",
    "    is_training: whether or not the model is being trained.\n",
    "    dropout_keep_prob: the probability that activations are kept in the dropout\n",
    "      layers during training.\n",
    "    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n",
    "      outputs. Useful to remove unnecessary dimensions for classification.\n",
    "    scope: Optional scope for the variables.\n",
    "  Returns:\n",
    "    the last op containing the log predictions and end_points dict.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope, 'vgg_19', [inputs]) as sc:\n",
    "    end_points_collection = sc.name + '_end_points'\n",
    "    # Collect outputs for conv2d, fully_connected and max_pool2d.\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n",
    "                        outputs_collections=end_points_collection):\n",
    "      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "      net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "      # Use conv2d instead of fully_connected layers.\n",
    "      net = slim.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout6')\n",
    "      net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout7')\n",
    "      net = slim.conv2d(net, num_classes, [1, 1],\n",
    "                        activation_fn=None,\n",
    "                        normalizer_fn=None,\n",
    "                        scope='fc8')\n",
    "      # Convert end_points_collection into a end_point dict.\n",
    "      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "      if spatial_squeeze:\n",
    "        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\n",
    "        end_points[sc.name + '/fc8'] = net\n",
    "      return net, end_points\n",
    "vgg_19.default_image_size = 224\n",
    "\n",
    "# Alias\n",
    "vgg_d = vgg_16\n",
    "vgg_e = vgg_19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Board\n",
    " - Very useful visualization tool\n",
    "  - Graph visualization, loss/accuracy plot, weight histogram plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    " - Simple usage\n",
    "    - Make a tf.summary.FileWriter to write summaries\n",
    "    - Create summaries using tf.summary.###\n",
    "    - Run summaries with sess.run(...)\n",
    "    - Add summaries to FileWriter with add_summary API\n",
    "    - Run command    tensorboard --logdir=## -host=## -port=##\n",
    "    - Default port is 6006\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "convolution() got an unexpected keyword argument 'weight_regularizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9e8c32dd40c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_arg_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_pts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"prediction\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-9e8c32dd40c3>\u001b[0m in \u001b[0;36mmy_net\u001b[0;34m(x, keep_prob, outputs_collections)\u001b[0m\n\u001b[1;32m     29\u001b[0m     with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n\u001b[1;32m     30\u001b[0m                        outputs_collections = outputs_collections):\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"conv1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pool1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\NSER-00\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_with_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_key_op'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_key_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: convolution() got an unexpected keyword argument 'weight_regularizer'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "max_steps = 10000\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "keep_prob = 0.5\n",
    "weight_decay = 0.0004\n",
    "logs_path = \"/tmp/tensorflow_logs/example\"\n",
    "\n",
    "def my_arg_scope(is_training, weight_decay):\n",
    "    with slim.arg_scope([slim.conv2d],\n",
    "                       activation_fn=tf.nn.relu,\n",
    "                       weight_regularizer = slim.l2_regularizer(weight_decay),\n",
    "                       weight_initializer = slim.variance_scaling_initializer(),\n",
    "                       biases_initializer = tf.zeros_initializer,\n",
    "                       stride=1, padding=\"SAME\"):\n",
    "        with slim.arg_scope([slim.dropout],\n",
    "                           is_training = is_training) as arg_sc:\n",
    "            return arg_sc\n",
    "        \n",
    "def my_net(x, keep_prob, outputs_collections =\"my_net\"):\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n",
    "                       outputs_collections = outputs_collections):\n",
    "        net = slim.conv2d(x, 64, [3,3], scope=\"conv1\")\n",
    "        net = slim.max_pool2d(net, [2,2], scope=\"pool1\")\n",
    "        \n",
    "        net = slim.conv2d(net, 128, [3,3], scope=\"conv2\")\n",
    "        net = slim.max_pool2d(net, [2,2], scope=\"pool2\")\n",
    "        \n",
    "        net = slim.conv2d(net, 256, [3,3], scope=\"conv3\")\n",
    "        # global average pooling\n",
    "        net = tf.reduce_mean(net, [1, 2], name=\"pool3\",keep_dims=True)\n",
    "        net = slim.dropout(net, keep_prob, scope = \"dropout3\")\n",
    "        net = slim.conv2d(net, 1024, [1,1], scope=\"fc4\")\n",
    "        \n",
    "        net = slim.dropout(net, keep_prob, scope = \"dropout4\")\n",
    "        net = slim.conv2d(net, 10, [1,1],\n",
    "                          activation_fn=None, scope=\"fc5\")\n",
    "        \n",
    "    end_points= \\\n",
    "        slim.utils.convert_collection_to_dict(outputs_collections)\n",
    "    return tf.reshape(net, [-1,10]), end_points\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "with slim.arg_scope(my_arg_scope(is_training, weight_decay)):\n",
    "    net, end_pts = my_net(x, keep_prob)\n",
    "    pred = slim.softmax(net, scope=\"prediction\")\n",
    "    \n",
    "with tf.variable_scope(\"losses\"):\n",
    "    cls_loss = slim.losses.softmax_cross_entropy(net, y)\n",
    "    reg_loss = tf.add_n(slim.losses.get_regularization_losses())\n",
    "    loss_op = cls_loss + reg_loss\n",
    "    \n",
    "with tf.variable_scope(\"Admam\"):\n",
    "    opt= tf.train.AdamOptimizer(lr)\n",
    "    # Op to calculate every variable gradient\n",
    "    grads = tf.gradients(loss_op, tf.trainable_variables())\n",
    "    grads = list(zip(grads, tf.trainable_variables()))\n",
    "    # Op to update all variables according to their gradient\n",
    "    apply_grads = opt.apply_gradients(grads_and_vars=grads)\n",
    "    \n",
    "with tf.variable_scope(\"accuracy\"):\n",
    "    correct_op= tf.equal(tf.argmax(net,1), tf.argmax(y,1))\n",
    "    acc_op = tf.reduce_mean(tf.cast(correct_op, tf.float32))\n",
    "    \n",
    "    \n",
    "# Create a summary to monitor loss and accuracy\n",
    "summ_loss = tf.summary.scalar(\"loss\", loss_op)\n",
    "summ_acc = tf.summary.scalar(\"accuracy_test\", acc_op)\n",
    "\n",
    "# Create summary to visualize weights and grads\n",
    "for var in tf.trainable_varialbes():\n",
    "    tf.summary.histogram(var.name, var, collections=[\"my_summ\"])\n",
    "for grad, var in grads:\n",
    "    tf.summary.histogram(var.name + \"/gradient\", grad, collections=[\"my_summ\"])\n",
    "for grad, var in grads:\n",
    "    tf.summary.histogram(var.name + \"/gradient\", grad, collections=[\"my_summ\"])\n",
    "\n",
    "summ_wg = tf.summary.merge_all(key=\"my_summ\")\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=sess.graph)\n",
    "\n",
    "for step in range(max_steps):\n",
    "    batch_X, batch_y = mnist.train.next_batch(batch_size)\n",
    "    _, loss, plot_loss, plot_wg = sess.run([apply_grads, loss_op, summ_loss, summ_wg],\n",
    "                                          feed_dict = {x: batch_X, y: batch_y, is_training: True})\n",
    "    summary_writer.add_summary(plot_loss, step)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "test_acc = sess.run(acc_op, feed_dict={x: mnist.test.images, \n",
    "                                      y:mnist.test.labels,\n",
    "                                      is_training: False})\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
